----- Chinese
# ğŸŒ CNIæ’ä»¶ï¼šK8Så¤§å­¦çš„"ç½‘ç»œè¿è¥å•†"

## å‰è¨€ï¼šæ²¡æœ‰ç½‘ç»œï¼Œå†å¥½çš„å®¿èˆä¹Ÿæ˜¯å­¤å²›
å¤§å®¶å¥½ï¼ä»Šå¤©æˆ‘ä»¬æ¥èŠèŠCNIæ’ä»¶è¿™ä¸ª"å¹•åè‹±é›„"ã€‚ä½ å¯èƒ½å¤©å¤©ç”¨kubectlï¼Œå¤©å¤©çœ‹Podï¼Œä½†ä½ æƒ³è¿‡æ²¡æœ‰ï¼šè¿™äº›Podæ˜¯æ€ä¹ˆäº’ç›¸èŠå¤©çš„ï¼Ÿå¤–é¢çš„æµé‡æ˜¯æ€ä¹ˆè¿›æ¥çš„ï¼Ÿ

æƒ³è±¡ä¸€ä¸‹ï¼Œå¦‚æœK8Så¤§å­¦é‡Œçš„å®¿èˆæ¥¼éƒ½æ²¡æœ‰ç½‘ç»œï¼š
- ğŸ  å®¿èˆå»ºå¾—å†æ¼‚äº®ï¼Œå­¦ç”Ÿä»¬ä¹Ÿæ— æ³•äº’ç›¸è”ç³»
- ğŸ“± WiFiè¿ä¸ä¸Šï¼Œå¤–å–å«ä¸äº†ï¼Œå¿«é€’æ”¶ä¸åˆ°
- ğŸ® æ¸¸æˆæ‰“ä¸äº†ï¼Œè§†é¢‘çœ‹ä¸äº†ï¼Œä½œä¸šäº¤ä¸äº†

è¿™å°±æ˜¯æ²¡æœ‰CNIæ’ä»¶çš„K8sé›†ç¾¤ - ä¸€å †"æ•°å­—å­¤å²›"ï¼

## 1ï¸âƒ£ CNIæ’ä»¶æ˜¯ä»€ä¹ˆï¼ŸK8Så¤§å­¦çš„"ç½‘ç»œè¿è¥å•†"

### CNIï¼šContainer Network Interfaceï¼ˆå®¹å™¨ç½‘ç»œæ¥å£ï¼‰
å°±åƒä½ å®¶é‡Œéœ€è¦é€‰æ‹©å®½å¸¦è¿è¥å•†ä¸€æ ·ï¼ŒK8Sé›†ç¾¤ä¹Ÿéœ€è¦é€‰æ‹©CNIæ’ä»¶æ¥æä¾›ç½‘ç»œæœåŠ¡ï¼š
- ç½‘ç»œè¦†ç›–ï¼šç»™æ¯ä¸ªå®¿èˆï¼ˆPodï¼‰æä¾›ç½‘ç»œè¿æ¥
- IPåˆ†é…ï¼šä¸ºæ¯ä¸ªå­¦ç”Ÿåˆ†é…å”¯ä¸€çš„ç½‘ç»œåœ°å€
- æ•°æ®ä¼ è¾“ï¼šç¡®ä¿æ¶ˆæ¯èƒ½åœ¨å®¿èˆé—´æ­£ç¡®ä¼ é€’
- æœåŠ¡è´¨é‡ï¼šæä¾›ä¸åŒç­‰çº§çš„ç½‘ç»œæœåŠ¡

ğŸ’¡ å…³é”®ç†è§£ï¼šä½ éœ€è¦åœ¨å¤šä¸ªè¿è¥å•†ä¸­é€‰æ‹©ä¸€ä¸ªæœ€é€‚åˆä½ éœ€æ±‚çš„ã€‚æ¯ä¸ªCNIæ’ä»¶éƒ½æœ‰è‡ªå·±çš„ç‰¹è‰²å’Œä¼˜åŠ¿ï¼Œå°±åƒä¸åŒè¿è¥å•†æœ‰ä¸åŒçš„å¥—é¤å’ŒæœåŠ¡è´¨é‡ã€‚

### å·¥ä½œåŸç†
è¿˜è®°å¾—K8Så¤§å­¦çš„è®¾æ–½ç®¡ç†å‘˜kubeletä¹ˆï¼Ÿæ•…äº‹ä»å®ƒè¿™é‡Œå¼€å§‹ï¼š
```bash
# ç¬¬1æ­¥ï¼šPodåˆ›å»ºæ—¶
kubelet: "å˜¿ï¼ŒCNIæ’ä»¶ï¼Œç»™è¿™ä¸ªæ–°Podåˆ†é…ä¸ªç½‘ç»œï¼"
CNIæ’ä»¶: "æ”¶åˆ°ï¼æ­£åœ¨åˆ†é…IPå’Œè®¾ç½®ç½‘ç»œ..."

# ç¬¬2æ­¥ï¼šç½‘ç»œé…ç½®
CNIæ’ä»¶: "IPåˆ†é…å®Œæ¯•ï¼š10.244.1.100"
CNIæ’ä»¶: "è·¯ç”±è¡¨å·²æ›´æ–°ï¼Œç½‘ç»œæ¥å£å·²é…ç½®"

# ç¬¬3æ­¥ï¼šPodåˆ é™¤æ—¶
kubelet: "è¿™ä¸ªPodè¦åˆ äº†ï¼Œæ¸…ç†ç½‘ç»œèµ„æº"
CNIæ’ä»¶: "æ˜ç™½ï¼æ­£åœ¨å›æ”¶IPå’Œæ¸…ç†è·¯ç”±..."
```

## 2ï¸âƒ£ é›†ç¾¤ä¸­å¦‚ä½•æ£€æŸ¥CNIç»„ä»¶ï¼šç½‘ç»œå·¥ç¨‹å¸ˆçš„"ä½“æ£€æŠ¥å‘Š" ğŸ”

### åŸºç¡€å¥åº·æ£€æŸ¥ï¼šCNIæ’ä»¶è¿˜æ´»ç€å—ï¼Ÿ
```bash
# 1. æŸ¥çœ‹CNIç›¸å…³PodçŠ¶æ€
kubectl get pods -n kube-system | grep -E "(flannel|calico|cilium|weave|canal)"

# æ­£å¸¸è¾“å‡ºç¤ºä¾‹ï¼ˆä»¥Calicoä¸ºä¾‹ï¼‰ï¼š
NAME                                      READY   STATUS    RESTARTS        AGE
calico-kube-controllers-7bb4b4d4d-6w29j   1/1     Running   1 (3m50s ago)   37h
canal-ck4gg                               2/2     Running   2 (3m50s ago)   37h
# - calico-kube-controllers: Calicoçš„"å¤§è„‘"ï¼Œè´Ÿè´£ç­–ç•¥ç®¡ç†å’Œå…¨å±€åè°ƒ
# - canal-xxx: Canalæ˜¯"æ‰§è¡Œå™¨"ï¼Œåœ¨æ¯ä¸ªèŠ‚ç‚¹ä¸Šå®é™…æ‰§è¡Œç½‘ç»œé…ç½®ï¼ˆDaemonSetï¼‰
#   Canal = Calicoï¼ˆç­–ç•¥åŠŸèƒ½ï¼‰+ Flannelï¼ˆç½‘ç»œå®ç°ï¼‰çš„ç»„åˆ

# 2. æ£€æŸ¥CNIæ’ä»¶è¯¦ç»†ä¿¡æ¯
kubectl describe pod -n kube-system calico-kube-controllers-7bb4b4d4d-6w29j
kubectl describe pod -n kube-system canal-ck4gg

# 3. æŸ¥çœ‹CNIé…ç½®æ–‡ä»¶
ls -la /etc/cni/net.d/
cat /etc/cni/net.d/10-canal.conflist  # Canalé…ç½®ç¤ºä¾‹
```

### æ·±åº¦è¯Šæ–­ï¼šæŸ¥çœ‹CNIæ’ä»¶æ—¥å¿—
```bash
# 1. æŸ¥çœ‹Calicoæ§åˆ¶å™¨æ—¥å¿—
kubectl logs -n kube-system calico-kube-controllers-7bb4b4d4d-6w29j -f

# 2. æŸ¥çœ‹Canal Podæ—¥å¿—
# å…ˆæ£€æŸ¥Canal Podçš„å®¹å™¨ç»“æ„ï¼š
kubectl get pod -n kube-system canal-ck4gg -o jsonpath='{.spec.containers[*].name}'
# è¾“å‡ºç¤ºä¾‹ï¼šå¯èƒ½æ˜¯ "calico-node" æˆ– "canal" ç­‰

# æŸ¥çœ‹Canal Podæ—¥å¿—ï¼ˆæ ¹æ®å®é™…å®¹å™¨åç§°ï¼‰
kubectl logs -n kube-system canal-ck4gg -f
# å¦‚æœæ˜¾ç¤ºå¤šä¸ªå®¹å™¨é”™è¯¯ï¼Œåˆ™éœ€è¦æŒ‡å®šå®¹å™¨åï¼š
# kubectl logs -n kube-system canal-ck4gg -c calico-node -f

# æŸ¥çœ‹å®¹å™¨è¯¦ç»†ä¿¡æ¯ï¼š
kubectl describe pod -n kube-system canal-ck4gg | grep -A 10 "Containers:"

# 3. æŸ¥çœ‹å†å²æ—¥å¿—ï¼ˆæœ€è¿‘100è¡Œï¼‰
kubectl logs -n kube-system calico-kube-controllers-7bb4b4d4d-6w29j --tail=100

# 4. æŸ¥çœ‹æ‰€æœ‰Calicoç›¸å…³Podæ—¥å¿—
kubectl logs -n kube-system -l k8s-app=calico-kube-controllers --tail=50
kubectl logs -n kube-system -l k8s-app=canal --tail=50
```

### å…¸å‹æ—¥å¿—å†…å®¹è§£è¯»ï¼š
```bash
âœ… æ­£å¸¸å·¥ä½œçš„æ—¥å¿—ï¼ˆCalicoæ§åˆ¶å™¨ç¤ºä¾‹ï¼‰ï¼š
# Calicoæ§åˆ¶å™¨æ­£å¸¸å¯åŠ¨æ—¥å¿—
2024-01-20 10:30:15.123 [INFO][1] main.go 123: Starting Calico kube-controllers version v3.26.1
2024-01-20 10:30:15.234 [INFO][1] main.go 145: Loaded configuration from environment config=&config.Config{...}
2024-01-20 10:30:15.345 [INFO][1] main.go 167: Ensuring Calico datastore is initialized
2024-01-20 10:30:15.456 [INFO][1] client.go 234: Initializing CalicoClient
2024-01-20 10:30:15.567 [INFO][1] controllers.go 89: Starting policy controller
2024-01-20 10:30:15.678 [INFO][1] controllers.go 123: Policy controller is now in sync
# Canal Podæ­£å¸¸æ—¥å¿—ï¼ˆcalico-nodeå®¹å™¨ï¼‰ï¼š
2024-01-20 10:30:16.123 [INFO][1] startup/startup.go 456: Early log level set to info
2024-01-20 10:30:16.234 [INFO][1] startup/startup.go 478: Using NODENAME environment for node name: controlplane
2024-01-20 10:30:16.345 [INFO][1] startup/startup.go 567: Determined node IP: 192.168.1.100
2024-01-20 10:30:16.456 [INFO][1] startup/startup.go 678: Node IPv4 changed, will check for conflicts
2024-01-20 10:30:16.567 [INFO][1] startup/startup.go 789: Calico node started successfully

âŒ æœ‰é—®é¢˜çš„æ—¥å¿—ç¤ºä¾‹ï¼š
# API Serverè¿æ¥å¤±è´¥
2024-01-20 10:30:15.123 [ERROR][1] client.go 234: Failed to create Calico client: Get "https://10.96.0.1:443/api/v1": dial tcp 10.96.0.1:443: connect: connection refused

# èŠ‚ç‚¹ç½‘ç»œé…ç½®é—®é¢˜
2024-01-20 10:30:16.234 [ERROR][1] startup.go 456: Failed to auto-detect an IPv4 address: no valid IPv4 addresses found on the host interfaces

# IPAMåˆ†é…å¤±è´¥
2024-01-20 10:30:17.345 [ERROR][1] ipam.go 123: Failed to allocate IP address: IPAM block affinity changed
```

### ç½‘ç»œæ¥å£æ£€æŸ¥ï¼šCNIåˆ›å»ºçš„"ç½‘ç»œç®¡é“"

# 1. æŸ¥çœ‹èŠ‚ç‚¹ç½‘ç»œæ¥å£ï¼ˆCalico/Canalç¯å¢ƒï¼‰
```bash
ip addr show: IPåœ°å€è§£æ
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
# â†‘ æœ¬åœ°å›ç¯æ¥å£ï¼Œæ‰€æœ‰ç³»ç»Ÿéƒ½æœ‰ï¼Œç”¨äºæœ¬æœºå†…éƒ¨é€šä¿¡
    inet 127.0.0.1/8 scope host lo
    ...

2: enp1s0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
# â†‘ èŠ‚ç‚¹ä¸»ç½‘å¡ï¼Œè¿æ¥å¤–éƒ¨ç½‘ç»œï¼ŒIP: 172.30.1.2/24
    inet 172.30.1.2/24 brd 172.30.1.255 scope global dynamic noprefixroute enp1s0
    ...

3: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1454 qdisc noqueue state DOWN group default
# â†‘ Dockerç½‘æ¡¥ï¼ŒçŠ¶æ€DOWNè¯´æ˜K8sç¯å¢ƒä¸­ä¸ä½¿ç”¨Dockerç½‘ç»œï¼Œè¿™æ˜¯æ­£å¸¸çš„
    inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0
    ...

4: flannel.1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue state UNKNOWN group default
# â†‘ Flannel VXLANéš§é“æ¥å£ï¼Œç”¨äºè·¨èŠ‚ç‚¹Podé€šä¿¡ï¼ŒIP: 192.168.0.0/32
    inet 192.168.0.0/32 brd 192.168.0.0 scope global flannel.1
    ...

7: cali61f99eae10e@if3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default
# â†‘ Pod1çš„Calicoæ¥å£ (Pod IP: 192.168.0.2) - æ¯ä¸ªPodéƒ½æœ‰å”¯ä¸€IPç”¨äºé€šä¿¡
    link-netns cni-842152e1-c862-1447-5370-39a4c62c7ae9
    ...

8: cali6dac6e169cb@if3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default
# â†‘ Pod2çš„Calicoæ¥å£ (Pod IP: 192.168.0.3) - å¦ä¸€ä¸ªPodçš„ç‹¬ç«‹IP
    link-netns cni-12d50b99-d658-eb60-0e74-32229c0e8782
    ...

9: calic45561273ca@if3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default
# â†‘ Pod3çš„Calicoæ¥å£ (Pod IP: 192.168.0.4) - ç¬¬ä¸‰ä¸ªPodçš„ç‹¬ç«‹IP
    link-netns cni-d4a8391a-1a8a-a19a-5d51-8ce7ad38a87d
    ...

10: cali0c6ec79a9d0@if3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default
# â†‘ Pod4çš„Calicoæ¥å£ (Pod IP: 192.168.0.5) - ç¬¬å››ä¸ªPodçš„ç‹¬ç«‹IP
    link-netns cni-84901d58-f898-6701-276e-d6c8c55afc27
    ...
```
ğŸ’¡ å…³é”®ç‚¹ï¼šæ¯ä¸ªPodéƒ½æœ‰ä¸€ä¸ªå”¯ä¸€çš„IPåœ°å€ï¼ˆç±»ä¼¼æ¯ä¸ªå­¦ç”Ÿæœ‰ç‹¬ç«‹çš„å®¿èˆé—¨ç‰Œå·ï¼‰ï¼Œç”¨äºé›†ç¾¤å†…é€šä¿¡
- Podå¯ä»¥ç›´æ¥é€šè¿‡IPäº’ç›¸è®¿é—®ï¼š192.168.0.2 â†” 192.168.0.3
- è¿™äº›IPæ˜¯é›†ç¾¤å†…éƒ¨IPï¼Œå¤–éƒ¨æ— æ³•ç›´æ¥è®¿é—®
- mtu 1500è¡¨ç¤ºæœ€å¤§ä¼ è¾“å•å…ƒä¸º1500å­—èŠ‚ï¼Œè¿™æ˜¯ä»¥å¤ªç½‘çš„æ ‡å‡†å€¼


# 2. æŸ¥çœ‹è·¯ç”±è¡¨
```bash
ip route show:è·¯ç”±è¡¨è§£æ

default via 172.30.1.1 dev enp1s0 proto dhcp src 172.30.1.2 metric 1002 mtu 1500
# â†‘ é»˜è®¤è·¯ç”±ï¼Œæ‰€æœ‰å¤–éƒ¨æµé‡é€šè¿‡ä¸»ç½‘å¡enp1s0å‡ºå»

172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 linkdown
# â†‘ Dockerç½‘ç»œè·¯ç”±ï¼Œlinkdownè¡¨ç¤ºæœªæ¿€æ´»ï¼ˆK8sç¯å¢ƒæ­£å¸¸ï¼‰

172.30.1.0/24 dev enp1s0 proto dhcp scope link src 172.30.1.2 metric 1002 mtu 1500
# â†‘ æœ¬åœ°ç½‘æ®µè·¯ç”±ï¼ŒåŒç½‘æ®µé€šä¿¡ç›´æ¥é€šè¿‡ä¸»ç½‘å¡

192.168.0.2 dev cali61f99eae10e scope link
192.168.0.3 dev cali6dac6e169cb scope link
192.168.0.4 dev calic45561273ca scope link
192.168.0.5 dev cali0c6ec79a9d0 scope link
# â†‘ Pod IPè·¯ç”±ï¼šæ¯ä¸ªPodéƒ½æœ‰ä¸€æ¡ç‚¹å¯¹ç‚¹è·¯ç”±åˆ°å¯¹åº”çš„caliæ¥å£
# è¿™æ˜¯Calicoçš„ç‰¹è‰²ï¼šç›´æ¥è·¯ç”±ï¼Œæ— éœ€ç½‘æ¡¥è½¬å‘ï¼Œæ€§èƒ½æ›´å¥½
```

# 3. æŸ¥çœ‹Calicoç‰¹æœ‰çš„ç½‘ç»œæ¥å£
```bash
ip link show | grep cali

# Calicoæ¥å£å‘½åè§„åˆ™è§£æï¼š
7: cali61f99eae10e@if3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP mode DEFAULT group default
# â†‘ cali + 11ä½éšæœºå­—ç¬¦ + @if3ï¼ˆå¯¹åº”Podå†…çš„eth0æ¥å£ï¼‰
8: cali6dac6e169cb@if3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP mode DEFAULT group default
9: calic45561273ca@if3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP mode DEFAULT group default
10: cali0c6ec79a9d0@if3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP mode DEFAULT group default
# æ¯ä¸ªæ¥å£éƒ½æ˜¯UPçŠ¶æ€ï¼Œè¯´æ˜å¯¹åº”çš„Podæ­£åœ¨è¿è¡Œ
```

# 4. æŸ¥çœ‹CNIç½‘æ¡¥
```bash
brctl show  # æˆ–è€… ip link show type bridge

# ç½‘æ¡¥çŠ¶æ€è§£æï¼š
bridge name     bridge id               STP enabled     interfaces
docker0         8000.22802df1db19       no
# â†‘ åªæœ‰docker0ç½‘æ¡¥ï¼Œæ²¡æœ‰æ¥å£è¿æ¥ï¼ˆæ­£å¸¸ï¼Œå› ä¸ºK8sä¸ä½¿ç”¨Dockerç½‘ç»œï¼‰
# æ³¨æ„ï¼šCalicoä½¿ç”¨ç‚¹å¯¹ç‚¹è·¯ç”±ï¼Œä¸éœ€è¦ä¼ ç»Ÿçš„ç½‘æ¡¥ï¼Œè¿™æ˜¯å®ƒçš„ä¼˜åŠ¿ä¹‹ä¸€
```

### Podç½‘ç»œè¿æ¥æ£€æŸ¥ï¼šç¡®è®¤"å®¿èˆç½‘ç»œ"æ­£å¸¸
```bash
# 1. æŸ¥çœ‹Podçš„ç½‘ç»œé…ç½®
kubectl exec -it <pod-name> -- ip addr show
# åº”è¯¥çœ‹åˆ°eth0æ¥å£ï¼Œæ¯ä¸ªPodéƒ½æœ‰å”¯ä¸€IPåœ¨192.168.0.xç½‘æ®µ

# 2. æµ‹è¯•Podé—´ç½‘ç»œè¿é€šæ€§ï¼ˆPod IPç›´æ¥é€šä¿¡ï¼‰
kubectl exec -it <pod1> -- ping <pod2-ip>
# ä¾‹å¦‚ï¼škubectl exec -it test-pod -- ping 192.168.0.3
# è¿™è¯æ˜äº†Podä¹‹é—´å¯ä»¥é€šè¿‡IPç›´æ¥é€šä¿¡ï¼Œæ— éœ€é¢å¤–é…ç½®

# 3. æµ‹è¯•Podåˆ°Serviceçš„è¿é€šæ€§
kubectl exec -it <pod-name> -- nslookup kubernetes.default.svc.cluster.local
# éªŒè¯DNSè§£æå’ŒServiceå‘ç°æ˜¯å¦æ­£å¸¸

# 4. æŸ¥çœ‹Podçš„ç½‘ç»œå‘½åç©ºé—´
kubectl exec -it <pod-name> -- cat /proc/net/route
# æŸ¥çœ‹Podå†…éƒ¨çš„è·¯ç”±è¡¨ï¼Œé»˜è®¤è·¯ç”±åº”è¯¥æŒ‡å‘ç½‘å…³

# 5. éªŒè¯Podä¸å¤–éƒ¨ç½‘ç»œè¿é€šæ€§
kubectl exec -it <pod-name> -- ping 8.8.8.8
# æµ‹è¯•Podæ˜¯å¦èƒ½è®¿é—®å¤–éƒ¨ç½‘ç»œ
```


### ç³»ç»Ÿçº§ç½‘ç»œæ£€æŸ¥ï¼šåº•å±‚"åŸºç¡€è®¾æ–½"çŠ¶æ€
```bash
# 1. æ£€æŸ¥iptablesè§„åˆ™ï¼ˆCNIæ’ä»¶ä¼šåˆ›å»ºç›¸å…³è§„åˆ™ï¼‰
iptables -t nat -L -n | grep -E "(KUBE|CNI)"
iptables -t filter -L -n | grep -E "(KUBE|CNI)"

# 2. æ£€æŸ¥å†…æ ¸æ¨¡å—ï¼ˆæŸäº›CNIéœ€è¦ç‰¹å®šæ¨¡å—ï¼‰
lsmod | grep -E "(vxlan|ipip|wireguard)"

# 3. æ£€æŸ¥ç³»ç»Ÿç½‘ç»œé…ç½®
sysctl net.bridge.bridge-nf-call-iptables
sysctl net.ipv4.ip_forward

# è¿™ä¸¤ä¸ªå€¼åº”è¯¥éƒ½æ˜¯1ï¼Œå¦åˆ™CNIå¯èƒ½å·¥ä½œå¼‚å¸¸
```

## 3ï¸âƒ£ ä¸»æµCNIæ’ä»¶å¤§PKï¼šç½‘ç»œè¿è¥å•†é€‰æ‹©æŒ‡å— ğŸ“¡

Flannelï¼šæ–°æ‰‹å‹å¥½çš„å…¥é—¨é€‰æ‹©
âœ… é…ç½®ç®€å•ï¼šå‡ è¡ŒYAMLæå®šï¼Œé€‚åˆK8sæ–°æ‰‹
âœ… ç¨³å®šå¯é ï¼šä¹…ç»è€ƒéªŒï¼Œç”Ÿäº§ç¯å¢ƒæ”¾å¿ƒç”¨
âœ… èµ„æºå ç”¨ä½ï¼šä¸åƒå†…å­˜ï¼Œå°é›†ç¾¤çš„æœ€çˆ±
âŒ åŠŸèƒ½æœ‰é™ï¼šç½‘ç»œç­–ç•¥æ”¯æŒè¾ƒå¼±
âŒ æ€§èƒ½ä¸€èˆ¬ï¼šå¤§è§„æ¨¡é›†ç¾¤å¯èƒ½åŠ›ä¸ä»å¿ƒ

é€‚ç”¨åœºæ™¯ï¼šå­¦ä¹ ç¯å¢ƒã€å°å‹é›†ç¾¤ã€è¿½æ±‚ç®€å•ç¨³å®š

Calicoï¼šåŠŸèƒ½å…¨é¢çš„ä¼ä¸šçº§é€‰æ‹©
âœ… æ€§èƒ½ä¼˜ç§€ï¼šçº¯ä¸‰å±‚ç½‘ç»œï¼Œæ€§èƒ½æ¥è¿‘åŸç”Ÿ
âœ… å®‰å…¨å¼ºå¤§ï¼šå†…ç½®ç½‘ç»œç­–ç•¥ï¼Œå®‰å…¨é˜²æŠ¤åˆ°ä½
âœ… æ‰©å±•æ€§å¥½ï¼šæ”¯æŒå¤§è§„æ¨¡é›†ç¾¤ï¼ˆ1000+èŠ‚ç‚¹ï¼‰
âœ… åŠŸèƒ½ä¸°å¯Œï¼šBGPè·¯ç”±ã€IPAMã€æœåŠ¡ç½‘æ ¼é›†æˆ
âŒ é…ç½®å¤æ‚ï¼šåŠŸèƒ½å¤šæ„å‘³ç€é…ç½®é¡¹å¤š
âŒ å­¦ä¹ æˆæœ¬é«˜ï¼šéœ€è¦ä¸€å®šç½‘ç»œåŸºç¡€

é€‚ç”¨åœºæ™¯ï¼šç”Ÿäº§ç¯å¢ƒã€å¤§è§„æ¨¡é›†ç¾¤ã€å¯¹å®‰å…¨æœ‰è¦æ±‚

Ciliumï¼šæ–°æ—¶ä»£çš„æŠ€æœ¯å…ˆé”‹
âœ… æŠ€æœ¯å…ˆè¿›ï¼šeBPFå†…æ ¸æŠ€æœ¯ï¼Œæ€§èƒ½çˆ†è¡¨
âœ… å¯è§‚æµ‹æ€§å¼ºï¼šç½‘ç»œæµé‡å¯è§†åŒ–ï¼Œæ•…éšœæ’æŸ¥åˆ©å™¨
âœ… å®‰å…¨å¢å¼ºï¼šAPIçº§åˆ«çš„å®‰å…¨ç­–ç•¥
âœ… æœåŠ¡ç½‘æ ¼ï¼šå†…ç½®Service MeshåŠŸèƒ½
âŒ å†…æ ¸è¦æ±‚é«˜ï¼šéœ€è¦è¾ƒæ–°çš„Linuxå†…æ ¸
âŒ å¤æ‚åº¦é«˜ï¼šåŠŸèƒ½å¼ºå¤§ä½†å­¦ä¹ æ›²çº¿é™¡å³­

é€‚ç”¨åœºæ™¯ï¼šäº‘åŸç”Ÿç¯å¢ƒã€å¾®æœåŠ¡æ¶æ„ã€è¿½æ±‚æè‡´æ€§èƒ½

Weave Netï¼šè‡ªåŠ¨åŒ–çš„æ™ºèƒ½é€‰æ‹©
âœ… é›¶é…ç½®ï¼šè‡ªåŠ¨å‘ç°èŠ‚ç‚¹ï¼Œè‡ªåŠ¨å»ºç«‹ç½‘ç»œ
âœ… åŠ å¯†é€šä¿¡ï¼šå†…ç½®ç½‘ç»œåŠ å¯†ï¼Œå®‰å…¨æ€§å¥½
âœ… æ•…éšœè‡ªæ„ˆï¼šç½‘ç»œåˆ†åŒºè‡ªåŠ¨æ¢å¤
âŒ æ€§èƒ½å¼€é”€ï¼šåŠ å¯†å’Œè‡ªåŠ¨åŒ–å¸¦æ¥æ€§èƒ½æŸè€—
âŒ è°ƒè¯•å›°éš¾ï¼šè‡ªåŠ¨åŒ–ç¨‹åº¦é«˜ï¼Œå‡ºé—®é¢˜ä¸å¥½æ’æŸ¥

é€‚ç”¨åœºæ™¯ï¼šå¤šäº‘ç¯å¢ƒã€ç½‘ç»œä¸ç¨³å®šåœºæ™¯ã€è¿½æ±‚è‡ªåŠ¨åŒ–


## 4ï¸âƒ£ å¸¸è§é—®é¢˜æ’æŸ¥ï¼šç½‘ç»œè¿è¥å•†æ•…éšœå¤„ç† ğŸ”§

### é—®é¢˜1ï¼šPodæ— æ³•äº’ç›¸é€šä¿¡
```bash
# ç—‡çŠ¶ï¼špingä¸é€šå…¶ä»–Pod
kubectl exec -it pod1 -- ping <pod2-ip>

# æ’æŸ¥æ­¥éª¤
1. æ£€æŸ¥CNIæ’ä»¶çŠ¶æ€
kubectl get pods -n kube-system | grep -E "(flannel|calico|cilium|weave)"

2. æŸ¥çœ‹ç½‘ç»œé…ç½®
kubectl get nodes -o wide
ip route show

3. æ£€æŸ¥é˜²ç«å¢™è§„åˆ™
iptables -L -n
```

### é—®é¢˜2ï¼šå¤–éƒ¨æ— æ³•è®¿é—®Service
```bash
# ç—‡çŠ¶ï¼šServiceåˆ›å»ºäº†ä½†è®¿é—®ä¸äº†
kubectl get svc

# æ’æŸ¥æ­¥éª¤  
1. æ£€æŸ¥Serviceé…ç½®
kubectl describe svc <service-name>

2. æŸ¥çœ‹Endpoints
kubectl get endpoints <service-name>

3. æ£€æŸ¥kube-proxy
kubectl get pods -n kube-system | grep kube-proxy
kubectl logs -n kube-system <kube-proxy-pod>
```

### é—®é¢˜3ï¼šç½‘ç»œæ€§èƒ½å·®
```bash
# ç—‡çŠ¶ï¼šç½‘ç»œå»¶è¿Ÿé«˜ï¼Œååé‡ä½

# æ€§èƒ½æµ‹è¯•
kubectl run test-pod --image=nicolaka/netshoot -it --rm
# åœ¨Podå†…æ‰§è¡Œ
iperf3 -c <target-ip>

# æ’æŸ¥æ–¹å‘
1. CNIæ’ä»¶æ€§èƒ½ç‰¹æ€§ï¼ˆOverlay vs Nativeï¼‰
2. ç½‘ç»œç­–ç•¥è§„åˆ™è¿‡å¤š
3. èŠ‚ç‚¹ç½‘ç»œé…ç½®é—®é¢˜
```

## ğŸ¯ æ€»ç»“
CNIæ’ä»¶å°±åƒK8Så¤§å­¦çš„"ç½‘ç»œè¿è¥å•†"ï¼Œè™½ç„¶å¹³æ—¶ä¸æ˜¾å±±ä¸éœ²æ°´ï¼Œä½†æ²¡æœ‰å®ƒï¼Œæ•´ä¸ªé›†ç¾¤å°±æ˜¯ä¸€ç›˜æ•£æ²™ï¼é€‰å¯¹äº†CNIæ’ä»¶ï¼Œä½ çš„K8sé›†ç¾¤å°±æœ‰äº†"ç½‘ç»œåŸºç¡€è®¾æ–½"ï¼ŒPodä»¬å¯ä»¥æ„‰å¿«åœ°é€šä¿¡ï¼ŒServiceå¯ä»¥æ­£å¸¸å·¥ä½œï¼Œä½ ä¹Ÿå¯ä»¥å®‰å¿ƒä¸‹ç­ï¼

----- English

# ğŸŒ CNI Plugins: The "Network Providers" of K8s University

## Introduction: Without Networking, Even the Best Dorms Are Islands

Hey there, today we're diving into CNI plugins - the unsung heroes working behind the scenes. You might use kubectl every day and check on Pods constantly, but have you ever wondered: How do these Pods actually talk to each other? How does external traffic get in?

Picture this: What if the dorm buildings at K8s University had zero network connectivity:
- ğŸ  No matter how fancy the dorms, students couldn't reach each other
- ğŸ“± No WiFi means no food delivery, no package pickup
- ğŸ® No gaming, no streaming, no homework submissions

That's exactly what a K8s cluster without CNI plugins looks like - a bunch of "digital islands"!

## 1ï¸âƒ£ What Are CNI Plugins? The "Network Providers" of K8s University

### CNI: Container Network Interface
Just like you need to choose an internet service provider for your home, K8s clusters need to pick a CNI plugin for network services:
- Network coverage: Connecting every dorm room (Pod) to the network
- IP allocation: Assigning unique network addresses to each student
- Data transmission: Ensuring messages get delivered between rooms correctly
- Service quality: Providing different tiers of network service

ğŸ’¡ Key insight: You need to choose from multiple providers to find the one that best fits your needs. Each CNI plugin has its own strengths and features, just like different ISPs offer different packages and service quality.

### How It Works
Let's say kubelet, just like the facilities manager at K8s University, kicks off the story:
```bash
# Step 1: When a Pod gets created
kubelet: "Hey CNI plugin, set up networking for this new Pod!"
CNI plugin: "Got it! Allocating IP and configuring network..."

# Step 2: Network configuration
CNI plugin: "IP assigned: 10.244.1.100"
CNI plugin: "Routing table updated, network interface configured"

# Step 3: When Pod gets deleted
kubelet: "This Pod's gotta go, clean up the network resources"
CNI plugin: "Roger that! Reclaiming IP and cleaning up routes..."
```

## 2ï¸âƒ£ How to Check CNI Components in Your Cluster: Network Engineer's "Health Report" ğŸ”

### Basic Health Check: Is the CNI Plugin Still Alive?
```bash
# 1. Check CNI-related Pod status
kubectl get pods -n kube-system | grep -E "(flannel|calico|cilium|weave|canal)"

# Normal output example (using Calico):
NAME                                      READY   STATUS    RESTARTS        AGE
calico-kube-controllers-7bb4b4d4d-6w29j   1/1     Running   1 (3m50s ago)   37h
canal-ck4gg                               2/2     Running   2 (3m50s ago)   37h
# - calico-kube-controllers: Calico's "brain", handles policy management and global coordination
# - canal-xxx: Canal is the "executor", actually implements network config on each node (DaemonSet)
#   Canal = Calico (policy features) + Flannel (network implementation) combo

# 2. Check CNI plugin details
kubectl describe pod -n kube-system calico-kube-controllers-7bb4b4d4d-6w29j
kubectl describe pod -n kube-system canal-ck4gg

# 3. Check CNI configuration files
ls -la /etc/cni/net.d/
cat /etc/cni/net.d/10-canal.conflist  # Canal config example
```

### Deep Dive Diagnostics: Check CNI Plugin Logs
```bash
# 1. Check Calico controller logs
kubectl logs -n kube-system calico-kube-controllers-7bb4b4d4d-6w29j -f

# 2. Check Canal Pod logs
# First check Canal Pod container structure:
kubectl get pod -n kube-system canal-ck4gg -o jsonpath='{.spec.containers[*].name}'
# Example output: might be "calico-node" or "canal" etc.

# View Canal Pod logs (based on actual container name)
kubectl logs -n kube-system canal-ck4gg -f
# If you get multiple container errors, specify container name:
# kubectl logs -n kube-system canal-ck4gg -c calico-node -f

# Check container details:
kubectl describe pod -n kube-system canal-ck4gg | grep -A 10 "Containers:"

# 3. Check recent logs (last 100 lines)
kubectl logs -n kube-system calico-kube-controllers-7bb4b4d4d-6w29j --tail=100

# 4. Check all Calico-related Pod logs
kubectl logs -n kube-system -l k8s-app=calico-kube-controllers --tail=50
kubectl logs -n kube-system -l k8s-app=canal --tail=50
```

### Typical Log Content Breakdown:
```bash
âœ… Normal working logs (Calico controller example):
# Calico controller normal startup logs
2024-01-20 10:30:15.123 [INFO][1] main.go 123: Starting Calico kube-controllers version v3.26.1
2024-01-20 10:30:15.234 [INFO][1] main.go 145: Loaded configuration from environment config=&config.Config{...}
2024-01-20 10:30:15.345 [INFO][1] main.go 167: Ensuring Calico datastore is initialized
2024-01-20 10:30:15.456 [INFO][1] client.go 234: Initializing CalicoClient
2024-01-20 10:30:15.567 [INFO][1] controllers.go 89: Starting policy controller
2024-01-20 10:30:15.678 [INFO][1] controllers.go 123: Policy controller is now in sync
# Canal Pod normal logs (calico-node container):
2024-01-20 10:30:16.123 [INFO][1] startup/startup.go 456: Early log level set to info
2024-01-20 10:30:16.234 [INFO][1] startup/startup.go 478: Using NODENAME environment for node name: controlplane
2024-01-20 10:30:16.345 [INFO][1] startup/startup.go 567: Determined node IP: 192.168.1.100
2024-01-20 10:30:16.456 [INFO][1] startup/startup.go 678: Node IPv4 changed, will check for conflicts
2024-01-20 10:30:16.567 [INFO][1] startup/startup.go 789: Calico node started successfully

âŒ Problematic log examples:
# API Server connection failure
2024-01-20 10:30:15.123 [ERROR][1] client.go 234: Failed to create Calico client: Get "https://10.96.0.1:443/api/v1": dial tcp 10.96.0.1:443: connect: connection refused

# Node network configuration issues
2024-01-20 10:30:16.234 [ERROR][1] startup.go 456: Failed to auto-detect an IPv4 address: no valid IPv4 addresses found on the host interfaces

# IPAM allocation failure
2024-01-20 10:30:17.345 [ERROR][1] ipam.go 123: Failed to allocate IP address: IPAM block affinity changed
```

### Network Interface Check: CNI-Created "Network Pipes"

# 1. Check node network interfaces (Calico/Canal environment)
```bash
ip addr show: IP address breakdown

1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
# â†‘ Local loopback interface, every system has this for internal communication
    inet 127.0.0.1/8 scope host lo
    ...

2: enp1s0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
# â†‘ Node's main network card, connects to external network, IP: 172.30.1.2/24
    inet 172.30.1.2/24 brd 172.30.1.255 scope global dynamic noprefixroute enp1s0
    ...

3: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1454 qdisc noqueue state DOWN group default
# â†‘ Docker bridge, DOWN state means K8s environment doesn't use Docker networking (this is normal)
    inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0
    ...

4: flannel.1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue state UNKNOWN group default
# â†‘ Flannel VXLAN tunnel interface, used for cross-node Pod communication, IP: 192.168.0.0/32
    inet 192.168.0.0/32 brd 192.168.0.0 scope global flannel.1
    ...

7: cali61f99eae10e@if3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default
# â†‘ Pod1's Calico interface (Pod IP: 192.168.0.2) - each Pod gets unique IP for communication
    link-netns cni-842152e1-c862-1447-5370-39a4c62c7ae9
    ...

8: cali6dac6e169cb@if3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default
# â†‘ Pod2's Calico interface (Pod IP: 192.168.0.3) - another Pod's independent IP
    link-netns cni-12d50b99-d658-eb60-0e74-32229c0e8782
    ...

9: calic45561273ca@if3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default
# â†‘ Pod3's Calico interface (Pod IP: 192.168.0.4) - third Pod's independent IP
    link-netns cni-d4a8391a-1a8a-a19a-5d51-8ce7ad38a87d
    ...

10: cali0c6ec79a9d0@if3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default
# â†‘ Pod4's Calico interface (Pod IP: 192.168.0.5) - fourth Pod's independent IP
    link-netns cni-84901d58-f898-6701-276e-d6c8c55afc27
    ...
```
ğŸ’¡ Key point: Each Pod gets a unique IP address (like each student having their own dorm room number) for cluster-internal communication
- Pods can directly access each other via IP: 192.168.0.2 â†” 192.168.0.3
- These are cluster-internal IPs, not accessible from outside
- mtu 1500 indicates Maximum Transmission Unit of 1500 bytes, which is the Ethernet standard


# 2. Check routing table
```bash
ip route show: routing table breakdown

default via 172.30.1.1 dev enp1s0 proto dhcp src 172.30.1.2 metric 1002 mtu 1500
# â†‘ Default route, all external traffic goes out through main network card enp1s0

172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 linkdown
# â†‘ Docker network route, linkdown means inactive (normal in K8s environment)

172.30.1.0/24 dev enp1s0 proto dhcp scope link src 172.30.1.2 metric 1002 mtu 1500
# â†‘ Local network segment route, same-segment communication goes directly through main network card

192.168.0.2 dev cali61f99eae10e scope link
192.168.0.3 dev cali6dac6e169cb scope link
192.168.0.4 dev calic45561273ca scope link
192.168.0.5 dev cali0c6ec79a9d0 scope link
# â†‘ Pod IP routes: each Pod has a point-to-point route to its corresponding cali interface
# This is Calico's specialty: direct routing, no bridge forwarding needed, better performance
```

# 3. Check Calico-specific network interfaces
```bash
ip link show | grep cali

# Calico interface naming rules breakdown:
7: cali61f99eae10e@if3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP mode DEFAULT group default
# â†‘ cali + 11 random characters + @if3 (corresponds to eth0 interface inside Pod)
8: cali6dac6e169cb@if3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP mode DEFAULT group default
9: calic45561273ca@if3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP mode DEFAULT group default
10: cali0c6ec79a9d0@if3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP mode DEFAULT group default
# Each interface is in UP state, indicating the corresponding Pod is running
```

# 4. Check CNI bridges
```bash
brctl show  # or ip link show type bridge

# Bridge status breakdown:
bridge name     bridge id               STP enabled     interfaces
docker0         8000.22802df1db19       no
# â†‘ Only docker0 bridge, no connected interfaces (normal, because K8s doesn't use Docker networking)
# Note: Calico uses point-to-point routing, doesn't need traditional bridges - that's one of its advantages
```

### Pod Network Connection Check: Making Sure "Dorm Networking" Works
```bash
# 1. Check Pod network configuration
kubectl exec -it <pod-name> -- ip addr show
# Should see eth0 interface, each Pod has unique IP in 192.168.0.x range

# 2. Test Pod-to-Pod network connectivity (direct Pod IP communication)
kubectl exec -it <pod1> -- ping <pod2-ip>
# Example: kubectl exec -it test-pod -- ping 192.168.0.3
# This proves Pods can communicate directly via IP without extra configuration

# 3. Test Pod-to-Service connectivity
kubectl exec -it <pod-name> -- nslookup kubernetes.default.svc.cluster.local
# Verify DNS resolution and Service discovery work properly

# 4. Check Pod's network namespace
kubectl exec -it <pod-name> -- cat /proc/net/route
# Check Pod's internal routing table, default route should point to gateway

# 5. Verify Pod external network connectivity
kubectl exec -it <pod-name> -- ping 8.8.8.8
# Test if Pod can access external networks
```


### System-Level Network Check: Underlying "Infrastructure" Status
```bash
# 1. Check iptables rules (CNI plugins create related rules)
iptables -t nat -L -n | grep -E "(KUBE|CNI)"
iptables -t filter -L -n | grep -E "(KUBE|CNI)"

# 2. Check kernel modules (some CNIs need specific modules)
lsmod | grep -E "(vxlan|ipip|wireguard)"

# 3. Check system network configuration
sysctl net.bridge.bridge-nf-call-iptables
sysctl net.ipv4.ip_forward

# Both values should be 1, otherwise CNI might not work properly
```

## 3ï¸âƒ£ Major CNI Plugin Showdown: Network Provider Selection Guide ğŸ“¡

### ğŸ† Flannel: Beginner-Friendly Entry Choice
âœ… Simple configuration: Just a few lines of YAML, perfect for K8s newbies
âœ… Rock solid: Battle-tested, production-ready reliability
âœ… Low resource usage: Doesn't hog memory, perfect for small clusters
âŒ Limited features: Weak network policy support
âŒ Average performance: Might struggle with large-scale clusters

Use cases: Learning environments, small clusters, when you want simple and stable

### ğŸš€ Calico: Full-Featured Enterprise Choice
âœ… Excellent performance: Pure Layer 3 networking, near-native performance
âœ… Security powerhouse: Built-in network policies, solid security protection
âœ… Great scalability: Supports large clusters (1000+ nodes)
âœ… Feature-rich: BGP routing, IPAM, service mesh integration
âŒ Complex configuration: More features mean more config options
âŒ Steep learning curve: Requires some networking background

Use cases: Production environments, large clusters, security requirements

### âš¡ Cilium: Next-Gen Tech Pioneer
âœ… Cutting-edge tech: eBPF kernel technology, mind-blowing performance
âœ… Strong observability: Network traffic visualization, troubleshooting powerhouse
âœ… Enhanced security: API-level security policies
âœ… Service mesh: Built-in Service Mesh functionality
âŒ High kernel requirements: Needs newer Linux kernels
âŒ High complexity: Powerful but steep learning curve

Use cases: Cloud-native environments, microservice architectures, performance-focused

### ğŸ”— Weave Net: Smart Automation Choice
âœ… Zero configuration: Auto-discovers nodes, auto-establishes network
âœ… Encrypted communication: Built-in network encryption, great security
âœ… Self-healing: Network partitions auto-recover
âŒ Performance overhead: Encryption and automation come with performance costs
âŒ Hard to debug: High automation makes troubleshooting tricky

Use cases: Multi-cloud environments, unstable network scenarios, automation-focused


## 4ï¸âƒ£ Common Troubleshooting: Network Provider Issue Resolution ğŸ”§

### Issue 1: Pods Can't Communicate with Each Other
```bash
# Symptom: Can't ping other Pods
kubectl exec -it pod1 -- ping <pod2-ip>

# Troubleshooting steps
1. Check CNI plugin status
kubectl get pods -n kube-system | grep -E "(flannel|calico|cilium|weave)"

2. Check network configuration
kubectl get nodes -o wide
ip route show

3. Check firewall rules
iptables -L -n
```

### Issue 2: External Access to Service Fails
```bash
# Symptom: Service created but can't access it
kubectl get svc

# Troubleshooting steps
1. Check Service configuration
kubectl describe svc <service-name>

2. Check Endpoints
kubectl get endpoints <service-name>

3. Check kube-proxy
kubectl get pods -n kube-system | grep kube-proxy
kubectl logs -n kube-system <kube-proxy-pod>
```

### Issue 3: Poor Network Performance
```bash
# Symptom: High network latency, low throughput

# Performance testing
kubectl run test-pod --image=nicolaka/netshoot -it --rm
# Execute inside Pod
iperf3 -c <target-ip>

# Investigation directions
1. CNI plugin performance characteristics (Overlay vs Native)
2. Too many network policy rules
3. Node network configuration issues
```

## ğŸ¯ Summary
CNI plugins are like the "network providers" of K8s University - they might work behind the scenes, but without them, your whole cluster would be a scattered mess! Pick the right CNI plugin, and your K8s cluster gets solid "network infrastructure" - Pods can chat happily, Services work properly, and you can clock out with peace of mind!
