# kube-proxyï¼šKubernetesç½‘ç»œçš„"éšå½¢å¿«é€’å‘˜"ä¸SREçš„"ç‰©æµç›‘æ§" ğŸ“¦

å¤§å®¶å¥½ï¼ä¹‹å‰èŠè¿‡äº†ä»£ç†çš„è¯é¢˜ï¼Œä»Šå¤©å’±ä»¬æ¥èŠèŠKubernetesç”Ÿæ€ä¸­ä¸€ä¸ªä½è°ƒä½†æå…¶é‡è¦çš„ç»„ä»¶â€”â€”kube-proxyï¼ä½œä¸ºK8Sçš„ä¸“ç”¨ä»£ç†ä¸“å‘˜ï¼Œå®ƒé»˜é»˜åœ°åœ¨æ¯ä¸ªèŠ‚ç‚¹ä¸Šå·¥ä½œï¼Œç¡®ä¿ä½ çš„Podèƒ½å¤Ÿæ­£ç¡®æ”¶åˆ°"åŒ…è£¹"ï¼ˆç½‘ç»œæµé‡ï¼‰ã€‚ä½œä¸ºSREï¼Œæˆ‘ä»¬éœ€è¦äº†è§£å®ƒçš„å·¥ä½œæœºåˆ¶ï¼Œç¡®ä¿å¼‚å¸¸éƒ½èƒ½è¢«åŠæ—¶å‘ç°å’Œå¤„ç†ï¼

## 1ï¸âƒ£ kube-proxyçš„"å¤šé‡èº«ä»½"ï¼šä»é‚®é€’å‘˜åˆ°äº¤é€šæŒ‡æŒ¥å®˜ ğŸš¦

kube-proxyåœ¨Kubernetesé›†ç¾¤ä¸­æ‰®æ¼”ç€å¤šä¸ªå…³é”®è§’è‰²ï¼š

### ğŸšš èº«ä»½ä¸€ï¼šServiceæµé‡"å¿«é€’å‘˜"ï¼ˆService Traffic Forwarderï¼‰
æƒ³è±¡ä¸€ä¸‹ï¼Œkube-proxyå°±åƒæ˜¯K8Så¤§å­¦çš„ä¸“å±å¿«é€’å‘˜ã€‚ä½ è¦å¯„é€ä¸šåŠ¡è¯·æ±‚ï¼ˆåŒ…è£¹ï¼‰åˆ°æŸä¸ªServiceï¼ˆå¤§å­¦é—¨ç‰Œå·ï¼‰ï¼Œä½†å®é™…å¤„ç†è¯·æ±‚çš„Podï¼ˆå…±äº«å®¿èˆï¼‰åˆ†å¸ƒåœ¨ä¸åŒçš„æ¥¼æ ‹ï¼ˆNodeï¼‰é‡Œã€‚kube-proxyå°±æ˜¯é‚£ä¸ªçŸ¥é“æ¯ä¸ªä¸šåŠ¡è¯·æ±‚è¯¥é€åˆ°å“ªä¸ªå®¿èˆçš„å¿«é€’å‘˜ï¼

å¿«é€’å‘˜å‹kube-proxyçš„æ ¸å¿ƒä»·å€¼ï¼š
- æœåŠ¡å‘ç°ï¼šçŸ¥é“æ¯ä¸ªServiceèƒŒåæœ‰å“ªäº›Podå®¿èˆï¼Œå°±åƒå¿«é€’å‘˜ç†Ÿæ‚‰å°åŒºæ¯æ ‹æ¥¼çš„æ¯ä¸ªå®¿èˆ
- è´Ÿè½½å‡è¡¡ï¼šæŠŠä¸šåŠ¡è¯·æ±‚å‡åŒ€åˆ†å‘ç»™å¤šä¸ªPodå®¿èˆï¼Œå°±åƒå¿«é€’å‘˜åˆç†å®‰æ’é€è´§è·¯çº¿
- å¥åº·æ£€æŸ¥ï¼šåªå‘å¥åº·çš„Podå®¿èˆå‘é€è¯·æ±‚ï¼Œå°±åƒå¿«é€’å‘˜ä¸å¾€ç©ºå®¿èˆé€åŒ…è£¹
- ä¼šè¯ä¿æŒï¼šæ”¯æŒä¼šè¯äº²å’Œæ€§ï¼Œå°±åƒVIPå®¢æˆ·çš„è¯·æ±‚æ€»æ˜¯é€åˆ°åŒä¸€ä¸ªå®¿èˆå¤„ç†

### ğŸš¦ èº«ä»½äºŒï¼šç½‘ç»œæµé‡"äº¤é€šæŒ‡æŒ¥å®˜"ï¼ˆNetwork Traffic Controllerï¼‰
kube-proxyè¿˜åƒæ˜¯ç½‘ç»œä¸–ç•Œçš„äº¤é€šè­¦å¯Ÿï¼Œåœ¨æ¯ä¸ªè·¯å£ï¼ˆNodeï¼‰æŒ‡æŒ¥äº¤é€šï¼š

- è·¯ç”±è§„åˆ™ç®¡ç†ï¼šç»´æŠ¤iptables/ipvsè§„åˆ™ï¼Œå°±åƒè®¾ç½®äº¤é€šä¿¡å·ç¯
- æµé‡è½¬å‘ï¼šç¡®ä¿æ•°æ®åŒ…èµ°æ­£ç¡®çš„è·¯å¾„ï¼Œå°±åƒæŒ‡æŒ¥è½¦è¾†èµ°æ­£ç¡®è½¦é“
- ç«¯å£æ˜ å°„ï¼šå°†Serviceç«¯å£æ˜ å°„åˆ°Podç«¯å£ï¼Œå°±åƒè®¾ç½®ä¸“ç”¨é€šé“
- ç½‘ç»œç­–ç•¥æ‰§è¡Œï¼šå®æ–½è®¿é—®æ§åˆ¶ï¼Œå°±åƒæ£€æŸ¥é€šè¡Œè¯

### ğŸ”„ èº«ä»½ä¸‰ï¼šé›†ç¾¤çŠ¶æ€"åŒæ­¥å‘˜"ï¼ˆCluster State Synchronizerï¼‰
kube-proxyé€šè¿‡watch API Serveræ¥ä¿æŒçŠ¶æ€åŒæ­¥ï¼š

- å®æ—¶ç›‘å¬ï¼šç›‘å¬Serviceå’ŒEndpointså˜åŒ–ï¼Œå°±åƒå¿«é€’å‘˜å®æ—¶æ¥æ”¶æ´¾é€ä»»åŠ¡
- è§„åˆ™æ›´æ–°ï¼šåŠ¨æ€æ›´æ–°è½¬å‘è§„åˆ™ï¼Œå°±åƒäº¤é€šè­¦å¯Ÿæ ¹æ®è·¯å†µè°ƒæ•´ä¿¡å·ç¯
- æ•…éšœæ¢å¤ï¼šPodæ•…éšœæ—¶è‡ªåŠ¨ç§»é™¤è½¬å‘è§„åˆ™ï¼Œå°±åƒç»•è¿‡å µå¡è·¯æ®µ

å®é™…åº”ç”¨åœºæ™¯å…¨æ™¯å›¾ï¼š
```
åœºæ™¯1ï¼šå¤–éƒ¨ç”¨æˆ·ä¸šåŠ¡è¯·æ±‚ â†’ LoadBalancer Service â†’ kube-proxy â†’ å¤„ç†è¯·æ±‚çš„Podå®¿èˆ
åœºæ™¯2ï¼šå†…éƒ¨Podå‘èµ·è¯·æ±‚ â†’ ClusterIP Service â†’ kube-proxy â†’ ç›®æ ‡Podå®¿èˆ
åœºæ™¯3ï¼šNodePortæš´éœ²è¯·æ±‚ â†’ kube-proxy â†’ å¤„ç†è¯·æ±‚çš„Podå®¿èˆ
åœºæ™¯4ï¼šHeadless Service â†’ kube-proxy â†’ ç›´æ¥æ‰¾åˆ°Podå®¿èˆIP
```

## 2ï¸âƒ£ kube-proxyçš„"å·¥ä½œæ¨¡å¼"ï¼šä»ä¼ ç»Ÿé‚®å±€åˆ°ç°ä»£ç‰©æµ ğŸ“®

kube-proxyæœ‰å››ç§ä¸»è¦å·¥ä½œæ¨¡å¼ï¼Œå°±åƒå¿«é€’è¡Œä¸šçš„å‘å±•å†ç¨‹ï¼š

### ğŸŒ userspaceæ¨¡å¼ï¼šä¼ ç»Ÿ"é‚®å±€æ¨¡å¼"ï¼ˆå·²åºŸå¼ƒï¼‰
```bash
# å·¥ä½œæµç¨‹ï¼š
å®¢æˆ·ç«¯ â†’ iptables â†’ kube-proxyè¿›ç¨‹ â†’ Pod
```
- ç‰¹ç‚¹ï¼šæ‰€æœ‰æµé‡éƒ½ç»è¿‡kube-proxyè¿›ç¨‹å¤„ç†
- é—®é¢˜ï¼šæ€§èƒ½å·®ï¼Œå•ç‚¹ç“¶é¢ˆï¼Œå°±åƒæ‰€æœ‰åŒ…è£¹éƒ½è¦ç»è¿‡é‚®å±€ä¸­è½¬
- çŠ¶æ€ï¼šå·²åŸºæœ¬åºŸå¼ƒï¼Œåªåœ¨è€ç‰ˆæœ¬ä¸­å­˜åœ¨

### âš¡ iptablesæ¨¡å¼ï¼šç°ä»£"ç›´è¾¾å¿«é€’"ï¼ˆé»˜è®¤æ¨¡å¼ï¼‰
```bash
# å·¥ä½œæµç¨‹ï¼š
å®¢æˆ·ç«¯ â†’ iptablesè§„åˆ™ â†’ ç›´æ¥è½¬å‘åˆ°Pod
```
- ç‰¹ç‚¹ï¼škube-proxyåªè´Ÿè´£ç»´æŠ¤iptablesè§„åˆ™ï¼Œä¸å¤„ç†æ•°æ®åŒ…
- ä¼˜åŠ¿ï¼šæ€§èƒ½å¥½ï¼Œå»¶è¿Ÿä½ï¼Œå°±åƒå¿«é€’ç›´æ¥é€åˆ°é—¨å£
- ç¼ºç‚¹ï¼šè§„åˆ™æ•°é‡å¤šæ—¶æ€§èƒ½ä¸‹é™ï¼Œè´Ÿè½½å‡è¡¡ç®—æ³•ç®€å•

### ğŸš€ ipvsæ¨¡å¼ï¼šé«˜æ€§èƒ½"æ™ºèƒ½ç‰©æµ"ï¼ˆæ¨èæ¨¡å¼ï¼‰
```bash
# å·¥ä½œæµç¨‹ï¼š
å®¢æˆ·ç«¯ â†’ IPVSè§„åˆ™ â†’ é«˜æ•ˆè½¬å‘åˆ°Pod
```
- ç‰¹ç‚¹ï¼šåŸºäºå†…æ ¸IPVSæ¨¡å—ï¼Œæ€§èƒ½æ›´å¼º
- **kube-proxyçš„å·¥ä½œ**ï¼šç›‘å¬API Serverï¼Œå°†Serviceå’ŒEndpointsä¿¡æ¯è½¬æ¢ä¸ºIPVSè§„åˆ™ï¼Œé…ç½®åˆ°å†…æ ¸IPVSæ¨¡å—ä¸­
- ä¼˜åŠ¿ï¼šæ”¯æŒæ›´å¤šè´Ÿè½½å‡è¡¡ç®—æ³•ï¼Œè§„åˆ™æŸ¥æ‰¾æ•ˆç‡é«˜
- é€‚ç”¨ï¼šå¤§è§„æ¨¡é›†ç¾¤ï¼Œé«˜å¹¶å‘åœºæ™¯

### ğŸ”¥ nftablesæ¨¡å¼ï¼šæ–°ä¸€ä»£"è¶…çº§ç‰©æµ"ï¼ˆå®éªŒæ€§ï¼‰
```bash
# å·¥ä½œæµç¨‹ï¼š
å®¢æˆ·ç«¯ â†’ nftablesè§„åˆ™ â†’ ç°ä»£åŒ–è½¬å‘åˆ°Pod
```
- ç‰¹ç‚¹ï¼šåŸºäºæ–°ä¸€ä»£nftablesæ¡†æ¶ï¼Œæ›¿ä»£ä¼ ç»Ÿiptables
- **kube-proxyçš„å·¥ä½œ**ï¼šå°†Serviceè§„åˆ™è½¬æ¢ä¸ºnftablesè§„åˆ™ï¼Œåˆ©ç”¨ç°ä»£å†…æ ¸ç‰¹æ€§
- ä¼˜åŠ¿ï¼šæ›´å¥½çš„æ€§èƒ½ï¼Œæ›´æ¸…æ™°çš„è§„åˆ™ç»“æ„ï¼Œæ›´å¥½çš„IPv6æ”¯æŒ
- é€‚ç”¨ï¼šè¾ƒæ–°çš„å†…æ ¸ç‰ˆæœ¬ï¼ˆLinux 4.17+ï¼‰ï¼Œå®éªŒæ€§åŠŸèƒ½

æ¨¡å¼å¯¹æ¯”ï¼šå¿«é€’è¡Œä¸šçš„è¿›åŒ–å²
```bash
# userspaceæ¨¡å¼ = ä¼ ç»Ÿé‚®å±€
æ‰€æœ‰åŒ…è£¹ â†’ é‚®å±€åˆ†æ‹£ â†’ å†æ´¾é€ï¼ˆæ…¢ï¼Œä½†å¯é ï¼‰
kube-proxyï¼šäº²è‡ªå¤„ç†æ¯ä¸ªåŒ…è£¹ï¼Œæˆä¸ºæ€§èƒ½ç“¶é¢ˆ

# iptablesæ¨¡å¼ = ç°ä»£å¿«é€’
åŒ…è£¹ â†’ æ™ºèƒ½åˆ†æ‹£ç³»ç»Ÿ â†’ ç›´æ¥æ´¾é€ï¼ˆå¿«ï¼Œä½†è§„åˆ™å¤æ‚ï¼‰
kube-proxyï¼šåªè´Ÿè´£æ›´æ–°åˆ†æ‹£è§„åˆ™ï¼Œä¸ç¢°åŒ…è£¹

# ipvsæ¨¡å¼ = æ™ºèƒ½ç‰©æµ
åŒ…è£¹ â†’ AIæ™ºèƒ½è°ƒåº¦ â†’ æœ€ä¼˜è·¯å¾„æ´¾é€ï¼ˆåˆå¿«åˆæ™ºèƒ½ï¼‰
kube-proxyï¼šé…ç½®æ™ºèƒ½è°ƒåº¦ç³»ç»Ÿï¼Œè®©å†…æ ¸é«˜æ•ˆå¤„ç†

# nftablesæ¨¡å¼ = æœªæ¥ç‰©æµ
åŒ…è£¹ â†’ æ–°ä¸€ä»£æ™ºèƒ½ç³»ç»Ÿ â†’ è¶…é«˜æ•ˆæ´¾é€ï¼ˆæœ€æ–°æŠ€æœ¯ï¼Œå®éªŒé˜¶æ®µï¼‰
kube-proxyï¼šä½¿ç”¨æœ€æ–°çš„å†…æ ¸æŠ€æœ¯ï¼Œæ›´å¥½çš„æ€§èƒ½å’Œå¯ç»´æŠ¤æ€§
```

## 3ï¸âƒ£ kube-proxyçš„"ç¿»è½¦ç°åœº"ï¼šå½“å¿«é€’å‘˜å‡ºé—®é¢˜ ğŸ’¥

æ— è®ºkube-proxyé‡‡ç”¨å“ªç§æ¨¡å¼ï¼Œéƒ½å¯èƒ½é‡åˆ°å„ç§"ç¿»è½¦"æƒ…å†µï¼š

### âš¡ iptablesæ¨¡å¼çš„"å‘"ï¼š

**è§„åˆ™çˆ†ç‚¸** - "å¿«é€’å•å¤ªå¤šï¼Œç³»ç»Ÿå´©æºƒ"ï¼š
```bash
# å¤§è§„æ¨¡é›†ç¾¤ä¸­iptablesè§„åˆ™æ•°é‡
Services: 1000ä¸ª
æ¯ä¸ªServiceå¹³å‡Endpoints: 10ä¸ª
æ€»è§„åˆ™æ•°: 1000 Ã— 10 Ã— å¤šæ¡è§„åˆ™ = æ•°ä¸‡æ¡è§„åˆ™
# ç»“æœï¼šè§„åˆ™æŸ¥æ‰¾å˜æ…¢ï¼Œç½‘ç»œå»¶è¿Ÿå¢åŠ 
```

**è´Ÿè½½å‡è¡¡ä¸å‡** - "å¿«é€’å‘˜åå¿ƒï¼Œæ€»å¾€ä¸€ä¸ªåœ°æ–¹é€"ï¼š
```bash
# iptablesä½¿ç”¨éšæœºç®—æ³•ï¼Œå¯èƒ½å‡ºç°åˆ†å¸ƒä¸å‡
Pod A: æ”¶åˆ°70%æµé‡
Pod B: æ”¶åˆ°20%æµé‡  
Pod C: æ”¶åˆ°10%æµé‡
# ç»“æœï¼šæŸäº›Podè¿‡è½½ï¼ŒæŸäº›Podç©ºé—²
```

**æ•…éšœæ¢å¤æ…¢** - "å¿«é€’å‘˜ååº”è¿Ÿé’"ï¼š
```bash
# Podæ•…éšœåï¼Œiptablesè§„åˆ™æ›´æ–°å»¶è¿Ÿ
Podæ•…éšœ â†’ API Serveræ„ŸçŸ¥ â†’ kube-proxyæ›´æ–°è§„åˆ™
# å»¶è¿Ÿï¼šå¯èƒ½éœ€è¦å‡ ç§’åˆ°å‡ åç§’
# ç»“æœï¼šæ•…éšœæœŸé—´æµé‡ä»ç„¶è½¬å‘åˆ°æ•…éšœPod
```

### ğŸš€ ipvsæ¨¡å¼çš„"å‘"ï¼š

**å†…æ ¸æ¨¡å—ä¾èµ–** - "æ™ºèƒ½è°ƒåº¦ç³»ç»Ÿéœ€è¦ç‰¹æ®Šç¡¬ä»¶"ï¼š
```bash
# kube-proxyéœ€è¦ç¡®ä¿IPVSå†…æ ¸æ¨¡å—å·²åŠ è½½
modprobe ip_vs
modprobe ip_vs_rr      # è½®è¯¢ç®—æ³•
modprobe ip_vs_wrr     # åŠ æƒè½®è¯¢ç®—æ³•
modprobe ip_vs_sh      # æºåœ°å€å“ˆå¸Œç®—æ³•
# é—®é¢˜ï¼šæŸäº›ç¯å¢ƒå¯èƒ½ä¸æ”¯æŒæˆ–æœªå®‰è£…
# kube-proxyå¯åŠ¨æ—¶ä¼šæ£€æŸ¥è¿™äº›æ¨¡å—ï¼Œç¼ºå¤±æ—¶ä¼šæŠ¥é”™
```

**è°ƒè¯•å›°éš¾** - "æ™ºèƒ½ç³»ç»Ÿé»‘ç›’åŒ–"ï¼š
```bash
# iptablesè§„åˆ™å¯ä»¥ç›´æ¥æŸ¥çœ‹
iptables -t nat -L

# IPVSè§„åˆ™éœ€è¦ä¸“é—¨å·¥å…·ï¼ˆå¯èƒ½éœ€è¦å®‰è£…ï¼‰
ipvsadm -L -n
# é—®é¢˜1ï¼šè°ƒè¯•å·¥å…·ä¸å¦‚iptablesæ™®åŠï¼Œéœ€è¦é¢å¤–å®‰è£…
# é—®é¢˜2ï¼šå¦‚æœipvsadmæœªå®‰è£…ï¼Œæ’æŸ¥é—®é¢˜ä¼šæ¯”è¾ƒå›°éš¾

# æ›¿ä»£æ£€æŸ¥æ–¹æ³•ï¼ˆæ— éœ€é¢å¤–å·¥å…·ï¼‰
cat /proc/net/ip_vs                    # æŸ¥çœ‹IPVSè™šæ‹ŸæœåŠ¡å™¨çŠ¶æ€
cat /proc/net/ip_vs_stats              # æŸ¥çœ‹IPVSç»Ÿè®¡ä¿¡æ¯
```

### ğŸ”„ é€šç”¨é—®é¢˜ï¼š

**ç½‘ç»œåˆ†åŒº** - "å¿«é€’å‘˜è¿·è·¯äº†"ï¼š
```bash
# èŠ‚ç‚¹é—´ç½‘ç»œä¸é€š
Node Aä¸Šçš„Pod â†’ æ— æ³•è®¿é—®Node Bä¸Šçš„Service
# ç°è±¡ï¼šéƒ¨åˆ†è¯·æ±‚å¤±è´¥ï¼Œéƒ¨åˆ†æ­£å¸¸
```

**DNSè§£æé—®é¢˜** - "å¿«é€’å‘˜æ‰¾ä¸åˆ°åœ°å€"ï¼š
```bash
# Service DNSè§£æå¤±è´¥
nslookup my-service.default.svc.cluster.local
# ç°è±¡ï¼šæœåŠ¡åæ— æ³•è§£æï¼Œåªèƒ½ç”¨IPè®¿é—®
```

**ç«¯å£å†²çª** - "å¿«é€’åœ°å€é‡å¤"ï¼š
```bash
# NodePortç«¯å£å†²çª
Service A: NodePort 30080
Service B: NodePort 30080  # å†²çªï¼
# ç»“æœï¼šå…¶ä¸­ä¸€ä¸ªServiceæ— æ³•æ­£å¸¸å·¥ä½œ
```

## 4ï¸âƒ£ SREè§†è§’çš„"ç›‘æ§ç—›ç‚¹"ï¼šç»™å¿«é€’å‘˜è£…GPS ğŸ¯

kube-proxyç»™SREå¸¦æ¥äº†ç‹¬ç‰¹çš„ç›‘æ§æŒ‘æˆ˜ï¼š

### ğŸ”„ é€šç”¨ç›‘æ§ç—›ç‚¹ï¼š
- æœåŠ¡å‘ç°å»¶è¿Ÿï¼šEndpointså˜åŒ–åˆ°è§„åˆ™ç”Ÿæ•ˆçš„æ—¶é—´å·®
- ç½‘ç»œè¿é€šæ€§ï¼šè·¨èŠ‚ç‚¹Serviceè®¿é—®çš„ç½‘ç»œè´¨é‡
- æ•…éšœå½±å“èŒƒå›´ï¼šå•ä¸ªkube-proxyæ•…éšœå¯¹é›†ç¾¤çš„å½±å“

### âš¡ iptablesæ¨¡å¼ç›‘æ§ç—›ç‚¹ï¼š
- è§„åˆ™æ•°é‡ç›‘æ§ï¼šå¦‚ä½•ç›‘æ§iptablesè§„åˆ™æ•°é‡å’Œæ€§èƒ½å½±å“ï¼Ÿ
- è´Ÿè½½å‡è¡¡æ•ˆæœï¼šå¦‚ä½•éªŒè¯æµé‡åˆ†å‘æ˜¯å¦å‡åŒ€ï¼Ÿ
- è§„åˆ™åŒæ­¥å»¶è¿Ÿï¼šå¦‚ä½•ç›‘æ§è§„åˆ™æ›´æ–°çš„åŠæ—¶æ€§ï¼Ÿ

### ğŸš€ ipvsæ¨¡å¼ç›‘æ§ç—›ç‚¹ï¼š
- å†…æ ¸æ¨¡å—çŠ¶æ€ï¼šå¦‚ä½•ç›‘æ§IPVSæ¨¡å—æ˜¯å¦æ­£å¸¸å·¥ä½œï¼Ÿ
- è´Ÿè½½å‡è¡¡ç®—æ³•ï¼šå¦‚ä½•éªŒè¯ä¸åŒç®—æ³•çš„æ•ˆæœï¼Ÿ
- è¿æ¥è·Ÿè¸ªï¼šå¦‚ä½•ç›‘æ§è¿æ¥çŠ¶æ€å’Œä¼šè¯ä¿æŒï¼Ÿ

## 5ï¸âƒ£ SREçš„"ç‰©æµç›‘æ§"ï¼šç»™å¿«é€’ç³»ç»Ÿè£…ä¸Šé€è§†é•œ ğŸ”

### å¥åº·æ£€æŸ¥ç­–ç•¥ï¼šå¿«é€’ç³»ç»Ÿçš„"ä½“æ£€æŠ¥å‘Š"

å°±åƒå®šæœŸç»™å¿«é€’å‘˜ä½“æ£€ï¼Œæˆ‘ä»¬ä¹Ÿè¦å®šæœŸæ£€æŸ¥kube-proxyçš„å¥åº·çŠ¶å†µï¼š

```bash
# å¿«é€’å‘˜"èº«ä½“æ£€æŸ¥"
# æ–¹æ³•1ï¼šæ£€æŸ¥kube-proxy PodçŠ¶æ€ï¼ˆç°ä»£K8sé›†ç¾¤å¸¸ç”¨ï¼‰
kubectl get pods -n kube-system -l k8s-app=kube-proxy
kubectl describe pods -n kube-system -l k8s-app=kube-proxy

# æ–¹æ³•2ï¼šæ£€æŸ¥systemdæœåŠ¡ï¼ˆä¼ ç»Ÿéƒ¨ç½²æ–¹å¼ï¼‰
systemctl status kube-proxy
# æ³¨æ„ï¼šå¾ˆå¤šç°ä»£K8sé›†ç¾¤ä¸­kube-proxyä»¥Podå½¢å¼è¿è¡Œï¼Œä¸æ˜¯systemdæœåŠ¡

# å¿«é€’å‘˜"è·¯çº¿ç†Ÿæ‚‰åº¦æ£€æŸ¥"
iptables -t nat -L | grep -c KUBE-SVC          # ä¼ ç»Ÿå¿«é€’å‘˜ï¼šæ•°æ•°è®°ä½äº†å¤šå°‘æ¡è·¯çº¿
# æ£€æŸ¥å½“å‰ä½¿ç”¨çš„æ˜¯å“ªç§æ¨¡å¼
kubectl logs -n kube-system -l k8s-app=kube-proxy | grep "Using.*proxy"

# æ£€æŸ¥ConfigMapä¸­çš„æ¨¡å¼é…ç½®
kubectl get cm -n kube-system kube-proxy -o yaml | grep -A5 -B5 "mode:"
# å¦‚æœmode: ""ï¼ˆç©ºå€¼ï¼‰ï¼Œåˆ™ä½¿ç”¨é»˜è®¤æ¨¡å¼ï¼ˆé€šå¸¸æ˜¯iptablesï¼‰

# å¦‚æœæ˜¯ipvsæ¨¡å¼ï¼Œéœ€è¦å…ˆå®‰è£…ipvsadmå·¥å…·
apt install ipvsadm                            # Ubuntu/Debianç³»ç»Ÿ
yum install ipvsadm                            # CentOS/RHELç³»ç»Ÿ
ipvsadm -L -n | grep -c TCP                    # ç°ä»£å¿«é€’å‘˜ï¼šæ£€æŸ¥æ™ºèƒ½å¯¼èˆªç³»ç»Ÿæœ‰å¤šå°‘è·¯çº¿

# å¦‚æœipvsadmæœªå®‰è£…ï¼Œå¯ä»¥é€šè¿‡å…¶ä»–æ–¹å¼æ£€æŸ¥
cat /proc/net/ip_vs                            # ç›´æ¥æŸ¥çœ‹å†…æ ¸IPVSçŠ¶æ€
ls /sys/module/ip_vs*/                         # æ£€æŸ¥IPVSæ¨¡å—æ˜¯å¦åŠ è½½

# å¿«é€’ç½‘ç»œ"è¿é€šæ€§å·¡æ£€"
kubectl get endpoints                           # æ£€æŸ¥å“ªäº›æ”¶è´§åœ°å€è¿˜èƒ½æ­£å¸¸æ”¶è´§
curl -I http://service-name:port/health         # å®é™…æµ‹è¯•ï¼šèƒ½ä¸èƒ½çœŸçš„é€åˆ°ç›®çš„åœ°
```

### å‘Šè­¦è§„åˆ™è®¾ç½®ï¼šå¿«é€’ç³»ç»Ÿçš„"ç´§æ€¥å‘¼å«å™¨"

**é‡è¦è¯´æ˜**ï¼šä»¥ä¸‹å‘Šè­¦è§„åˆ™æä¾›äº†å¤šç§æ£€æµ‹æ–¹æ³•ï¼Œè¯·æ ¹æ®æ‚¨çš„å®é™…Prometheusé…ç½®é€‰æ‹©åˆé€‚çš„æ–¹æ¡ˆï¼š

```bash
# ğŸš¨ çº¢è‰²è­¦æŠ¥ï¼šå¿«é€’ç³»ç»Ÿç˜«ç—ª

# æ–¹æ³•1ï¼šé€šè¿‡kube-proxyè‡ªèº«metricsæ£€æµ‹ï¼ˆå¦‚æœé…ç½®äº†æŠ“å–kube-proxy:10249ï¼‰
- alert: KubeProxyDown
  expr: up{job="kube-proxy"} == 0
  annotations:
    summary: "å¿«é€’å‘˜å¤±è”äº†ï¼kube-proxyè¿›ç¨‹åœæ­¢è¿è¡Œ"
    description: "å°±åƒå¿«é€’å‘˜çªç„¶æ¶ˆå¤±ï¼Œæ‰€æœ‰åŒ…è£¹éƒ½é€ä¸å‡ºå»äº†"

# æ–¹æ³•2ï¼šé€šè¿‡kube-state-metricsæ£€æµ‹DaemonSetçŠ¶æ€ï¼ˆæ›´å¸¸è§ï¼‰
- alert: KubeProxyPodsDown
  expr: kube_daemonset_status_number_ready{daemonset="kube-proxy"} < kube_daemonset_status_desired_number_scheduled{daemonset="kube-proxy"}
  annotations:
    summary: "å¿«é€’å‘˜ç¼ºå‹¤äº†ï¼kube-proxy Podæ•°é‡ä¸è¶³"
    description: "æœŸæœ›{{$labels.daemonset}}æœ‰{{$value}}ä¸ªPodè¿è¡Œï¼Œä½†å®é™…è¿è¡Œæ•°é‡ä¸è¶³"

- alert: ServiceEndpointsNotReady
  expr: kube_endpoint_address_not_ready > 0
  annotations:
    summary: "æ”¶è´§åœ°å€æœ‰é—®é¢˜ï¼Serviceæœ‰ä¸å¥åº·çš„Endpoints"
    description: "å°±åƒæŸäº›å°åŒºå°é—­äº†ï¼Œå¿«é€’é€ä¸è¿›å»"
```

## 6ï¸âƒ£ kube-proxyä¼˜åŒ–çš„"ç‰©æµç§˜ç±" ğŸ¥‹

### æ€§èƒ½è°ƒä¼˜ï¼šè®©å¿«é€’ç³»ç»Ÿæ›´é«˜æ•ˆ

### æ•…éšœå¤„ç†ï¼šå¿«é€’ç³»ç»Ÿçš„"åº”æ€¥é¢„æ¡ˆ"

```bash
# kube-proxyé‡å¯æ¢å¤
systemctl restart kube-proxy
# æˆ–è€…
kubectl delete pod -n kube-system -l k8s-app=kube-proxy

# æ‰‹åŠ¨æ¸…ç†è§„åˆ™ï¼ˆç´§æ€¥æƒ…å†µï¼‰
iptables -t nat -F                             # æ¸…ç©ºNATè¡¨ï¼ˆå±é™©æ“ä½œï¼ï¼‰
ipvsadm -C                                     # æ¸…ç©ºIPVSè§„åˆ™ï¼ˆå±é™©æ“ä½œï¼ï¼‰

# ç½‘ç»œè¿é€šæ€§ä¿®å¤
# é¦–å…ˆæ£€æŸ¥æ‚¨çš„é›†ç¾¤ä½¿ç”¨ä»€ä¹ˆç½‘ç»œæ’ä»¶
kubectl get pods -n kube-system | grep -E "(flannel|calico|weave|cilium|aws-node)"
kubectl get pods --all-namespaces | grep -E "(flannel|calico|weave|cilium|antrea)"

# æ ¹æ®å®é™…ä½¿ç”¨çš„ç½‘ç»œæ’ä»¶é‡å¯ï¼ˆé€‰æ‹©å¯¹åº”çš„å‘½ä»¤ï¼‰ï¼š
# Flannelç½‘ç»œæ’ä»¶
kubectl delete pod -n kube-system -l k8s-app=flannel

# Calicoç½‘ç»œæ’ä»¶
kubectl delete pod -n kube-system -l k8s-app=calico-node
kubectl delete pod -n calico-system -l k8s-app=calico-node

# AWS VPC CNIï¼ˆEKSé›†ç¾¤ï¼‰
kubectl delete pod -n kube-system -l k8s-app=aws-node

# Weaveç½‘ç»œæ’ä»¶
kubectl delete pod -n kube-system -l name=weave-net

# å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç½‘ç»œæ’ä»¶ï¼Œå¯èƒ½æ˜¯äº‘å‚å•†æ‰˜ç®¡æˆ–å•èŠ‚ç‚¹é›†ç¾¤
```

## ä¸€å¥è¯æ€»ç»“

kube-proxyå°±åƒKubernetesé›†ç¾¤çš„"éšå½¢å¿«é€’å‘˜"ï¼Œé»˜é»˜åœ°åœ¨æ¯ä¸ªèŠ‚ç‚¹ä¸Šç¡®ä¿ç½‘ç»œåŒ…è£¹èƒ½å‡†ç¡®é€è¾¾ç›®çš„åœ°ã€‚å®ƒæ—¢æ˜¯Serviceæµé‡çš„"æ™ºèƒ½åˆ†æ‹£å‘˜"ï¼Œåˆæ˜¯ç½‘ç»œè§„åˆ™çš„"äº¤é€šæŒ‡æŒ¥å®˜"ï¼Œè¿˜æ˜¯é›†ç¾¤çŠ¶æ€çš„"å®æ—¶åŒæ­¥å‘˜"ã€‚SREçš„ä»»åŠ¡å°±æ˜¯ç»™è¿™ä½"å¿«é€’å‘˜"è£…ä¸ŠGPSè¿½è¸ªå™¨å’Œæ™ºèƒ½è°ƒåº¦ç³»ç»Ÿï¼Œç¡®ä¿æ— è®ºæ˜¯ä¼ ç»Ÿçš„iptables"ç›´è¾¾å¿«é€’"æ¨¡å¼ï¼Œè¿˜æ˜¯ç°ä»£çš„ipvs"æ™ºèƒ½ç‰©æµ"æ¨¡å¼ï¼Œä»»ä½•é…é€å¼‚å¸¸éƒ½èƒ½è¢«åŠæ—¶å‘ç°å’Œå¤„ç†ï¼è®°ä½ï¼šå¥½çš„ç½‘ç»œæ˜¯æ— æ„Ÿçš„ï¼Œä½†å¥½çš„ç›‘æ§æ˜¯å…¨é¢çš„ï¼

---

## English Version

# kube-proxy: Kubernetes Network's "Invisible Courier" and SRE's "Logistics Monitoring" ğŸ“¦

Hey everyone! Following up on our previous discussion about proxies, today let's dive into a low-key but absolutely critical component in the Kubernetes ecosystem - kube-proxy! As K8s's dedicated proxy specialist, it quietly works on every node to ensure your Pods correctly receive their "packages" (network traffic). As SREs, we need to understand its mechanisms and ensure any anomalies are detected and handled promptly!

## 1ï¸âƒ£ kube-proxy's "Multiple Identities": From Mailman to Traffic Controller ğŸš¦

kube-proxy plays several key roles in a Kubernetes cluster:

### ğŸšš Identity One: Service Traffic "Courier" (Service Traffic Forwarder)
Think of kube-proxy as the dedicated delivery person for K8s University. You need to send a business request (package) to a Service (university building number), but the actual request handlers (Pod dorms) are scattered across different buildings (Nodes). kube-proxy is that courier who knows exactly which dorm each business request should be delivered to!

Core value of courier-type kube-proxy:
- Service Discovery: Knows which Pod dorms are behind each Service, just like a courier familiar with every dorm in every building
- Load Balancing: Distributes business requests evenly among multiple Pod dorms, like a courier optimizing delivery routes
- Health Checking: Only sends requests to healthy Pod dorms, like not delivering packages to empty dorms
- Session Affinity: Supports session persistence, like VIP customers' requests always going to the same dorm for processing

### ğŸš¦ Identity Two: Network Traffic "Controller" (Network Traffic Controller)
kube-proxy also acts like a traffic cop at every intersection (Node):

- Routing Rule Management: Maintains iptables/ipvs rules, like setting up traffic signals
- Traffic Forwarding: Ensures data packets take the right paths, like directing vehicles to proper lanes
- Port Mapping: Maps Service ports to Pod ports, like setting up dedicated lanes
- Network Policy Enforcement: Implements access control, like checking IDs at checkpoints

### ğŸ”„ Identity Three: Cluster State "Synchronizer" (Cluster State Synchronizer)
kube-proxy keeps everything in sync by watching the API Server:

- Real-time Monitoring: Watches Service and Endpoints changes, like a courier receiving delivery updates in real-time
- Rule Updates: Dynamically updates forwarding rules, like a traffic cop adjusting signals based on traffic conditions
- Failure Recovery: Automatically removes forwarding rules when Pods fail, like rerouting around blocked roads

Real-world application scenarios:
```
Scenario 1: External user business request â†’ LoadBalancer Service â†’ kube-proxy â†’ Request-handling Pod dorm
Scenario 2: Internal Pod request â†’ ClusterIP Service â†’ kube-proxy â†’ Target Pod dorm
Scenario 3: NodePort exposed request â†’ kube-proxy â†’ Request-handling Pod dorm
Scenario 4: Headless Service â†’ kube-proxy â†’ Direct Pod dorm IP lookup
```

## 2ï¸âƒ£ kube-proxy's "Working Modes": From Traditional Post Office to Modern Logistics ğŸ“®

kube-proxy has four main working modes, like the evolution of the delivery industry:

### ğŸŒ userspace Mode: Traditional "Post Office Mode" (Deprecated)
```bash
# Workflow:
Client â†’ iptables â†’ kube-proxy process â†’ Pod
```
- Characteristics: All traffic processed through kube-proxy process
- Issues: Poor performance, single point bottleneck, like all packages going through post office sorting
- Status: Basically deprecated, only exists in legacy versions

### âš¡ iptables Mode: Modern "Direct Delivery" (Default Mode)
```bash
# Workflow:
Client â†’ iptables rules â†’ Direct forwarding to Pod
```
- Characteristics: kube-proxy only maintains iptables rules, doesn't handle data packets
- Advantages: Good performance, low latency, like direct delivery to your door
- Disadvantages: Performance degrades with many rules, simple load balancing algorithms

### ğŸš€ ipvs Mode: High-Performance "Smart Logistics" (Recommended Mode)
```bash
# Workflow:
Client â†’ IPVS rules â†’ Efficient forwarding to Pod
```
- Characteristics: Based on kernel IPVS module, superior performance
- **kube-proxy's job**: Monitors API Server, converts Service and Endpoints info into IPVS rules, configures them into kernel IPVS module
- Advantages: Supports more load balancing algorithms, efficient rule lookup
- Use Cases: Large-scale clusters, high-concurrency scenarios

### ğŸ”¥ nftables Mode: Next-Gen "Super Logistics" (Experimental)
```bash
# Workflow:
Client â†’ nftables rules â†’ Modern forwarding to Pod
```
- Characteristics: Based on next-generation nftables framework, replacing traditional iptables
- **kube-proxy's job**: Converts Service rules to nftables rules, leveraging modern kernel features
- Advantages: Better performance, cleaner rule structure, better IPv6 support
- Use Cases: Newer kernel versions (Linux 4.17+), experimental feature

Mode comparison: Evolution of the delivery industry
```bash
# userspace mode = Traditional post office
All packages â†’ Post office sorting â†’ Re-delivery (slow but reliable)
kube-proxy: Personally handles every package, becomes performance bottleneck

# iptables mode = Modern delivery
Packages â†’ Smart sorting system â†’ Direct delivery (fast but complex rules)
kube-proxy: Only updates sorting rules, doesn't touch packages

# ipvs mode = Smart logistics
Packages â†’ AI smart scheduling â†’ Optimal path delivery (fast and intelligent)
kube-proxy: Configures smart scheduling system, lets kernel handle efficiently

# nftables mode = Future logistics
Packages â†’ Next-gen smart system â†’ Ultra-efficient delivery (latest tech, experimental stage)
kube-proxy: Uses latest kernel technology, better performance and maintainability
```

## 3ï¸âƒ£ kube-proxy "Crash Scenarios": When the Courier Goes Wrong ğŸ’¥

No matter which mode kube-proxy uses, various "crash" situations can occur:

### âš¡ iptables Mode "Pitfalls":

**Rule Explosion** - "Too many delivery slips, system crashes":
```bash
# iptables rule count in large-scale clusters
Services: 1000
Average Endpoints per Service: 10
Total rules: 1000 Ã— 10 Ã— multiple rules = tens of thousands of rules
# Result: Slow rule lookup, increased network latency
```

**Uneven Load Balancing** - "Courier plays favorites, always delivers to one place":
```bash
# iptables uses random algorithm, may cause uneven distribution
Pod A: Receives 70% traffic
Pod B: Receives 20% traffic
Pod C: Receives 10% traffic
# Result: Some Pods overloaded, others idle
```

**Slow Failure Recovery** - "Courier reacts slowly":
```bash
# After Pod failure, iptables rule update delay
Pod failure â†’ API Server awareness â†’ kube-proxy updates rules
# Delay: May take several seconds to tens of seconds
# Result: Traffic still forwarded to failed Pod during delay
```

### ğŸš€ ipvs Mode "Pitfalls":

**Kernel Module Dependencies** - "Smart scheduling system needs special hardware":
```bash
# kube-proxy needs to ensure IPVS kernel modules are loaded
modprobe ip_vs
modprobe ip_vs_rr      # Round-robin algorithm
modprobe ip_vs_wrr     # Weighted round-robin algorithm
modprobe ip_vs_sh      # Source address hash algorithm
# Issue: Some environments may not support or have modules installed
# kube-proxy checks these modules at startup, reports errors if missing
```

**Debugging Difficulties** - "Smart system black box":
```bash
# iptables rules can be viewed directly
iptables -t nat -L

# IPVS rules need special tools (may need installation)
ipvsadm -L -n
# Issue 1: Debugging tools not as widespread as iptables, need extra installation
# Issue 2: If ipvsadm not installed, troubleshooting becomes difficult

# Alternative checking methods (no extra tools needed)
cat /proc/net/ip_vs                    # View IPVS virtual server status
cat /proc/net/ip_vs_stats              # View IPVS statistics
```

### ğŸ”„ Common Issues:

**Network Partitioning** - "Courier gets lost":
```bash
# Inter-node network connectivity issues
Pod on Node A â†’ Cannot access Service on Node B
# Symptoms: Some requests fail, others work normally
```

**DNS Resolution Problems** - "Courier can't find the address":
```bash
# Service DNS resolution failure
nslookup my-service.default.svc.cluster.local
# Symptoms: Service names can't be resolved, only IP access works
```

**Port Conflicts** - "Duplicate delivery addresses":
```bash
# NodePort port conflicts
Service A: NodePort 30080
Service B: NodePort 30080  # Conflict!
# Result: One of the Services can't work properly
```

## 4ï¸âƒ£ SRE Perspective "Monitoring Pain Points": Installing GPS on the Courier ğŸ¯

kube-proxy brings unique monitoring challenges for SREs:

### ï¿½ Common Monitoring Pain Points:
- Service Discovery Latency: Time gap between Endpoints changes and rule effectiveness
- Network Connectivity: Network quality for cross-node Service access
- Failure Impact Scope: Impact of single kube-proxy failure on the cluster

### âš¡ iptables Mode Monitoring Pain Points:
- Rule Count Monitoring: How to monitor iptables rule count and performance impact?
- Load Balancing Effectiveness: How to verify if traffic distribution is even?
- Rule Sync Latency: How to monitor timeliness of rule updates?

### ğŸš€ ipvs Mode Monitoring Pain Points:
- Kernel Module Status: How to monitor if IPVS modules are working properly?
- Load Balancing Algorithms: How to verify effectiveness of different algorithms?
- Connection Tracking: How to monitor connection status and session persistence?

## 5ï¸âƒ£ SRE's "Logistics Monitoring": Installing X-Ray Vision on the Courier System ğŸ”

### Health Check Strategy: Courier System "Physical Exam Report"

Just like giving couriers regular health checkups, we need to regularly check kube-proxy's health:

```bash
# Courier "Physical Examination"
# Method 1: Check kube-proxy Pod status (common in modern K8s clusters)
kubectl get pods -n kube-system -l k8s-app=kube-proxy
kubectl describe pods -n kube-system -l k8s-app=kube-proxy

# Method 2: Check systemd service (traditional deployment)
systemctl status kube-proxy
# Note: Many modern K8s clusters run kube-proxy as Pods, not systemd services

# Courier "Route Familiarity Check"
iptables -t nat -L | grep -c KUBE-SVC          # Traditional courier: count how many routes memorized
# Check which mode is currently being used
kubectl logs -n kube-system -l k8s-app=kube-proxy | grep "Using.*proxy"

# Check mode configuration in ConfigMap
kubectl get cm -n kube-system kube-proxy -o yaml | grep -A5 -B5 "mode:"
# If mode: "" (empty), uses default mode (usually iptables)

# If ipvs mode, need to install ipvsadm tool first
apt install ipvsadm                            # Ubuntu/Debian systems
yum install ipvsadm                            # CentOS/RHEL systems
ipvsadm -L -n | grep -c TCP                    # Modern courier: check smart navigation system routes

# If ipvsadm not installed, can check through other methods
cat /proc/net/ip_vs                            # Direct view of kernel IPVS status
ls /sys/module/ip_vs*/                         # Check if IPVS modules are loaded

# Courier Network "Connectivity Patrol"
kubectl get endpoints                           # Check which delivery addresses can still receive packages
curl -I http://service-name:port/health         # Actual test: can packages really reach destination
```

### Alert Rule Setup: Courier System "Emergency Alarm"

**Important Note**: The following alert rules provide multiple detection methods. Please choose the appropriate solution based on your actual Prometheus configuration:

```bash
# ğŸš¨ Red Alert: Courier system down

# Method 1: Detect via kube-proxy self metrics (if configured to scrape kube-proxy:10249)
- alert: KubeProxyDown
  expr: up{job="kube-proxy"} == 0
  annotations:
    summary: "Courier lost contact! kube-proxy process stopped"
    description: "Like a courier suddenly disappearing, all packages can't be delivered"

# Method 2: Detect via kube-state-metrics DaemonSet status (more common)
- alert: KubeProxyPodsDown
  expr: kube_daemonset_status_number_ready{daemonset="kube-proxy"} < kube_daemonset_status_desired_number_scheduled{daemonset="kube-proxy"}
  annotations:
    summary: "Courier called in sick! Insufficient kube-proxy Pods"
    description: "Expected {{$labels.daemonset}} to have {{$value}} Pods running, but actual count is insufficient"

- alert: ServiceEndpointsNotReady
  expr: kube_endpoint_address_not_ready > 0
  annotations:
    summary: "Delivery address problem! Service has unhealthy Endpoints"
    description: "Like some neighborhoods being closed off, packages can't be delivered"
```

## 6ï¸âƒ£ kube-proxy Optimization "Logistics Secrets" ğŸ¥‹

### Performance Tuning: Making the Courier System More Efficient

### Failure Handling: Courier System "Emergency Plans"

```bash
# kube-proxy Restart Recovery
systemctl restart kube-proxy
# Or
kubectl delete pod -n kube-system -l k8s-app=kube-proxy

# Manual Rule Cleanup (Emergency)
iptables -t nat -F                             # Clear NAT table (Dangerous!)
ipvsadm -C                                     # Clear IPVS rules (Dangerous!)

# Network Connectivity Repair
# First check what network plugin your cluster uses
kubectl get pods -n kube-system | grep -E "(flannel|calico|weave|cilium|aws-node)"
kubectl get pods --all-namespaces | grep -E "(flannel|calico|weave|cilium|antrea)"

# Restart based on actual network plugin used (choose corresponding command):
# Flannel network plugin
kubectl delete pod -n kube-system -l k8s-app=flannel

# Calico network plugin
kubectl delete pod -n kube-system -l k8s-app=calico-node
kubectl delete pod -n calico-system -l k8s-app=calico-node

# AWS VPC CNI (EKS clusters)
kubectl delete pod -n kube-system -l k8s-app=aws-node

# Weave network plugin
kubectl delete pod -n kube-system -l name=weave-net

# If no network plugin found, might be cloud provider managed or single-node cluster
```

## Summary

kube-proxy is like the "invisible courier" of the Kubernetes cluster, quietly ensuring network packages are accurately delivered to their destinations on every node. It serves as both an "intelligent sorter" for Service traffic, a "traffic controller" for network rules, and a "real-time synchronizer" for cluster state. The SRE's job is to equip this "courier" with GPS tracking and intelligent scheduling systems, ensuring that whether it's the traditional iptables "direct delivery" mode or the modern ipvs "smart logistics" mode, any delivery anomalies can be detected and handled promptly! Remember: Good networking is invisible, but good monitoring is comprehensive!
