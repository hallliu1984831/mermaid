----- Chinese
# ğŸ’¾ CSI (Container Storage Interface)ï¼šK8Så¤§å­¦çš„"ä¸“ä¸šå­˜å‚¨å·¥ç¨‹å¸ˆ"

## å‰è¨€ï¼šæ²¡æœ‰å­˜å‚¨ä¸“å®¶ï¼Œæ•°æ®å°±æ— å®¶å¯å½’

å¤§å®¶å¥½ï¼ä»Šå¤©æˆ‘ä»¬æ¥èŠèŠCSIï¼ˆContainer Storage Interfaceï¼‰è¿™ä¸ª"ä¸“ä¸šå­˜å‚¨å·¥ç¨‹å¸ˆ"ã€‚ä½ å¯èƒ½å¤©å¤©å’Œå­˜å‚¨æ‰“äº¤é“ï¼Œä½†ä½ æƒ³è¿‡æ²¡æœ‰ï¼šä¸ºä»€ä¹ˆä¸€ä¸ªYAMLæ–‡ä»¶å°±èƒ½è‡ªåŠ¨åˆ›å»ºå­˜å‚¨å·ï¼Ÿä¸ºä»€ä¹ˆä¸åŒçš„å­˜å‚¨ç³»ç»Ÿéƒ½èƒ½æ— ç¼æ¥å…¥K8sï¼Ÿ

æƒ³è±¡ä¸€ä¸‹ï¼Œå¦‚æœK8Så¤§å­¦é‡Œæ²¡æœ‰ä¸“ä¸šå­˜å‚¨å·¥ç¨‹å¸ˆï¼š
- ğŸ“š å­¦ç”Ÿæƒ³è¦å‚¨ç‰©æŸœï¼ŸæŠ±æ­‰ï¼Œä½ å¾—è‡ªå·±å»ä¹°ã€è‡ªå·±å®‰è£…ã€è‡ªå·±ç»´æŠ¤
- ğŸ’» æ•°æ®åº“éœ€è¦æŒä¹…åŒ–å­˜å‚¨ï¼Ÿä½ å¾—æ‰‹åŠ¨é…ç½®æ¯ä¸€ä¸ªå­˜å‚¨è®¾å¤‡
- ğŸ”„ æƒ³è¦å¤‡ä»½æ•°æ®ï¼Ÿä½ å¾—å­¦ä¼šæ¯ç§å­˜å‚¨ç³»ç»Ÿçš„ä¸“ç”¨å·¥å…·

è¿™å°±æ˜¯æ²¡æœ‰CSIçš„K8sé›†ç¾¤ - ä¸€ä¸ª"å­˜å‚¨ç®¡ç†çš„å™©æ¢¦"ï¼

## 1ï¸âƒ£ CSIæ˜¯ä»€ä¹ˆï¼ŸK8Så¤§å­¦çš„"ä¸“ä¸šå­˜å‚¨å·¥ç¨‹å¸ˆ"

### CSIï¼šContainer Storage Interfaceï¼ˆå®¹å™¨å­˜å‚¨æ¥å£ï¼‰
å°±åƒå¤§å­¦é‡Œçš„ä¸“ä¸šå­˜å‚¨å·¥ç¨‹å¸ˆå¸®ä½ ç®¡ç†å„ç§å‚¨ç‰©è®¾å¤‡ä¸€æ ·ï¼ŒCSIæŠŠå¤æ‚çš„å­˜å‚¨æ“ä½œæ ‡å‡†åŒ–ï¼Œè®©ä¸åŒçš„å­˜å‚¨ç³»ç»Ÿéƒ½èƒ½ä¸ºK8sæä¾›æœåŠ¡ï¼š

- å­˜å‚¨æŠ½è±¡ï¼šæŠŠå„ç§å­˜å‚¨ç³»ç»Ÿç»Ÿä¸€æˆæ ‡å‡†æ¥å£
- åŠ¨æ€ä¾›åº”ï¼šæ ¹æ®éœ€æ±‚è‡ªåŠ¨åˆ›å»ºå’Œåˆ é™¤å­˜å‚¨å·
- ç”Ÿå‘½å‘¨æœŸç®¡ç†ï¼šä»åˆ›å»ºåˆ°åˆ é™¤çš„å…¨ç¨‹ç®¡ç†
- è·¨å¹³å°æ”¯æŒï¼šæ”¯æŒäº‘å­˜å‚¨ã€æœ¬åœ°å­˜å‚¨ã€ç½‘ç»œå­˜å‚¨ç­‰

ğŸ’¡ å…³é”®ç†è§£ï¼šCSIå°±åƒæ ¡å›­é‡Œçš„"ä¸‡èƒ½å­˜å‚¨ç®¡å®¶"ï¼Œä½ åªéœ€è¦è¯´å‡ºå­˜å‚¨éœ€æ±‚ï¼Œå®ƒå°±å¸®ä½ æå®šä¸€åˆ‡ã€‚

## 2ï¸âƒ£ CSIçš„å·¥ä½œåŸç†ï¼šå­˜å‚¨å·¥ç¨‹å¸ˆçš„"æ ‡å‡†ä½œä¸šæµç¨‹"

### CSIæ¶æ„ï¼šä¸‰ä¸ªä¸“ä¸šè§’è‰²çš„åˆ†å·¥åä½œ
```
K8S API Server (K8Så¤§å­¦æ€»åŠ¡å¤„) <--> CSI Driver (å­˜å‚¨å·¥ç¨‹å¸ˆ) <--> Storage System (å­˜å‚¨è®¾å¤‡)
```

### CSI Driverçš„ä¸‰å¤§ç»„ä»¶
1. Controller Pluginï¼ˆå­˜å‚¨è°ƒåº¦å‘˜ï¼‰
   - è´Ÿè´£å­˜å‚¨å·çš„åˆ›å»ºã€åˆ é™¤ã€æ‰©å®¹
   - å¤„ç†å¿«ç…§ã€å…‹éš†ç­‰é«˜çº§åŠŸèƒ½
   - è¿è¡Œåœ¨æ§åˆ¶èŠ‚ç‚¹ä¸Šï¼Œä»¥Deploymentçš„å½¢å¼ç»´æŠ¤

2. Node Pluginï¼ˆç°åœºå·¥ç¨‹å¸ˆï¼‰
   - è´Ÿè´£å­˜å‚¨å·çš„æŒ‚è½½ã€å¸è½½
   - å¤„ç†èŠ‚ç‚¹çº§åˆ«çš„å­˜å‚¨æ“ä½œ
   - è¿è¡Œåœ¨æ¯ä¸ªå·¥ä½œèŠ‚ç‚¹ä¸Šï¼Œä»¥DaemonSetçš„å½¢å¼ç»´æŠ¤

3. CSI Sidecar Containersï¼ˆä¸“ä¸šåŠ©æ‰‹ï¼‰
   - external-provisionerï¼šåŠ¨æ€ä¾›åº”åŠ©æ‰‹
   - external-attacherï¼šæŒ‚è½½ç®¡ç†åŠ©æ‰‹
   - external-resizerï¼šæ‰©å®¹ç®¡ç†åŠ©æ‰‹
   - external-snapshotterï¼šå¿«ç…§ç®¡ç†åŠ©æ‰‹

## 3ï¸âƒ£ é›†ç¾¤ä¸­ä¸å­˜åœ¨CSIç»„ä»¶ï¼Ÿæœ¬åœ°å­˜å‚¨ä¹Ÿèƒ½å°†å°±

å¦‚æœä½ çš„K8sé›†ç¾¤ä¸­æ²¡æœ‰ä»»ä½•CSI Driverï¼Œè¿™å®Œå…¨æ­£å¸¸ï¼å› ä¸ºK8så†…ç½®æ”¯æŒæœ¬åœ°å­˜å‚¨ï¼š

### ğŸ  æ— éœ€CSIçš„æœ¬åœ°å­˜å‚¨æ–¹æ¡ˆ

#### ç›´æ¥ä½¿ç”¨hostPath
å¦‚æœä»å½“å‰èŠ‚ç‚¹ä¸Šåˆ é™¤PODï¼Œå¹¶ä¸”åœ¨åŒä¸€ä¸ªèŠ‚ç‚¹é‡æ–°åˆ›å»ºPODï¼Œæ•°æ®ä¸ä¼šä¸¢å¤±ï¼Œå› ä¸ºæ•°æ®å­˜å‚¨åœ¨èŠ‚ç‚¹çš„æœ¬åœ°ç£ç›˜ä¸Šã€‚å¦‚æœåˆ é™¤åè¢«è°ƒåº¦åˆ°å…¶ä»–èŠ‚ç‚¹ä¸Šï¼Œæ•°æ®ä¼šä¸¢å¤±ã€‚å› ä¸ºæ–°PODä¼šåœ¨æ–°çš„èŠ‚ç‚¹ä¸Šåˆ›å»ºç›®å½•ï¼Œæ²¡æœ‰åŸèŠ‚ç‚¹æ•°æ®ã€‚ä¸¾ä¾‹å¦‚ä¸‹ï¼š
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: app-pod
spec:
  containers:
  - name: app
    image: nginx
    volumeMounts:
    - name: data
      mountPath: /data
  volumes:
  - name: data
    hostPath:
      path: /opt/app-data
      type: DirectoryOrCreate
```

#### ä½¿ç”¨emptyDirä¸´æ—¶å­˜å‚¨
PODè¢«åˆ é™¤ä»¥åï¼ŒemptyDirä¸­çš„æ•°æ®ä¼šä¸¢å¤±ï¼Œæ‰€ä»¥emptyDiré€‚åˆå­˜å‚¨ä¸´æ—¶æ•°æ®ï¼Œä¸æ”¯æŒæŒä¹…åŒ–ã€‚ä¸¾ä¾‹å¦‚ä¸‹ï¼š
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: temp-pod
spec:
  containers:
  - name: app
    image: nginx
    volumeMounts:
    - name: temp-data
      mountPath: /tmp
  volumes:
  - name: temp-data
    emptyDir: {}
```

#### æ‰‹åŠ¨åˆ›å»ºæœ¬åœ°PV
åŒæ ·å­˜åœ¨å¯¹èŠ‚ç‚¹çš„ä¾èµ–ï¼Œè¯·æ³¨æ„å¦‚ä¸‹nodeAffinityçš„é…ç½®ï¼Œå®ƒé™åˆ¶äº†è¯¥PVåªèƒ½è¢«worker-node-1èŠ‚ç‚¹ä½¿ç”¨ã€‚å³ç¡®ä¿äº†PODå’ŒPVåœ¨åŒä¸€ä¸ªèŠ‚ç‚¹ä¸Šï¼Œæ•°æ®æ‰ä¸ä¼šä¸¢å¤±ã€‚ä¸¾ä¾‹å¦‚ä¸‹ï¼š
```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: local-pv
spec:
  capacity:
    storage: 10Gi
  accessModes:
  - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  local:
    path: /mnt/data
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - worker-node-1
```

### âœ… æœ¬åœ°å­˜å‚¨çš„ä¼˜åŠ¿
- é›¶é…ç½®ï¼šK8så†…ç½®æ”¯æŒï¼Œæ— éœ€å®‰è£…ä»»ä½•ç»„ä»¶
- é«˜æ€§èƒ½ï¼šç›´æ¥è®¿é—®æœ¬åœ°ç£ç›˜ï¼Œå»¶è¿Ÿæœ€ä½
- ç®€å•å¯é ï¼šæ²¡æœ‰ç½‘ç»œä¾èµ–ï¼Œæ•…éšœç‚¹æœ€å°‘
- æˆæœ¬æœ€ä½ï¼šæ— éœ€é¢å¤–çš„å­˜å‚¨è®¾å¤‡æˆ–æœåŠ¡

## 4ï¸âƒ£ ä»€ä¹ˆåœºæ™¯ä¸‹éœ€è¦CSI

å½“ä½ éœ€è¦ä½¿ç”¨å¤–éƒ¨å­˜å‚¨ç³»ç»Ÿæ—¶ï¼Œæ‰éœ€è¦CSIï¼š

### ğŸ¢ æ•°æ®ä¸­å¿ƒå­˜å‚¨åœºæ™¯
```bash
# éœ€è¦å…±äº«å­˜å‚¨æ—¶
- å¤šä¸ªPodéœ€è¦è®¿é—®åŒä¸€ä»½æ•°æ®
- æ•°æ®éœ€è¦åœ¨Podé‡å¯åä¿æŒ
- éœ€è¦è·¨èŠ‚ç‚¹çš„æ•°æ®è®¿é—®

# å…¸å‹åº”ç”¨
- å…±äº«é…ç½®æ–‡ä»¶
- é™æ€èµ„æºæ–‡ä»¶
- å¤šå‰¯æœ¬åº”ç”¨çš„å…±äº«æ•°æ®
```

### â˜ï¸ äº‘å­˜å‚¨åœºæ™¯
```bash
# éœ€è¦äº‘æœåŠ¡é›†æˆæ—¶
- ä½¿ç”¨äº‘å‚å•†çš„æ‰˜ç®¡å­˜å‚¨æœåŠ¡
- éœ€è¦è‡ªåŠ¨å¤‡ä»½å’Œé«˜å¯ç”¨
- éœ€è¦å¼¹æ€§æ‰©å®¹å­˜å‚¨å®¹é‡

# å…¸å‹åº”ç”¨
- ç”Ÿäº§æ•°æ®åº“å­˜å‚¨
- å¤§æ–‡ä»¶å­˜å‚¨å’Œå¤„ç†
- è·¨åœ°åŸŸæ•°æ®åŒæ­¥
```

### ğŸ¯ é€‰æ‹©å†³ç­–æ ‘
```
ä½ çš„åº”ç”¨éœ€è¦ä»€ä¹ˆï¼Ÿ
â”œâ”€ åªéœ€è¦ä¸´æ—¶å­˜å‚¨ â†’ emptyDirï¼ˆæ— éœ€CSIï¼‰
â”œâ”€ åªéœ€è¦æœ¬åœ°æŒä¹…åŒ– â†’ hostPath/local PVï¼ˆæ— éœ€CSIï¼‰
â”œâ”€ éœ€è¦è·¨Podå…±äº«æ•°æ® â†’ NFS CSI / Ceph FS CSI
â”œâ”€ éœ€è¦é«˜æ€§èƒ½æ•°æ®åº“å­˜å‚¨ â†’ æœ¬åœ°SSDï¼ˆæ— éœ€CSIï¼‰æˆ–äº‘ç›˜CSI
â””â”€ éœ€è¦äº‘æœåŠ¡é›†æˆ â†’ å¯¹åº”äº‘å‚å•†çš„CSI Driver
```

### ğŸ“‹ å¸¸è§CSI Driver
```bash
# æ•°æ®ä¸­å¿ƒå­˜å‚¨
nfs.csi.k8s.io          # NFSå…±äº«å­˜å‚¨
rbd.csi.ceph.com        # Cephå—å­˜å‚¨
cephfs.csi.ceph.com     # Cephæ–‡ä»¶ç³»ç»Ÿ

# äº‘å­˜å‚¨
ebs.csi.aws.com         # AWSäº‘ç›˜
disk.csi.azure.com      # Azureç£ç›˜
pd.csi.storage.gke.io   # Googleäº‘ç›˜
```

## 5ï¸âƒ£ CSIçš„ä½¿ç”¨ä¸¾ä¾‹

### ğŸ“ ä¾‹å­1ï¼šæ•°æ®ä¸­å¿ƒNFSå…±äº«å­˜å‚¨

åœºæ™¯ï¼šå¤šä¸ªWebåº”ç”¨éœ€è¦å…±äº«é™æ€èµ„æºæ–‡ä»¶

#### æ­¥éª¤1ï¼šéƒ¨ç½²NFS Subdir External Provisioner
```bash
# ä½¿ç”¨Helmå®‰è£…ï¼ˆæ¨èï¼‰
helm repo add nfs-subdir-external-provisioner https://kubernetes-sigs.github.io/nfs-subdir-external-provisioner/
helm install nfs-subdir-external-provisioner nfs-subdir-external-provisioner/nfs-subdir-external-provisioner \
    --set nfs.server=192.168.1.100 \
    --set nfs.path=/data/shared

# éªŒè¯å®‰è£…
kubectl get pods -n default | grep nfs-subdir-external-provisioner
```

#### æ­¥éª¤2ï¼šåˆ›å»ºStorageClass
```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: nfs-storage
provisioner: k8s-sigs.io/nfs-subdir-external-provisioner
parameters:
  archiveOnDelete: "false"    # åˆ é™¤PVCæ—¶ä¸å½’æ¡£æ•°æ®
reclaimPolicy: Delete
allowVolumeExpansion: true
```

#### æ­¥éª¤3ï¼šåˆ›å»ºPVCå’Œä½¿ç”¨
```yaml
# PVC
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: shared-files-pvc
spec:
  accessModes:
    - ReadWriteMany        # å¤šä¸ªPodå¯åŒæ—¶è¯»å†™
  resources:
    requests:
      storage: 10Gi
  storageClassName: nfs-storage # æŒ‡å®šNFSå­˜å‚¨ç±»ï¼Œä¸Šå›¾SCçš„åå­—

---
# ä½¿ç”¨PVCçš„Pod
apiVersion: v1
kind: Pod
metadata:
  name: web-app
spec:
  containers:
  - name: nginx
    image: nginx
    volumeMounts:
    - name: shared-files
      mountPath: /usr/share/nginx/html
  volumes:
  - name: shared-files
    persistentVolumeClaim:
      claimName: shared-files-pvc
```

### â˜ï¸ ä¾‹å­2ï¼šæœ¬åœ°é›†ç¾¤æŒ‚è½½AWS S3å­˜å‚¨

åœºæ™¯ï¼šæœ¬åœ°K8sé›†ç¾¤éœ€è¦ä½¿ç”¨AWS S3ä½œä¸ºå¯¹è±¡å­˜å‚¨

#### æ­¥éª¤1ï¼šå®‰è£…S3 CSI Driver
```bash
# å®‰è£…S3 CSI Driver
kubectl apply -f https://raw.githubusercontent.com/awslabs/mountpoint-s3-csi-driver/main/deploy/kubernetes/overlays/stable/kustomization.yaml

# éªŒè¯å®‰è£…
kubectl get pods -n kube-system | grep s3-csi
```

#### æ­¥éª¤2ï¼šåˆ›å»ºAWSå‡­è¯
```bash
# åˆ›å»ºAWSè®¿é—®å¯†é’¥Secret
kubectl create secret generic aws-secret \
  --from-literal=key_id=YOUR_ACCESS_KEY_ID \
  --from-literal=access_key=YOUR_SECRET_ACCESS_KEY \
  -n kube-system
```

#### æ­¥éª¤3ï¼šåˆ›å»ºStorageClass
```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: s3-storage
provisioner: s3.csi.aws.com
parameters:
  mounter: mountpoint-s3           # ä½¿ç”¨Mountpoint for S3
  bucketName: my-k8s-bucket        # S3å­˜å‚¨æ¡¶åç§°
  region: us-west-2                # AWSåŒºåŸŸ
mountOptions:
  - allow-delete                   # å…è®¸åˆ é™¤æ“ä½œ
  - region=us-west-2
```

#### æ­¥éª¤4ï¼šåˆ›å»ºPVCå’Œä½¿ç”¨
```yaml
# PVC
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: s3-storage-pvc
spec:
  accessModes:
    - ReadWriteMany      # S3æ”¯æŒå¤šPodè®¿é—®
  resources:
    requests:
      storage: 100Gi     # é€»è¾‘å®¹é‡
  storageClassName: s3-storage

---
# åº”ç”¨Pod
apiVersion: v1
kind: Pod
metadata:
  name: s3-app-pod
spec:
  containers:
  - name: app
    image: nginx
    volumeMounts:
    - name: s3-data
      mountPath: /data
    env:
    - name: AWS_ACCESS_KEY_ID
      valueFrom:
        secretKeyRef:
          name: aws-secret
          key: key_id
    - name: AWS_SECRET_ACCESS_KEY
      valueFrom:
        secretKeyRef:
          name: aws-secret
          key: access_key
  volumes:
  - name: s3-data
    persistentVolumeClaim:
      claimName: s3-storage-pvc
```

### ğŸ” éªŒè¯å’Œæµ‹è¯•

```bash
# æŸ¥çœ‹PVCçŠ¶æ€
kubectl get pvc

# æµ‹è¯•æ•°æ®å†™å…¥
kubectl exec s3-app-pod -- sh -c "echo 'Hello S3!' > /data/test.txt"

# æŸ¥çœ‹S3å­˜å‚¨æ¡¶å†…å®¹
aws s3 ls s3://my-k8s-bucket/
```

## ğŸ¯ æ€»ç»“

| ç‰¹æ€§ | æœ¬åœ°å­˜å‚¨ | NFSå­˜å‚¨ | S3äº‘å­˜å‚¨ |
|------|---------|---------|----------|
| æ˜¯å¦éœ€è¦CSI | âŒ K8så†…ç½® | âœ… éœ€è¦CSI | âœ… éœ€è¦CSI |
| è®¿é—®æ¨¡å¼ | ReadWriteOnce | ReadWriteMany | ReadWriteMany |
| é€‚ç”¨åœºæ™¯ | é«˜æ€§èƒ½ã€ä¸´æ—¶æ•°æ® | å…±äº«æ–‡ä»¶ã€é™æ€èµ„æº | å¯¹è±¡å­˜å‚¨ã€å¤‡ä»½å½’æ¡£ |
| æ€§èƒ½ | æœ€é«˜ï¼ˆæœ¬åœ°ç£ç›˜ï¼‰ | ç½‘ç»œé™åˆ¶ | ç½‘ç»œé™åˆ¶ï¼Œé«˜åå |
| å¯ç”¨æ€§ | ä¾èµ–èŠ‚ç‚¹ | ä¾èµ–NFSæœåŠ¡å™¨ | äº‘æœåŠ¡ï¼Œ99.9%å¯ç”¨ |
| æ•°æ®æŒä¹…æ€§ | èŠ‚ç‚¹æ•…éšœä¸¢å¤± | æœåŠ¡å™¨çº§æŒä¹… | äº‘çº§æŒä¹…ï¼Œå¤šå‰¯æœ¬ |
| æˆæœ¬ | æœ€ä½ï¼ˆæ— é¢å¤–æˆæœ¬ï¼‰ | è‡ªå»ºNFSæˆæœ¬ | æŒ‰ä½¿ç”¨é‡ä»˜è´¹ |
| è·¨èŠ‚ç‚¹è®¿é—® | âŒ ä¸æ”¯æŒ | âœ… æ”¯æŒ | âœ… æ”¯æŒ |

è®°ä½è¿™ä¸ªç®€å•åŸåˆ™ï¼š
- ğŸ  æœ¬åœ°å­˜å‚¨ --> K8så†…ç½® --> æ— éœ€CSI
- ğŸŒ å¤–éƒ¨å­˜å‚¨ --> éœ€è¦CSI --> æ ‡å‡†åŒ–æ¥å£
- ç”Ÿäº§ç¯å¢ƒä¸€èˆ¬éƒ½ä¼šä½¿ç”¨CSI

----English

# ğŸ’¾ CSI (Container Storage Interface): The "Storage Engineer" of K8s University

## Preface: Without Storage Experts, Data Has No Home

Hey there! Today we're diving into CSI (Container Storage Interface) - the "professional storage engineer" of our K8s world. You probably work with storage every day, but have you ever wondered: How does a simple YAML file automatically create storage volumes? How do different storage systems seamlessly integrate with K8s?

Imagine if K8s University had no professional storage engineers:
- ğŸ“š Students need lockers? Sorry, you'll have to buy, install, and maintain them yourself
- ğŸ’» Database needs persistent storage? You'll have to manually configure every storage device
- ğŸ”„ Want to backup data? You'll need to learn each storage system's proprietary tools

That's what a K8s cluster without CSI looks like - a "storage management nightmare"!

## 1ï¸âƒ£ What is CSI? The "Professional Storage Engineer" of K8s University

### CSI: Container Storage Interface
Just like professional storage engineers on campus help you manage various storage devices, CSI standardizes complex storage operations, enabling different storage systems to serve K8s:

- Storage Abstraction: Unifies various storage systems into standard interfaces
- Dynamic Provisioning: Automatically creates and deletes storage volumes based on demand
- Lifecycle Management: Complete management from creation to deletion
- Cross-Platform Support: Supports cloud storage, local storage, network storage, etc.

ğŸ’¡ Key Understanding: CSI is like the "universal storage butler" on campus - you just specify your storage needs, and it handles everything else.

## 2ï¸âƒ£ How CSI Works: The "Standard Operating Procedures" of Storage Engineers

### CSI Architecture: Collaborative Division of Three Professional Roles
```
K8s API Server (University Administration) <--> CSI Driver (Storage Engineer) <--> Storage System (Storage Equipment)
```

### Three Major Components of CSI Driver
1. Controller Plugin (Storage Coordinator)
   - Handles storage volume creation, deletion, and expansion
   - Manages advanced features like snapshots and cloning
   - Runs on control nodes, maintained as a Deployment

2. Node Plugin (Field Engineer)
   - Handles storage volume mounting and unmounting
   - Manages node-level storage operations
   - Runs on every worker node, maintained as a DaemonSet

3. CSI Sidecar Containers (Professional Assistants)
   - external-provisioner: Dynamic provisioning assistant
   - external-attacher: Mount management assistant
   - external-resizer: Expansion management assistant
   - external-snapshotter: Snapshot management assistant

## 3ï¸âƒ£ No CSI Components in Your Cluster: Local Storage is Enough

This is completely normal! Many K8s clusters run perfectly fine without any CSI components because local storage doesn't need CSI.

### ğŸ  Local Storage Solutions That Don't Need CSI

#### Direct hostPath Usage
If you delete a Pod from the current node and recreate it on the same node, data won't be lost because it's stored on the node's local disk. However, if the Pod gets scheduled to a different node after deletion, data will be lost since the new Pod will create directories on the new node without access to the original node's data.

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: app-pod
spec:
  containers:
  - name: app
    image: nginx
    volumeMounts:
    - name: data
      mountPath: /data
  volumes:
  - name: data
    hostPath:
      path: /opt/app-data
      type: DirectoryOrCreate
```

#### Using emptyDir for Temporary Storage
After a Pod is deleted, data in emptyDir will be lost, so emptyDir is suitable for storing temporary data and doesn't support persistence.

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: temp-pod
spec:
  containers:
  - name: app
    image: nginx
    volumeMounts:
    - name: temp-data
      mountPath: /tmp/data
  volumes:
  - name: temp-data
    emptyDir: {}
```

#### Manually Creating Local PVs
This also has node dependency. Note the nodeAffinity configuration below, which restricts this PV to only be used by worker-node-1. This ensures the Pod and PV are on the same node so data won't be lost.

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: local-pv
spec:
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Delete
  storageClassName: local-storage
  local:
    path: /mnt/data
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - worker-node-1
```

## 4ï¸âƒ£ When Do You Need CSI?

You only need CSI when you need to use external storage systems:

### ğŸ¢ Data Center Storage Scenarios
```bash
# When you need shared storage
- Multiple Pods need to access the same data
- Data needs to persist after Pod restarts
- Cross-node data access is required

# Typical applications
- Shared configuration files
- Static resource files
- Shared data for multi-replica applications
```

### â˜ï¸ Cloud Storage Scenarios
```bash
# When you need cloud service integration
- Using cloud provider managed storage services
- Need automatic backup and high availability
- Need elastic storage capacity scaling

# Typical applications
- Production database storage
- Large file storage and processing
- Cross-region data synchronization
```

### ğŸ¯ Decision Tree
```
What does your application need?
â”œâ”€ Just temporary storage â†’ emptyDir (No CSI needed)
â”œâ”€ Just local persistence â†’ hostPath/local PV (No CSI needed)
â”œâ”€ Cross-Pod data sharing â†’ NFS CSI / Ceph FS CSI
â”œâ”€ High-performance database storage â†’ Local SSD (No CSI) or Cloud disk CSI
â””â”€ Cloud service integration â†’ Corresponding cloud provider CSI Driver
```

### ğŸ“‹ Common CSI Drivers
```bash
# Data Center Storage
- NFS: k8s-sigs.io/nfs-subdir-external-provisioner
- Ceph RBD: rbd.csi.ceph.com
- Ceph FS: cephfs.csi.ceph.com
- GlusterFS: gluster.org/glusterfs

# Cloud Storage
- AWS EBS: ebs.csi.aws.com
- AWS EFS: efs.csi.aws.com
- AWS S3: s3.csi.aws.com
- Azure Disk: disk.csi.azure.com
- GCP PD: pd.csi.storage.gke.io
```

## 5ï¸âƒ£ CSI Usage Examples

### ğŸ¢ Example 1: Data Center NFS Shared Storage

Scenario: Multiple web applications need to share static resources

#### Step 1: Install NFS Subdir External Provisioner
```bash
# Add Helm repository
helm repo add nfs-subdir-external-provisioner https://kubernetes-sigs.github.io/nfs-subdir-external-provisioner/

# Install using Helm
helm install nfs-subdir-external-provisioner nfs-subdir-external-provisioner/nfs-subdir-external-provisioner \
    --set nfs.server=192.168.1.100 \
    --set nfs.path=/data/shared

# Verify installation
kubectl get pods | grep nfs-subdir
```

#### Step 2: Create StorageClass
```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: nfs-storage
provisioner: k8s-sigs.io/nfs-subdir-external-provisioner
parameters:
  archiveOnDelete: "false"    # Don't archive on delete
reclaimPolicy: Delete
```

#### Step 3: Create PVC and Use It
```yaml
# PVC
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: shared-files-pvc
spec:
  accessModes:
    - ReadWriteMany      # Multiple Pods can access simultaneously
  resources:
    requests:
      storage: 10Gi
  storageClassName: nfs-storage

---
# Application Pod
apiVersion: v1
kind: Pod
metadata:
  name: web-app
spec:
  containers:
  - name: nginx
    image: nginx
    volumeMounts:
    - name: shared-files
      mountPath: /usr/share/nginx/html
  volumes:
  - name: shared-files
    persistentVolumeClaim:
      claimName: shared-files-pvc
```

### â˜ï¸ Example 2: Local Cluster Mounting AWS S3 Storage

Scenario: Local K8s cluster needs to use AWS S3 as object storage

#### Step 1: Install S3 CSI Driver
```bash
# Install S3 CSI Driver
kubectl apply -f https://raw.githubusercontent.com/awslabs/mountpoint-s3-csi-driver/main/deploy/kubernetes/overlays/stable/kustomization.yaml

# Verify installation
kubectl get pods -n kube-system | grep s3-csi
```

#### Step 2: Create AWS Credentials
```bash
# Create AWS access key Secret
kubectl create secret generic aws-secret \
  --from-literal=key_id=YOUR_ACCESS_KEY_ID \
  --from-literal=access_key=YOUR_SECRET_ACCESS_KEY \
  -n kube-system
```

#### Step 3: Create StorageClass
```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: s3-storage
provisioner: s3.csi.aws.com
parameters:
  mounter: mountpoint-s3           # Use Mountpoint for S3
  bucketName: my-k8s-bucket        # S3 bucket name
  region: us-west-2                # AWS region
mountOptions:
  - allow-delete                   # Allow delete operations
  - region=us-west-2
```

#### Step 4: Create PVC and Use It
```yaml
# PVC
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: s3-storage-pvc
spec:
  accessModes:
    - ReadWriteMany      # S3 supports multiple Pod access
  resources:
    requests:
      storage: 100Gi     # Logical capacity
  storageClassName: s3-storage

---
# Application Pod
apiVersion: v1
kind: Pod
metadata:
  name: s3-app-pod
spec:
  containers:
  - name: app
    image: nginx
    volumeMounts:
    - name: s3-data
      mountPath: /data
    env:
    - name: AWS_ACCESS_KEY_ID
      valueFrom:
        secretKeyRef:
          name: aws-secret
          key: key_id
    - name: AWS_SECRET_ACCESS_KEY
      valueFrom:
        secretKeyRef:
          name: aws-secret
          key: access_key
  volumes:
  - name: s3-data
    persistentVolumeClaim:
      claimName: s3-storage-pvc
```

### ğŸ” Verification and Testing

```bash
# Check PVC status
kubectl get pvc

# Test data writing
kubectl exec s3-app-pod -- sh -c "echo 'Hello S3!' > /data/test.txt"

# Check S3 bucket contents
aws s3 ls s3://my-k8s-bucket/
```

## ğŸ¯ Summary

| Feature | Local Storage | NFS Storage | S3 Cloud Storage |
|---------|---------------|-------------|------------------|
| Requires CSI | âŒ K8s built-in | âœ… Needs CSI | âœ… Needs CSI |
| Access Mode | ReadWriteOnce | ReadWriteMany | ReadWriteMany |
| Use Cases | High performance, temporary data | Shared files, static resources | Object storage, backup archival |
| Performance | Highest (local disk) | Network limited | Network limited, high throughput |
| Availability | Node dependent | NFS server dependent | Cloud service, 99.9% available |
| Data Persistence | Lost on node failure | Server-level persistence | Cloud-level persistence, multi-replica |
| Cost | Lowest (no extra cost) | Self-hosted NFS cost | Pay-per-use |
| Cross-Node Access | âŒ Not supported | âœ… Supported | âœ… Supported |

Remember these simple principles:
- ğŸ  Local Storage --> K8s built-in --> No CSI needed
- ğŸŒ External Storage --> Requires CSI --> Standardized interface
- Production environments typically use CSI
