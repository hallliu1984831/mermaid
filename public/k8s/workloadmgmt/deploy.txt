章节1:
大家好，今天来聊一聊在Kubernetes集团中，Pod的直属领导们：ReplicaSet 和 Deployment。
Pod：共享宿舍
在 K8S 的世界里，Pod 是最小的调度单元，类似一间共享宿舍，里面住着一群舍友（容器）。
Pod 的特色：
共享资源：Pod 里的舍友共享插座、WiFi、饮水机等资源，就像容器共享网络和存储。
生死与共：Pod 是一个整体，如果宿舍房间出问题了（Pod 挂了），所有舍友都会一起搬家（全部容器被清理）。
短期租赁：Pod 的寿命通常很短，今天住的是一群考研党，明天可能就换成了游戏开黑团队。

ReplicaSet：宿舍楼管大妈

K8S 中的宿舍管理器，负责确保宿舍房间（Pod）的数量始终符合要求。

宿管职责：
确保宿舍满员：楼管每天统计宿舍的人数。如果某个宿舍的舍友“突然退学”（Pod 掉了），楼管会立刻安排新舍友入住，确保宿舍不会空着。
扩招/缩招：学校突然扩招了？楼管会迅速联系后勤，安排新宿舍上线；如果学校缩招了，楼管也会负责清退多余的宿舍。
服从上级：虽然楼管负责执行具体管理，但她并不决定宿舍楼的总房间数，她的所有工作听命于总宿舍管理员（Deployment）。

Deployment——总宿舍管理员
K8S 中用来管理无状态应用的控制器，它负责整个宿舍楼的规划和管理，楼管（ReplicaSet）只是她的“执行工具人”。

管理员职责：
分配宿舍数量
总管理员会告诉楼管：“我们需要 5 间宿舍，每间住 4 个人。” 楼管就根据这个要求安排宿舍数量，并持续保证每间宿舍的人数满员。
宿舍升级改造
发现宿舍设施老旧（容器版本过时了）？总管理员会安排宿舍升级，比如换新床、装空调。这种升级通常是滚动进行，一个宿舍一个宿舍地改造，避免所有宿舍同时停用导致全楼瘫痪。
快速回滚
如果新宿舍的设施有问题，比如空调漏水，总管理员会迅速撤销升级计划，把宿舍恢复到之前的样子（旧版本）。
新增策略支持
总管理员还可以制定各种“高级策略”，比如：
蓝绿部署：新宿舍和旧宿舍同时存在一段时间，看看哪个更受舍友喜爱。
金丝雀部署：先试点升级一两个宿舍，确保没问题再推广到全楼。

章节2:
大家好，昨天介绍了Kubernetes中Pod的直属领导们：ReplicaSet 和 Deployment，今天来说说为什么需要它们。
pod是易失的：作为K8S中的最小部署单元，它的存在是短暂的，可能因为节点故障、更新操作等原因被销毁或重启。
难以保证应用的高可用性：手动管理多个 Pod 时，很难实时监控并确保应用始终有足够的实例在线。
复杂的版本管理需求：在实际生产环境中，应用需要不断更新（如升级到新版本的容器镜像、修改配置），Pod 无法应对

引入 ReplicaSet 的原因：

ReplicaSet 的核心目标是确保指定数量的 Pod 持续运行，自动补充缺失的 Pod，实现高可用性。

自动扩缩容：

通过设置副本数（replicas），ReplicaSet 可以自动创建或删除 Pod，确保集群中运行的 Pod 数量符合需求。
自愈能力（自动修复）：当 Pod 因节点故障或其他问题挂掉时，ReplicaSet 会自动检测并重新创建新的 Pod。


虽然 ReplicaSet 能确保 Pod 的数量，但它无法管理 Pod 的版本更新

引入 Deployment 的原因：

Deployment 是在 ReplicaSet 的基础上增加了一层抽象，专注于声明式管理应用的生命周期，解决了复杂场景下的版本管理和扩缩容需求。

声明式管理：
管理员只需要声明期望的目标状态（如 Pod 的副本数、版本等），Deployment 会自动创建或更新底层的 ReplicaSet，使集群达到目标状态。
滚动更新：
Deployment 支持逐步替换 Pod 的版本，在不中断服务的情况下完成升级。例如，逐个替换旧版本的 Pod，直到所有 Pod 都更新为新版本。
快速回滚：
如果新版本的应用出现问题，可以快速回滚到之前的版本，避免服务长时间不可用。它记录历史版本，支持一键恢复。
扩展发布策略：
支持复杂的发布策略，比如：蓝绿部署/金丝雀发布

与 ReplicaSet 的协作：
Deployment 内部会自动创建和管理 ReplicaSet，确保底层 Pod 的数量和状态。
每次 Deployment 更新时，会创建一个新的 ReplicaSet 来管理新版本的 Pod。

一句话总结

ReplicaSet：解决了 Pod 的自动扩缩容和自愈问题，让应用始终保持高可用。
Deployment：在 ReplicaSet 之上，提供了声明式管理、滚动更新和回滚能力，满足了复杂应用场景下的运维需求。

通过引入这两者，Kubernetes 构建了一个高效、可靠、自动化的应用管理系统，让开发和运维人员的工作更加轻松！

章节3:
大家好！今天我们来聊聊 Kubernetes 中的 Service 和 Endpoint。在有了 Pod 的自动管理系统（Deployment 和 ReplicaSet）之后，我们还需要解决一个重要问题：如何高效地访问 Pod？

这时，就需要 Service 出场了！

Service：外卖配送调度中心
由于共享宿舍（Pod）的“宿舍房间”随时可能发生变化（如扩容、缩容、或者翻修换房间），外卖员（客户端或外部流量）不可能直接找到这些动态变化的房间地址。
这时候，Service 就像一个“外卖配送调度中心”，它负责：
固定入口地址：
Service 给整个宿舍分配了一个固定的“调度中心地址”（IP 或域名）。外卖员只需要记住这个固定地址，无需关心宿舍房间（Pod）具体地址的变化。
智能分单：
调度中心会把外卖订单分配给正在营业的房间（Pod），确保任务被合理分发。


Endpoint：调度中心的动态地址簿
Endpoint 是 Service 的“地址簿”，它会自动监控 Pod 的变化，动态更新活跃房间的名单：
动态更新：
如果某个 Pod（房间）挂了，Endpoint 会立刻将其从地址簿中移除；如果新增了 Pod，Endpoint 会自动添加它到名单中。
分单逻辑：
每次接到外卖订单时，调度中心（Service）都会翻看最新的地址簿（Endpoint），将订单分配给还在“营业”的房间。挂掉的房间不会再接到任务。


如何创建 Service？
如下图（示例图 2），我们可以通过 YAML 文件定义一个 Service 对象，并将其与一组 Pod 关联起来。

注意事项：
示例中的 Pod 使用的是 busybox 镜像，它默认不会监听 80 端口。因此，YAML 中定义的 port 和 targetPort 只是演示用，实际上这个 Pod 没有任何端口可以被 Service 访问。

当执行以下命令后，Kubernetes 会自动创建相应的 Endpoint：
kubectl apply -f <SERVICE>.yaml

章节4:
大家好，之前我用形象的类比介绍了 pod 这个共享宿舍和它的直属领导，今天来介绍 deployment 的伙伴们。

StatefulSet：辅导员专属宿舍，房间永远不变
如果把K8S的 worker 节点想象一栋宿舍楼，这栋楼一定会有几件“辅导员宿舍”，每个房间门口都挂着专属门牌号：101、102、103。房间号一旦分配，永远不变，学生换了也不影响房间的“身份”，送走了毕业生，辅导员还等着新人来报道呢。
StatefulSet 就是给那些需要“专属房间号”的应用准备的，比如数据库、分布式存储等。虽然辅导员宿舍（Pod）可以换，但宿舍（Pod 名字和存储）永远不变，数据和身份都稳稳当当！

DaemonSet：每栋楼的配电间
你以为每栋宿舍楼只有房间和楼管？错！还有一个神秘存在——配电间。DaemonSet 就像是每栋楼的配电间，无论新盖几栋楼，每栋都必须有一个，负责全楼供电、网络、安防等基础设施。
只要新楼一建好，唯一的配电间（DaemonSet Pod）就会自动安排到位，绝不缺席。
比如日志收集、监控探针、杀毒软件这些“全楼服务”，都靠 DaemonSet 的“配电间”默默守护。它要是罢工了，那整栋楼就不能正常运转了，要么缺了监控，要么漏了日志，总之必须有它，没有之一。


Cronjob：定时来打扫卫生的清洁阿姨
还有一位默默奉献的“清洁阿姨”。她每天（或每周、每月）定点来宿舍楼打扫卫生、收拾垃圾、消毒杀菌。
清洁阿姨不会一直待在楼里，只有到点才会出现，任务完成后就离开，等下次再来。
有时候是早上7点来扫地，有时候是晚上来收垃圾，时间表全靠提前安排好。
有了她，宿舍楼（集群）才能一直干净整洁、井井有条！

Job：水电工
作为 K8S 里的支持人员，他们专门负责定时执行一次性任务，比如备份、清理、导入/到处数据等。
水电工来了就干活，干完就走，不会常驻。


好，到现在为止，我介绍了kubernetes里创建 pod 的主要控制器，来看看它们是如何创建 pod 的。如图2，3，4

K8S 的这些‘宿舍角色’，让集群管理像生活一样井井有条、充满烟火气。

章节5:
再探Kubernetes Pod访问：一招鲜吃遍天
大家好，之前我们聊过了在使用deployment 控制器生成 pod 的场合，如何高效访问 Pod，今天我来补充一下其他控制器访问 pod 的方式，来看看有啥新鲜的玩意。

核心真相：Service是万能钥匙
99%的情况： 不管什么控制器创建的Pod，都用Service访问！
Deployment的Pod → Service访问 ✅
DaemonSet的Pod → Service访问 ✅
Job的Pod → Service访问 ✅
ReplicaSet的Pod → Service访问 ✅

唯一例外：StatefulSet - 这个特立独行的家伙！
为什么特殊？ 因为它的Pod有身份证（固定hostname），毕竟作为“辅导员”的 statefulset 房间标识还是要特殊一些，必须要好找。访问方式如图2，需要创建 headless service：
无虚拟IP - K8s不会分配ClusterIP
直接解析 - DNS直接返回Pod IP列表
访问特色：
实名访问：pod-0.service-name.namespace.svc.cluster.local
点对点：想找mysql-0就找mysql-0，想找mysql-1就找mysql-1
有状态专用：数据库、消息队列的最爱

章节6:
大家好！前面我们搞定了K8s内部的"宿舍管理"，现在该让外面的人进来参观、使用了。

1️⃣ 核心问题：怎么访问K8S的服务 ？
内部访问： 住在宿舍里的同学互相串门 - ClusterIP 就够了
外部访问： 校外的朋友要来玩 - 需要特殊的"通行证"！
Service的外部访问只有两条路：NodePort 和 LoadBalancer 😂

2️⃣ NodePort：每栋楼开个小门
工作原理： 在每个节点上开个固定端口，像每栋宿舍楼都开个侧门，门牌号30000-32767
访问方式： 任意节点IP + 端口号，比如 192.168.1.100:30080

✅ 免费使用 - 不花钱，学生的最爱
✅ 多个入口 - 每个节点都有门，坏了一个还有其他的
✅ 简单粗暴 - 配置简单，立马能用

❌ 端口奇葩 - 谁记得住30080是啥服务？像古代暗号
❌ 用户困惑 - 普通用户看到端口号就懵了

3️⃣ LoadBalancer：请个专业门卫
工作原理： 云厂商提供专业的负载均衡器，像请了个保安来指路
访问方式： 直接用外部IP或域名，比如 http://my-app.com

云部署集群（有钱人的选择）：
✅ 专业服务 - 云厂商的负载均衡器，高大上
✅ 真正的外部IP - 不用记奇葩端口号，用户体验满分
✅ 智能分发 - 自动负载均衡，井然有序
但是要花钱 - 云厂商服务按小时计费

本地部署集群（高性价比）：
✅ 局域网内有效 - 只能在内网访问
✅ EXTERNAL-IP永远pending - 没有云厂商撑腰，没有外部IP可以直接通过浏览器访问
想体验LoadBalancer，需要额外方案 - 要么装MetalLB模拟IP资源池（只能在内网访问），要么老实用NodePort

4️⃣ 小结
NodePort = 自己开门 - 便宜但端口奇葩，而且不安全
LoadBalancer = 请门卫 - 专业且让人放心，但要花钱

Service外部访问就两招：NodePort自己开门，LoadBalancer请门卫，选哪个看钱包！

到此篇为止，我已经完整介绍了 POD 相关的创建，控制器，访问，对外暴露等。有了这些基本知识，服务基本可以上线了 👍

章节7:
Pod装修指南：给共享宿舍来个精装修 🏠

大家好！前面我们聊了Pod这个"共享宿舍"和它的管理员们，也知道了怎么开门迎客。今天该给这个宿舍好好装修一下了！毕竟，光有房子不行，还得有密码锁、限电器这些基础设施，不然舍友们住得不安全也不舒服。

核心问题：宿舍需要哪些装修？

基础装修清单
Secret（保险箱）：存放重要的密码和证件
Resource Limits（限电器）：防止某个舍友用电过度
ConfigMap（公告板）：贴一些公共配置信息
Volume（储物柜）：数据持久化存储

今天重点聊前两个：保险箱和限电器！

Secret：宿舍里的小保险箱 🔐
就像宿舍里放个小保险箱，专门存放重要的密码、证件、私房钱

必要性：
数据库密码不能明文写在代码里（就像不能把银行卡密码贴在卡上）
API密钥需要安全存储（泄露了就完蛋）
证书文件要保密（HTTPS证书很重要）
请参考前篇文章 “Kubernetes Secret 踩坑案例”，具体了解 secret 的配置

举个栗子：
Secret就像宿舍的小保险箱，只有知道密码的舍友才能打开
环境变量就像把保险箱里的东西悄悄递给需要的人
base64编码就像给密码穿了件"隐身衣"，不是真正的加密但至少不会一眼看穿

Resource Limits：宿舍限电器
就像宿舍装个智能限电器，防止某个舍友开太多电器把整个宿舍电闸跳了

必要性：
防止"电老虎"舍友：某个容器占用过多CPU/内存
保证公平使用：每个容器都有自己的资源配额
避免宿舍"停电"：防止整个节点资源耗尽

举个栗子：
requests：就像宿舍保证每个人至少有1000W的用电额度
limits：就像限电器设置最高不能超过3000W，超了就跳闸
没有limits：就像某个舍友开了10个暖风机，把整栋楼都搞停电了

来一个给nginx宿舍装修的案例：

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-luxury-edition
spec:
  replicas: 2
  selector:
    matchLabels:
      app: nginx-luxury
  template:
    metadata:
      labels:
        app: nginx-luxury
    spec:
      containers:
      - name: nginx
        image: nginx:latest
        # 🔐 从保险箱里拿密码
        env:
        - name: ADMIN_USER
          valueFrom:
            secretKeyRef:
              name: nginx-secrets
              key: admin-user
        - name: ADMIN_PASS
          valueFrom:
            secretKeyRef:
              name: nginx-secrets
              key: admin-pass
        # ⚡ 安装限电器
        resources:
          requests:
            memory: "64Mi"    # 基础内存：64MB
            cpu: "50m"        # 基础CPU：0.05核心
          limits:
            memory: "128Mi"   # 最大内存：128MB
            cpu: "100m"       # 最大CPU：0.1核心
        ports:
        - containerPort: 80
```

装修前（毛坯）：
❌ 密码明文写在配置里，一眼就被看穿
❌ 没有资源限制，某个容器可能把节点搞崩
❌ 就像住在毛坯房里，啥都没有

装修后（像模像样了）：
✅ 密码安全存储在Secret里，需要授权才能访问
✅ 资源使用有序，每个容器都有自己的"用电额度"
✅ 就像住进了精装修公寓，安全又舒适

Pod装修就像宿舍装修：Secret是保险箱保护隐私，Resource Limits是限电器保证公平，装修到位了大家住得才安心！



章节8:
Pod装修进阶：公告板和储物柜的艺术 📋

大家好！上次我们给共享宿舍装了保险箱和限电器，今天继续装修大业，聊聊另外两个必备神器：公告板和储物柜！毕竟，一个完美的宿舍不仅要安全（Secret），还要有序（ConfigMap）和实用（Volume）。

今日装修清单：
ConfigMap（公告板）：贴一些公共配置信息，让大家都知道WiFi密码，啥时候停水停电等
Volume（移动储物柜）：数据持久化存储，重要东西不能丢

ConfigMap：宿舍里的智能公告板，就像宿舍门口贴个智能公告板，写着WiFi密码、垃圾分类规则、外卖电话等公共信息
配置信息要公开透明（不像Secret那么神秘）
多个应用共享配置（大家都用同一个WiFi）
配置变更要方便（换个WiFi密码不用重装系统）

注意点：
不要把敏感信息放公告板上（那是保险箱的活）
配置变更后记得重启Pod（公告板更新了要通知大家）
合理组织配置结构（不要把公告板贴得乱七八糟）

ConfigMap的创建：
```yaml
# 智能公告板配置
apiVersion: v1
kind: ConfigMap
metadata:
  name: dorm-config
data:
  wifi-password: "dorm123456"
  takeout-phone: "400-123-4567"
  cleaning-schedule: "Monday: 小明, Tuesday: 小红"
  app.properties: |
    server.port=8080
    database.url=jdbc:mysql://localhost:3306/mydb
    log.level=INFO
```


Volume：宿舍里的移动储物柜
工作原理： 就像给宿舍配个移动储物柜，重要东西放里面，搬家时也能带走
数据要持久化（Pod重启数据不能丢）
多个容器共享数据（舍友之间共享文件）
数据要备份（重要文件不能只存一份）

注意点：
选择合适的存储类型（临时用emptyDir，持久用PVC）
注意挂载路径冲突（不要把两个储物柜放同一个位置）
定期备份重要数据（储物柜也可能丢失）

Volume的类型：
emptyDir：临时储物箱，Pod删除时数据就没了；缓存文件、临时日志
hostPath：就像借用房东家的储物间，数据存在节点上；需要访问节点文件系统
persistentVolumeClaim：就像租个专业储物柜，数据永久保存；数据库文件、用户上传文件


实际装修案例：给nginx宿舍升级装修

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-decorate-v2
spec:
  replicas: 2
  selector:
    matchLabels:
      app: nginx-decorate
  template:
    metadata:
      labels:
        app: nginx-decorate
    spec:
      containers:
      - name: nginx
        image: nginx:latest
        # 从公告板读取配置
        env:
        - name: SERVER_NAME
          valueFrom:
            configMapKeyRef:
              name: nginx-config
              key: server-name
        # 挂载储物柜
        volumeMounts:
        - name: nginx-config-volume
          mountPath: /etc/nginx/conf.d
        - name: nginx-data-volume
          mountPath: /usr/share/nginx/html
        - name: nginx-logs-volume
          mountPath: /var/log/nginx
        ports:
        - containerPort: 80
      volumes:
      # 公告板挂载
      - name: nginx-config-volume
        configMap:
          name: nginx-config
      # 数据储物柜
      - name: nginx-data-volume
        persistentVolumeClaim:
          claimName: nginx-data-pvc
      # 日志储物柜
      - name: nginx-logs-volume
        emptyDir: {}
```

装修后：
- ✅ 智能公告板（ConfigMap）：配置管理
- ✅ 移动储物柜（Volume）：数据持久化
- ✅ 就像从经济型酒店升级到五星级套房

总结
第二轮装修后：Secret保险箱守护隐私，ConfigMap公告板分享信息，Volume储物柜保存数据，Resource Limits限电器控制资源，宿舍生活越来越好了！


章节9:
Pod装修终极版：选宿舍楼的艺术与准入指南 �

大家好！前面我们给共享宿舍装了保险箱、限电器、公告板、储物柜，今天来聊聊最高级的装修技能：选宿舍楼！住哪栋楼直接影响居住体验。

选楼指南

Node Affinity（选楼偏好）：我想住哪种类型的楼
Pod Affinity（邻居偏好）：我想住在某些服务附近
Pod Anti-Affinity（避坑指南）：我绝对不和某些应用住同楼
Taint/Toleration（准入制度）：特殊楼栋需要通行证

Node Affinity：选楼类型 🏢

工作原理： 根据楼栋设施选择，就像"我只住有空调的楼"

使用场景：
- GPU应用要住有显卡的楼
- 就近部署，降低网络延迟

Pod Affinity：选邻居 🤝

工作原理： 选择"靠近图书馆的宿舍"，希望和某些服务住得近

使用场景：
- Web应用住在缓存服务附近，降低延迟
- 相关服务住一起，方便数据共享

Pod Anti-Affinity：避坑指南 ⚡

工作原理： "避雷指南"，绝对不和某些应用住同一栋楼

使用场景：
- 多个副本分散部署，避免"一锅端"
- CPU密集型应用分开，避免抢资源

Taint & Toleration：准入制度 🏷️

工作原理： 给楼栋贴标签"仅限研究生"，只有符合条件的应用才能入住

Taint（楼栋标签）： 管理员设置准入要求，即接受哪些人入住
Toleration（通行证）： 应用声明资格，即声明我愿意入住一定标准的楼。
二者结合，才能双向奔赴，达到最好的匹配状态

选楼策略对比 📊

| 策略类型 | 作用 | 使用场景 |
|---------|------|----------|
| Node Affinity | 选楼栋类型 | 选择特定硬件节点 |
| Pod Affinity | 选邻居 | 服务间协作优化 |
| Pod Anti-Affinity | 避坑指南 | 高可用分散部署 |
| Taint/Toleration | 准入制度 | 专用节点管理 |

调度策略优先级 🎯

Required（硬性要求）： 必须满足，否则不能入住
Preferred（软性偏好）： 尽量满足，但不是必须的

实际选楼案例 🏗️

高可用Web应用需求：
- 3个副本分散部署（避免单点故障）
- 希望住在缓存服务附近（提高性能）
- 只能住计算型节点（硬件要求）

选楼策略：
- Node Affinity：只选计算型楼栋
- Pod Affinity：优先靠近Redis缓存
- Pod Anti-Affinity：3个副本住不同楼栋

常见搭配 🏠

黄金搭配：
- Web服务靠近缓存：降低延迟
- 数据库主从分离：提高可用性

避坑组合：
- 多个数据库副本：绝对不能住同楼
- CPU密集型应用：要分散部署

选楼小贴士 💡

- 不要设置过多限制：可能导致无楼可住
- 专用楼栋才设Taint：普通楼不要随便贴标签
- 考虑集群规模：小集群别设太严格要求

一句话总结 🎯

Pod终极装修：Node Affinity选楼栋，Pod Affinity/Anti-Affinity管邻居，Taint/Toleration控门槛，科学选楼让每个Pod都住得舒心！ 🏠

章节10:
Pod装修收尾工程：软装让生活更美好 ✨

大家好！前面我们给共享宿舍进行了3轮硬装修，今天该来聊聊软装了！毕竟，硬装解决"能住"的问题，软装解决"住得好"的问题。

软装三件套：
Health Checks（健康监测）：安装智能健康检测设备
Security Context（安全管家）：设置门禁权限和安全规则
Init Containers（保洁阿姨）：入住前的清洁和准备工作

Health Checks：就像给宿舍装个智能健康监测系统，随时检查宿舍的各种状况
- 及时发现问题：应用挂了要马上知道
- 避免服务中断：不健康的Pod不接收流量
- 自动恢复：检测到问题自动重启

三种健康检查：
1. Liveness Probe（生命体征检查）
```yaml
livenessProbe:
  httpGet:
    path: /health
    port: 8080
  initialDelaySeconds: 30
  periodSeconds: 10
```
就像每10秒问一次："你还活着吗？"如果连续几次没回应，就重启Pod

2. Readiness Probe（就绪检查）
```yaml
readinessProbe:
  httpGet:
    path: /ready
    port: 8080
  initialDelaySeconds: 5
  periodSeconds: 5
```
就像问："你准备好接客了吗？"没准备好就不给你分配流量

3. Startup Probe（启动检查）
```yaml
startupProbe:
  httpGet:
    path: /startup
    port: 8080
  failureThreshold: 30
  periodSeconds: 10
```
就像问："你启动完了吗？"给慢启动的应用更多时间

生动比喻：
- Liveness = 心跳检测器，确保还活着
- Readiness = 门铃，确认是否可以接待客人
- Startup = 闹钟，确认是否已经起床

Security Context：宿舍安全管家

工作原理： 就像给宿舍配个专业安全管家，管理谁能进、谁能做什么

为什么需要Security Context？
- 权限控制：不是谁都能当管理员
- 安全隔离：防止恶意操作
- 合规要求：满足安全审计标准

主要安全设置：
1. 用户权限控制
```yaml
securityContext:
  runAsUser: 1000        # 以普通用户身份运行
  runAsGroup: 3000       # 设置用户组
  runAsNonRoot: true     # 禁止以root运行
```

2. 文件系统权限
```yaml
securityContext:
  fsGroup: 2000          # 文件系统用户组
  readOnlyRootFilesystem: true  # 根文件系统只读
```

3. 特权控制
```yaml
securityContext:
  allowPrivilegeEscalation: false  # 禁止提权
  privileged: false                # 禁止特权模式
  capabilities:
    drop:
    - ALL                          # 删除所有特权
```

生动比喻：
- runAsUser = 给每个住户发身份证
- readOnlyRootFilesystem = 把重要文件锁在保险柜里
- capabilities = 限制住户的"超能力"

Init Containers：专业保洁阿姨 🧹

工作原理： 就像请专业保洁阿姨，在正式入住前把房间打扫干净、准备就绪

为什么需要Init Containers？
- 环境准备：数据库初始化、文件下载
- 依赖检查：确保依赖服务已启动
- 权限设置：修改文件权限、创建目录

Init Container配置：
```yaml
initContainers:
- name: init-myservice
  image: busybox:1.28
  command: ['sh', '-c', 'until nslookup myservice; do echo waiting for myservice; sleep 2; done;']
- name: init-mydb
  image: busybox:1.28
  command: ['sh', '-c', 'until nslookup mydb; do echo waiting for mydb; sleep 2; done;']
```

工作流程：
1. Init Container 1 执行完成
2. Init Container 2 执行完成
3. 主容器才开始启动

生动比喻：
- 像酒店的客房服务，客人入住前把一切准备好
- 按顺序工作，前一个保洁阿姨干完，下一个才开始
- 所有准备工作完成，客人才能入住

软装配置实战案例 

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webapp-deluxe
spec:
  replicas: 2
  selector:
    matchLabels:
      app: webapp-deluxe
  template:
    metadata:
      labels:
        app: webapp-deluxe
    spec:
      # 保洁阿姨先工作
      initContainers:
      - name: init-db-check
        image: busybox
        command: ['sh', '-c', 'until nc -z db-service 5432; do sleep 1; done']

      containers:
      - name: webapp
        image: nginx:latest
        ports:
        - containerPort: 80

        # 健康监测设备
        livenessProbe:
          httpGet:
            path: /
            port: 80
          initialDelaySeconds: 30
          periodSeconds: 10

        readinessProbe:
          httpGet:
            path: /
            port: 80
          initialDelaySeconds: 5
          periodSeconds: 5

        # 安全管家设置
        securityContext:
          runAsNonRoot: true
          runAsUser: 1000
          readOnlyRootFilesystem: true
          allowPrivilegeEscalation: false
```

软装效果对比 📊

硬装完成后：
- ✅ 有地方住（基础功能）
- ✅ 安全存储（Secret、ConfigMap）
- ✅ 资源管理（Resource Limits）
- ✅ 位置优化（Affinity）

软装完成后：
- ✅ 健康监控（随时知道状态）
- ✅ 安全防护（权限控制到位）
- ✅ 环境整洁（Init Container准备）
- ✅ 生活品质大幅提升

软装小贴士 💡

Health Checks建议：
- 设置合理的检查间隔：太频繁浪费资源，太稀疏发现问题慢
- 区分不同检查类型：Liveness检查要宽松，Readiness检查要严格
- 提供专门的健康检查接口：不要用业务接口做健康检查

Security Context建议：
- 最小权限原则：能用普通用户就别用root
- 只读文件系统：能只读就别可写
- 定期审计：检查是否有不必要的权限

Init Container建议：
- 保持简单：Init Container应该做简单的准备工作
- 快速执行：不要让Init Container运行太久
- 幂等操作：多次执行结果应该一样

一句话总结 🎯

Pod软装三件套：Health Checks当健康管家随时体检，Security Context当安全管家控制权限，Init Containers当保洁阿姨提前准备，让你的Pod住得安全、舒适、放心！ 🏠

*记住：硬装让Pod能跑起来，软装让Pod跑得更好！* ✨

章节11:
Kubernetes Namespace: 大学校园的院系管理

大家好！Kubernetes Pod 这个共享宿舍，在多篇连载的持续介绍下，加上操作视频的展示，基本介绍完毕。现在宿舍越来越多了，该考虑如何科学地分区管理了。就像一所大学有不同的院系一样，K8S 也需要用 Namespace 来划分不同的"院系"，让资源管理更有序。

1️⃣ 核心问题：宿舍多了怎么管理？ 🤔
按照常理，都需要按照专业职能来进行如下区分：
👉 计算机科学系的学生住一片区域
👉 物理系的学生住另一片区域
👉 每个院系有自己的管理制度和资源

2️⃣ Namespace：院系分区管理，就像大学把学生按院系分区管理，Namespace 把 K8s 资源按用途分组管理
👉 资源隔离：计算机系的实验不会影响物理系
👉 权限管理：每个院系有自己的管理员
👉 命名冲突：不同院系可以有同名的实验室
👉 资源配额：每个院系有自己的预算和资源限制

3️⃣ 集群预置 Namespace
👉 default（主校区）：就像大学的主校区，没有特别指定院系的都在这里
“kubectl get pods” 默认显示default 命名空间的Pod

👉 kube-system（行政管理区）：就像大学的行政楼，校长、教务处、后勤部门都在这里
“kubectl get pods -n kube-system” 

👉 kube-public（公共区域）：就像大学的图书馆、体育馆等公共设施
kubectl get pods -n kube-public

4️⃣ 创建 Namespace
如图2，创建命名空间可使用命令行或者 yaml 文件，有了自定义的命名空间以后，就可以在它的地盘通过控制器来创建 Pod 了，如图3。

5️⃣ Namespace的各项操作
如图4：
👉 跨院系的访问需要指定服务所在的 namespace 并符合特定的格式
👉 查看各院系的部署对象、资源等（当然，你得有足够的权限才可以）
👉 清理命名空间（这可是删库跑路的节奏🎵，需谨慎）

6️⃣ Namespace就像大学的院系管理：不同院系有各自的学生、老师、实验室和预算，既保持独立又能协作，让整个校园井然有序！合理的院系划分是大学管理的基础，也是K8s集群管理的关键！


1️⃣ 创建新院系
👉 创建计算机科学系 (computer-science.yaml)：
```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: computer-science
  labels:
    department: "cs"
    budget: "high"
```
kubectl apply -f computer-science.yaml

👉 命令行快速创建：
```bash
kubectl create namespace computer-science
```

2️⃣ 在指定院系部署应用 
在计算机科学系部署Web应用：
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-app
  namespace: computer-science  # 指定院系
spec:
  replicas: 3
  selector:
    matchLabels:
      app: web-app
  template:
    metadata:
      labels:
        app: web-app
    spec:
      containers:
      - name: nginx
        image: nginx:latest
        ports:
        - containerPort: 80
```

1️⃣ 跨院系访问 
👉 院系内部访问（简单）：
```bash
# 在computer-science院系内
curl http://web-service
```
👉 跨院系访问（需要全名）：
```bash
# 从computer-science访问physics的服务
curl http://database-service.physics.svc.cluster.local
```

2️⃣ Kubernetes DNS命名规则：
```
<service-name>.<namespace>.<service-type>.cluster.local
```

3️⃣ 院系资源管理 
查看各院系资源：
```bash
# 查看所有院系
kubectl get namespaces
# 查看计算机科学系的所有资源
kubectl get all -n computer-science
# 查看物理系的Pod
kubectl get pods -n physics
```

4️⃣ 设置默认院系：
```bash
# 设置默认工作在computer-science院系
kubectl config set-context --current --namespace=computer-science
# 现在所有命令默认在CS院系执行
kubectl get pods  # 等同于 kubectl get pods -n computer-science
```

5️⃣ 资源清理：
```bash
# 删除整个院系（谨慎操作！）
kubectl delete namespace computer-science
# 这会删除该院系下的所有资源
```

章节12:
ResourceQuota：院系预算管理办公室 🏛️

大家好！今天接着上篇的namespace，继续Kubernetes的资源管理话题。大学的院系（namespace）分好了，但问题来了：如果不加管理，计算机科学系可能把整个学校的电费都用光！今天聊聊如何给院系设预算。

核心问题：如何防止某个院系把学校搞破产？ 🤔

精细管理是稳步发展的关键，来看看下面的情况：

👉 计算机科学系：挖矿、AI训练，电费爆表
👉 物理系：粒子加速器24小时运转
👉 化学系：各种设备全开，水电费飞涨
👉 学校财务：这个月预算超支500%！😱

这时候，需要ResourceQuota（院系预算管理办公室）来把关了！

ResourceQuota工作原理 💰

就像给每个院系设置年度预算，超了就不能再申请资源，防止某个院系把所有资源都占了。

为什么需要ResourceQuota？
- 防止资源垄断：某个院系不能把所有资源都占了
- 成本控制：每个院系有明确的资源预算
- 公平分配：确保各院系都能获得合理资源
- 避免雪崩：防止某个院系的问题影响整个集群

回顾：Pod的个人资源设置
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: student-pod
spec:
  containers:
  - name: app
    image: nginx
    resources:
      requests:          # 保底需求 ⬇️
        memory: "128Mi"   # 至少要128MB内存
        cpu: "100m"       # 至少要0.1个CPU核心
      limits:            # 最高上限 ⬆️
        memory: "512Mi"   # 最多用512MB内存
        cpu: "500m"       # 最多用0.5个CPU核心
```
- requests = 保证的基础用电量（1000W）
- limits = 断路器设置（3000W，超了就跳闸）

ResourceQuota：院系预算管理办公室 🏛️

给每个院系设置年度预算，超了就不能再申请资源，防止某个院系把所有资源都占了。

计算机科学系的预算设置：
```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: cs-budget
  namespace: computer-science
spec:
  hard:
    # CPU和内存总预算
    requests.cpu: "10"           # 总共10个CPU核心
    requests.memory: 20Gi        # 总共20GB内存
    limits.cpu: "20"             # 最高峰值20个CPU核心
    limits.memory: 40Gi          # 最高峰值40GB内存

    # 对象数量限制
    pods: "50"                   # 最多50个Pod（50个学生）
    services: "10"               # 最多10个Service
    secrets: "20"                # 最多20个Secret
    configmaps: "30"             # 最多30个ConfigMap

    # 存储预算
    persistentvolumeclaims: "10" # 最多10个存储申请
    requests.storage: 100Gi      # 总共100GB存储
```

物理系的预算设置（预算较少）：
```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: physics-budget
  namespace: physics
spec:
  hard:
    requests.cpu: "5"            # 总共5个CPU核心
    requests.memory: 10Gi        # 总共10GB内存
    limits.cpu: "10"             # 最高峰值10个CPU核心
    limits.memory: 20Gi          # 最高峰值20GB内存
    pods: "20"                   # 最多20个Pod
    services: "5"                # 最多5个Service
```

实际效果演示 🎬

创建超预算的Pod会怎样？
```yaml
# 尝试在physics系创建大内存Pod
apiVersion: v1
kind: Pod
metadata:
  name: big-pod
  namespace: physics
spec:
  containers:
  - name: app
    image: nginx
    resources:
      requests:
        memory: 15Gi  # 超过physics系的总预算10Gi
```

结果：
```
Error: exceeded quota: physics-budget, requested: requests.memory=15Gi,
used: requests.memory=8Gi, limited: requests.memory=10Gi
```
*翻译：物理系预算不够了！你要15GB，但总预算只有10GB，已经用了8GB*

查看和管理
```bash
# 查看所有院系配额
kubectl get quota --all-namespaces

# 查看预算使用情况
kubectl describe quota cs-budget -n computer-science

# 实时监控资源使用
kubectl top pods -n computer-science
```

最佳实践 💡

- 生产环境：设置严格预算防止资源滥用
- 开发环境：可以宽松一些便于调试和实验
- 监控告警：预算使用超过80%时告警
- 定期审查：根据实际使用情况调整配额
- 分层管理：不同重要级别的院系设置不同预算

一句话总结 🎯

ResourceQuota就像大学的财务处，给每个院系设置明确的资源预算，防止某个院系把学校搞破产，确保资源公平分配！ 🏛️

*记住：没有预算管理的大学会破产，没有ResourceQuota的K8s集群会崩溃！* 💰

章节13:
LimitRange：宿舍用电用水标准 🏠

大家好！上一章我介绍了给院系设置预算（ResourceQuota），今天来聊聊如何给每个宿舍制定用电用水标准。有了院系总预算这个大盘，还需要管好每个宿舍的具体使用情况，这样才能对 K8S 集群进行更精细的管理，一句话，省💰！尤其是你在用云端资源的场合😂

1️⃣ LimitRange：就像宿舍管理规定，给每个宿舍设置用电用水的标准和上限，防止某个学生把整栋楼电用光，同时给新学生设置标准配置。
想象一下这些场景：
👉 计算机系学生：在宿舍挖矿，功率拉满，整栋楼跳闸
👉 物理系研究生：实验设备24小时运转，电费爆表
👉 新入学学生：不知道标准配置，申请资源时一脸懵
👉 宿舍管理员：每天处理各种资源纠纷，头疼不已
这时候，需要LimitRange（宿舍用电用水标准）来规范管理！

2️⃣ 为什么需要LimitRange？
👉 防止个别宿舍过度消耗：某个学生不能把整栋楼电用光
👉 设置合理默认值：新学生入住时有标准配置
👉 强制最低标准：确保每个应用都有基本资源
👉 简化管理：统一的标准减少管理复杂度

3️⃣ 与ResourceQuota的关系 🔗
ResourceQuota = 院系总预算（计算机系总共100kW电力配额）
LimitRange = 宿舍标准（每个宿舍最多3kW，默认1kW）

两者配合使用：
👉 ResourceQuota控制总量
👉 LimitRange控制单体
👉 双重保险，确保资源合理分配

4️⃣ LimitRange的创建、使用以及维护等场景，请查看图2，3，4

5️⃣ 使用建议
👉 设置合理默认值：新应用有标准配置，减少配置错误
👉 上限不要太严格：给特殊需求留空间
👉 下限要保证基本运行：避免资源不足导致故障
👉 配合ResourceQuota使用：双重保险确保资源合理分配
👉 定期审查标准：根据实际使用情况调整标准

良好管理的宿舍标准让大家住得安心，好的 LimitRange 让 Pod 跑得稳定！


宿舍标准配置实战 📋

计算机科学系的宿舍标准：
```yaml
apiVersion: v1
kind: LimitRange
metadata:
  name: cs-dorm-standards
  namespace: computer-science
spec:
  limits:
  # Pod级别的限制（整个宿舍）
  - type: Pod
    max:
      cpu: "2"                   # 单个Pod最多2个CPU核心
      memory: 4Gi                # 单个Pod最多4GB内存
    min:
      cpu: "100m"                # 单个Pod至少0.1个CPU核心
      memory: 128Mi              # 单个Pod至少128MB内存

  # Container级别的限制（单个学生）
  - type: Container
    default:                     # 默认配置（新学生标准）
      cpu: "200m"                # 默认0.2个CPU核心
      memory: 256Mi              # 默认256MB内存
    defaultRequest:              # 默认请求（保底配置）
      cpu: "100m"                # 保底0.1个CPU核心
      memory: 128Mi              # 保底128MB内存
    max:                         # 单个容器上限
      cpu: "1"                   # 最多1个CPU核心
      memory: 2Gi                # 最多2GB内存
    min:                         # 单个容器下限
      cpu: "50m"                 # 至少0.05个CPU核心
      memory: 64Mi               # 至少64MB内存
```

物理系的宿舍标准（更严格）：
```yaml
apiVersion: v1
kind: LimitRange
metadata:
  name: physics-dorm-standards
  namespace: physics
spec:
  limits:
  - type: Container
    default:
      cpu: "100m"                # 默认更少的CPU
      memory: 128Mi              # 默认更少的内存
    defaultRequest:
      cpu: "50m"
      memory: 64Mi
    max:
      cpu: "500m"                # 上限也更低
      memory: 1Gi
```

实际效果演示 🎬

新学生入住时的自动配置：
```yaml
# 新学生创建Pod时没有指定resources
apiVersion: v1
kind: Pod
metadata:
  name: new-student-pod
  namespace: computer-science
spec:
  containers:
  - name: app
    image: nginx
    # 没有指定resources，LimitRange会自动设置默认值
```

K8s会自动添加：
```yaml
resources:
  requests:
    cpu: "100m"      # 来自defaultRequest
    memory: 128Mi
  limits:
    cpu: "200m"      # 来自default
    memory: 256Mi
```

查看和管理
```bash
# 查看宿舍标准
kubectl describe limitrange cs-dorm-standards -n computer-science

# 查看所有院系的宿舍标准
kubectl get limitrange --all-namespaces

# 检查Pod的实际资源配置
kubectl describe pod new-student-pod -n computer-science
```

一句话总结 🎯

LimitRange就像宿舍管理规定，给每个宿舍设置用电用水标准，防止个别学生作妖，同时给新学生提供标准配置！ 🏠

*记住：好的宿舍标准让大家住得安心，好的LimitRange让Pod跑得稳定！* ⚡

章节14:
PV & PVC：大学的储物柜管理系统 📦
大家好！前面聊完了院系预算和宿舍标准，今天来聊校园的储物管理！大学有储物管理系统，K8S也有存储管理体系，毕竟 Pod 这个共享宿舍如果总是重启的话，数据丢失就说不过去了😂

1️⃣ 数据放哪里才不会丢？ 🤔
👉 网站用户上传的图片，Pod重启后全没了 😭
👉 数据库存储的订单信息，容器挂了就消失了 😱
👉 应用生成的日志文件，重新部署就找不到了 😂
这时候，需要PV（储物柜）和PVC（储物申请）来拯救数据，对数据做持久化处理！

2️⃣ K8S 存储的优势：
👉 数据和应用分离：应用挂了，数据还在
👉 自动管理：K8s帮你处理绑定和挂载
👉 灵活调度：数据可以跟着Pod走

3️⃣ PV (PersistentVolume)：大学的储物柜系统 🗄️
如图2，PV 就像大学提供的储物柜，独立于学生存在，学生毕业了储物柜还在。
👉 独立存在
👉 集群共享
👉 有生命周期
👉 类型丰富

4️⃣ PVC (PersistentVolume Claim)：储物柜申请单 📝
如图3，就像学生填写储物柜申请表，告诉管理员需要什么样的储物柜。
👉 需求描述
👉 自动匹配
👉 抽象层：Pod不需要知道具体是本地硬盘还是网络存储，只管用就行

5️⃣ PV与PVC的相亲匹配 🔗
提交申请 → 自动匹配 → 绑定成功 → Pod使用，如图4

6️⃣ 存储访问模式
如图5，存储访问一共有三种模式，根据使用场景进行合理选择 🔑，举几个 🌰 如下：
👉 数据库用RWO，确保数据一致性
👉 静态文件用ROX，多Pod共享读取
👉 日志收集用RWX，多Pod写入
👉 定期备份重要数据，设置合理回收策略，毕竟PV、PVC也会被意外删除

7️⃣ 存储回收策略
👉 Retain：手动回收，数据保留
👉 Delete：自动删除，数据清空
👉 Recycle：清理重用（由于安全性和兼容性问题，已废弃）

对于SRE来说，存储管理是保障业务连续性的关键环节。没有持久化存储，应用重启后数据就丢失了，这在生产环境是不可接受的。K8S 提供的 PV 储物柜和 PVC 申请单的完整体系，让数据和应用分离，确保Pod重启后数据依然安全！

本地储物柜（HostPath PV）：
```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: local-storage-pv
spec:
  capacity:
    storage: 10Gi                    # 储物柜容量：10GB
  accessModes:
  - ReadWriteOnce                    # 单人专用
  persistentVolumeReclaimPolicy: Retain  # 手动回收
  storageClassName: local-storage    # 本地存储类型
  hostPath:
    path: /data/storage              # 节点本地路径
    type: DirectoryOrCreate          # 目录不存在就创建
```

网络共享储物柜（NFS PV）：
```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfs-storage-pv
spec:
  capacity:
    storage: 50Gi                    # 共享储物柜：50GB
  accessModes:
  - ReadWriteMany                    # 多人共享
  persistentVolumeReclaimPolicy: Recycle  # 自动清理重用
  storageClassName: nfs-storage      # NFS存储类型
  nfs:
    server: 192.168.1.100           # NFS服务器地址
    path: /shared/storage           # 共享目录路径
```

个人储物柜申请：
```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-personal-storage
  namespace: computer-science
spec:
  accessModes:
  - ReadWriteOnce                    # 个人专用
  storageClassName: local-storage    # 匹配本地存储PV
  resources:
    requests:
      storage: 5Gi                   # 申请5GB空间
```

共享储物柜申请：
```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: team-shared-storage
  namespace: computer-science
spec:
  accessModes:
  - ReadWriteMany                    # 团队共享
  storageClassName: nfs-storage      # 匹配NFS存储PV
  resources:
    requests:
      storage: 20Gi                  # 申请20GB共享空间
```


匹配条件： 容量匹配、访问模式匹配、存储类型匹配
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: my-app-pod
spec:
  containers:
  - name: app
    image: nginx
    volumeMounts:
    - name: data-volume
      mountPath: /usr/share/nginx/html  # 挂载到容器内
  volumes:
  - name: data-volume
    persistentVolumeClaim:
      claimName: my-personal-storage    # 使用申请的储物柜
```

RWO示例（个人储物柜）：
```yaml
# 数据库Pod专用存储
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mysql-storage
spec:
  accessModes:
  - ReadWriteOnce      # 只有一个Pod能写入
  resources:
    requests:
      storage: 10Gi
```

RWX示例（共享储物柜）：
```yaml
# 多个Pod共享的日志存储
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: shared-logs
spec:
  accessModes:
  - ReadWriteMany      # 多个Pod都能写入
  resources:
    requests:
      storage: 50Gi
```

实际操作 🔧

```bash
# 查看储物柜状态
kubectl get pv

# 查看储物申请
kubectl get pvc -n computer-science

# 查看绑定详情
kubectl describe pvc my-personal-storage -n computer-science
```

章节15:
StorageClass：大学的智能储物管理员 🤖

大家好！上一章聊了 PV(PersistentVolume) 储物柜和 PVC(PersistentVolumeClaim) 申请单，细心的小红薯可能注意到了，每次需要使用存储时，都要手动创建 PV ，这看着很麻烦，其实一点都不方便。今天来聊一个创建 PV 更好的选项： Kubernetes StorageClass(SC)，大学里的"智能储物管理员"。

1️⃣ 为什么需要SC：因为手动管理太累了🥱
👉 学生申请储物柜，管理员要手动去买、装、配置
👉 学生多了，管理员忙不过来
StorageClass解决方案：学生提交申请，系统自动采购、安装、配置！

2️⃣ 工作原理：动态供应流程：
PVC申请 → StorageClass响应 → 调用Provisioner → 自动创建PV → 自动绑定

3️⃣ 三种智能管理员：
👉 本地存储管理员：使用本地存储创建，依赖于集群的各个节点的磁盘存储。
👉 云存储管理员：使用云端存储，虽然选项很多：aws云，阿里云，腾讯云。。这带个云的存储啥都好，就是对钱包不好😂
👉 网络存储管理员：使用公司/部门的网络存储：Network File System，免费的才是真香👍
三种存储请参考图2、3和4，主要参数说明如下：
Provisioner（供应商）：决定用哪家"厂商"制造储物柜
VolumeBindingMode（绑定模式）：Immediate：立即分配储物柜；WaitForFirstConsumer：等学生来了再分配
ReclaimPolicy（回收策略）：Delete：用完自动回收；Retain：保留数据，手动清理
AllowVolumeExpansion（扩容支持）：储物柜是否可以扩大

4️⃣ 使用示例，请参考图5，在创建 PVC 时指定 SC

5️⃣ SC 使用建议：
👉 为不同业务场景创建不同StorageClass
👉 生产环境用高性能存储，开发环境用标准存储
👉 定期清理不用的动态PV，避免成本浪费
👉 敏感数据使用加密存储

StorageClass就像大学里的智能储物管理员，学生只需要提交申请，它就能自动采购、安装、配置储物柜，让存储管理从手工作坊升级为智能工厂！在 K8S 的世界，有了StorageClass，SRE再也不用半夜起来手动创建 PV 了，就它了！


三种智能管理员 🎯

本地存储管理员：
```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: local-fast-storage
provisioner: kubernetes.io/no-provisioner  # 不自动创建，需要手动准备PV
volumeBindingMode: WaitForFirstConsumer    # 等学生来了再分配
allowVolumeExpansion: false                # 不支持扩容
reclaimPolicy: Delete                      # 用完就删除
```

云存储管理员：
```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: aws-gp3-storage
provisioner: ebs.csi.aws.com              # AWS EBS供应商
parameters:
  type: gp3                                # SSD类型
  fsType: ext4                             # 文件系统
  encrypted: "true"                        # 加密存储
allowVolumeExpansion: true                 # 支持扩容
reclaimPolicy: Delete                      # 自动清理
volumeBindingMode: Immediate               # 立即分配
```

网络存储管理员：
```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: nfs-shared-storage
provisioner: nfs.csi.k8s.io               # NFS供应商
parameters:
  server: nfs-server.example.com          # Network File System服务器
  path: /shared/storage                    # 共享路径
  mountOptions: "vers=4.1,rsize=1048576"  # 挂载选项
allowVolumeExpansion: true                 # 支持扩容
reclaimPolicy: Retain                      # 保留数据
volumeBindingMode: Immediate               # 立即分配
```


使用示例
```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-dynamic-storage
  namespace: computer-science
spec:
  accessModes:
  - ReadWriteOnce
  storageClassName: aws-gp3-storage        # 指定智能管理员
  resources:
    requests:
      storage: 10Gi                        # 申请10GB
```


常用命令 📋

```bash
# 查看所有StorageClass
kubectl get storageclass

# 设置默认StorageClass
kubectl patch storageclass aws-gp3-storage -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'
```
killercode:
controlplane:~/k8s$ kubectl get sc
NAME                   PROVISIONER             RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE
local-path (default)   rancher.io/local-path   Delete          WaitForFirstConsumer   false                  13d
test-path              rancher.io/local-path   Retain          WaitForFirstConsumer   false                  4s

1. sc.yaml 
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  annotations:
    storageclass.kubernetes.io/is-default-class: "false"
  name: test-path
provisioner: rancher.io/local-path
reclaimPolicy: Retain
volumeBindingMode: WaitForFirstConsumer

2. create pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: test-pod
spec:
  containers:
  - name: test-container
    image: nginx
    volumeMounts:
    - name: test-volume
      mountPath: /usr/share/nginx/html
  volumes:
  - name: test-volume
    persistentVolumeClaim:
      claimName: local-pvc

3. create pvc
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: local-pvc
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
  storageClassName: local-path

4. pod status become from pending to running
5. kubectl delete pvc local-pvc
6. kubectl get pvc : local-pvc is still there but status is terminating
7. kubectl patch pvc local-pvc --type='merge' -p='{"metadata":{"deletionTimestamp":null}}'
persistentvolumeclaim/local-pvc patched (no change)
kubectl get pvc local-pvc -oyaml | grep -A10 finalizer

8. kubectl patch pvc local-pvc -p '{"metadata":{"finalizers":null}}'
persistentvolumeclaim/local-pvc patched
pvc is deleted -- same effect as to remove whole finalizer section in yaml

9 kubectl exec test-pod -- mkdir -p /usr/share/nginx/html/test
mkdir: cannot create directory '/usr/share/nginx/html/test': No such file or directory
command terminated with exit code 1

10 kubectl get pv : pv had been deleted

章节16:
大家好！前面我们从下往上聊了Kubernetes的共享宿舍（Pod）、楼管大妈（ReplicaSet）、总宿舍管理员（Deployment）、外卖调度中心（Service），今天来聊整个校园的核心——学校总务处（API Server）。

1️⃣ 想象一下，如果你是学生，想要申请宿舍、查询宿舍信息、投诉宿舍问题，你会去找谁？没错，就是学校总务处！在K8S的世界里，API Server就是这个"总务处"，它是整个集群的统一服务窗口。

2️⃣ 请求统一受理
无论你是想创建新的共享宿舍（Pod），还是想调整宿舍数量（修改Deployment），都必须通过总务处。它是唯一的官方渠道：
想申请宿舍？想退宿？想投诉楼管？找总务处！

3️⃣ 身份验证和权限管理
总务处可不是谁来都给办事的。首先，你得出示学生证（身份认证），然后检查你的权限：
👉 研究生可以申请研究生公寓
👉 教职工才能申请教师宿舍
如果你是个校外人员，想混进来申请宿舍？No Door!

4️⃣ 统一调度协调
总务处接到申请后，开始协调各个相关部门：
👉 通知楼管大妈（ReplicaSet）："给他安排个床位"
👉 联系外卖调度中心（Service）："新宿舍的外卖地址记得更新"
👉 协调后勤部门（Scheduler）："看看哪栋楼还有空房间"
就像一个指挥中心，把需求传达给各个执行部门。

5️⃣ 档案管理和信息存储
总务处最重要的职责就是记录存档。所有的宿舍分配信息都存在档案室（etcd）里：
👉 张三住在3号楼201室
👉 李四的宿舍申请在审批中
这些信息必须准确无误，因为其他部门都要依赖这些数据工作。

6️⃣ 标准化服务流程 
为了提高效率，总务处制定了标准的API：
👉 GET：查询信息，查看我的宿舍信息
👉 POST：创建资源，申请新宿舍
👉 PUT：更新信息，我要更换宿舍
👉 DELETE：删除资源，我要退宿
全校师生都按照统一的流程办事。

7️⃣ 高可用性
如果总务处突然关门了，整个宿舍管理系统都会瘫痪！所以，API Server通常会部署多个副本，就像总务处会安排多个值班人员一样。

记住哈，当你使用kubectl命令时，请注意你其实是在和这个"总务处"打交道！


实战演示：如何与总务处（API Server）打交道 💻

虽然我们不能"创建"总务处，但可以通过各种方式与它交互：

基础的总务处业务办理（kubectl 命令）
```bash
# 查看总务处的基本信息
kubectl cluster-info

# 查看总务处支持哪些业务（API 版本）
kubectl api-versions

# 查看总务处能管理哪些资源类型
kubectl api-resources

# 向总务处申请创建宿舍
kubectl apply -f nginx-pod.yaml

# 向总务处查询宿舍状态
kubectl get pods

# 向总务处查询详细的宿舍信息
kubectl describe pod nginx-pod

# 向总务处申请删除宿舍
kubectl delete pod nginx-pod
```

直接与总务处对话（REST API 调用）
```bash
# 获取总务处的地址
APISERVER=$(kubectl config view --minify -o jsonpath='{.clusters[0].cluster.server}')

# 获取访问令牌（学生证）
TOKEN=$(kubectl get secret $(kubectl get serviceaccount default -o jsonpath='{.secrets[0].name}') -o jsonpath='{.data.token}' | base64 --decode)

# 直接向总务处查询所有宿舍信息
curl -X GET $APISERVER/api/v1/pods \
  --header "Authorization: Bearer $TOKEN" \
  --insecure

# 向总务处提交宿舍申请
curl -X POST $APISERVER/api/v1/namespaces/default/pods \
  --header "Authorization: Bearer $TOKEN" \
  --header "Content-Type: application/json" \
  --data '{
    "apiVersion": "v1",
    "kind": "Pod",
    "metadata": {"name": "api-test-pod"},
    "spec": {
      "containers": [{
        "name": "nginx",
        "image": "nginx:latest"
      }]
    }
  }' \
  --insecure
```

编程方式与总务处交互（Python 客户端）
```python
from kubernetes import client, config

# 加载配置（相当于出示学生证）
config.load_kube_config()

# 创建API客户端（连接总务处）
v1 = client.CoreV1Api()

# 向总务处查询所有宿舍
print("向总务处查询宿舍列表:")
pods = v1.list_pod_for_all_namespaces()
for pod in pods.items:
    print(f"宿舍: {pod.metadata.name}, 状态: {pod.status.phase}")

# 向总务处申请新宿舍
pod_manifest = {
    "apiVersion": "v1",
    "kind": "Pod",
    "metadata": {"name": "python-test-pod"},
    "spec": {
        "containers": [{
            "name": "busybox",
            "image": "busybox",
            "command": ["sleep", "3600"]
        }]
    }
}

print("向总务处申请新宿舍...")
v1.create_namespaced_pod(namespace="default", body=pod_manifest)
print("宿舍申请已提交！")
```

监控总务处的工作状态
```bash
# 查看总务处的健康状态
kubectl get componentstatuses

# 查看总务处的详细信息
kubectl get endpoints kubernetes

# 查看总务处的访问日志（如果有权限）
kubectl logs -n kube-system kube-apiserver-<node-name>

# 查看总务处处理的事件
kubectl get events --sort-by=.metadata.creationTimestamp
```

总务处的权限验证演示
```bash
# 创建一个受限用户（普通学生）
kubectl create serviceaccount student-user

# 给学生分配有限权限（只能查看，不能创建）
kubectl create clusterrolebinding student-binding \
  --clusterrole=view \
  --serviceaccount=default:student-user

# 使用学生身份访问总务处
kubectl --as=system:serviceaccount:default:student-user get pods
# ✅ 可以查看宿舍

kubectl --as=system:serviceaccount:default:student-user delete pod nginx-pod
# ❌ 没有权限删除宿舍
```

总务处的负载均衡演示
```yaml
# 如果你有多个API Server（多个总务处窗口）
apiVersion: v1
kind: Endpoints
metadata:
  name: kubernetes
subsets:
- addresses:
  - ip: 192.168.1.100  # 总务处窗口1
  - ip: 192.168.1.101  # 总务处窗口2
  - ip: 192.168.1.102  # 总务处窗口3
  ports:
  - port: 6443
    protocol: TCP
```

总务处的幕后工作流程 🔄

当你执行 `kubectl apply -f pod.yaml` 时，总务处的工作流程：

1. 接收请求：总务处收到你的宿舍申请
2. 身份验证：检查你的学生证是否有效
3. 权限检查：确认你有申请这类宿舍的权限
4. 数据验证：检查申请表是否填写正确
5. 存储记录：将申请信息存入档案室（etcd）
6. 通知相关部门：告诉调度器（Scheduler）安排宿舍
7. 返回结果：告诉你申请已受理

这就是为什么 API Server 被称为 Kubernetes 的"心脏"——所有的操作都要经过它！

章节17:
Kubernetes API Server的特殊身份：静态pod
大家好！之前我们聊了API Server这个"学校总务处"，在操作视频中也看到了API Server其实是kube-system命名空间下的一个Pod。
今天来揭开它的神秘面纱——为什么API Server这么特殊？答案就在于它的"特殊身份"：静态Pod！

1️⃣ 什么是静态Pod 🤔
想象一下，在我们的K8S大学里，大部分学生住在由楼管大妈（ReplicaSet）管理的普通共享宿舍（Pod）里。但是，总务处的工作人员却住在特殊的"VIP宿舍"里——这就是静态Pod！

2️⃣ 静态Pod的特点：
👉 不需要通过总务处申请
👉 直接由设施维护员(kubelet)管理
👉 通常运行在控制平面节点(master节点)上
👉 配置文件就放在本地，随时可以查看
👉 一旦配置文件改变，立即"搬家"重建

3️⃣ 为什么要用静态Pod？🏠
这里有个"鸡生蛋，蛋生鸡"的问题：普通Pod需要通过API Server创建，但如果API Server本身也需要通过API Server来创建，那谁来创建它呢？🤯

K8s设计者想出了绝妙方案：让API Server自力更生，住进"静态Pod"这种特殊宿舍！
工作原理：
👉 设施维护员有特殊文件夹，里面放着"宿舍设计图"
👉 kubelet每隔几秒检查这个文件夹
👉 发现设计图变化，立即重建宿舍

4️⃣ 静态Pod的神奇特性
👉 配置即生效
修改配置文件 → 保存 → 几秒后Pod自动重建 → 新配置生效。就像魔法一样！
👉 本地文件管理，如图2
👉 自动恢复机制：即使API Server挂了，kubelet 仍能管理API Server这个静态Pod本身，会自动尝试重启它。
👉 无法直接删除：用kubectl删除API Server的Pod会发现几秒后又出现了！因为kubelet看到配置文件还在，会立即重建。

5️⃣ 静态Pod家族成员
K8s集群中的控制平面组件都是静态Pod，除了API Server以外，后续我会继续介绍其他组件

API Server使用静态Pod这种特殊身份，就像住在"VIP宿舍"里的总务处工作人员——不需要通过自己来申请宿舍，直接由设施维护员（kubelet）根据本地配置文件管理，实现了"自力更生"的完美闭环！


实际操作体验 🛠️

想体验静态Pod的神奇吗？试试这个：

```bash
# 1. 查看当前API Server配置
sudo cat /etc/kubernetes/manifests/kube-apiserver.yaml

# 2. 备份原文件
sudo cp /etc/kubernetes/manifests/kube-apiserver.yaml /tmp/backup.yaml

# 3. 修改一个无关紧要的标签
sudo vim /etc/kubernetes/manifests/kube-apiserver.yaml
# 在metadata.labels下加一行：test: "modified"

# 4. 保存后观察Pod重建
watch kubectl get pods -n kube-system | grep apiserver

# 5. 恢复原配置
sudo cp /tmp/backup.yaml /etc/kubernetes/manifests/kube-apiserver.yaml
```

章节18: K8S Controller Manager：校园管理中枢

大家好，之前介绍了校园总务处（API Server），我们再来聊聊与总务处密切配合的另一个重要角色——Controller Manager，它就像是校园管理中枢：协调各部门运转的核心。
1️⃣ Controller Manager 的核心职责：
声明式管理：根据学校的管理规定，自动创建、更新或删除资源
监督执行：24/7不间断工作，每隔一段时间检查检查集群状态，确保总务处的决定被正确执行
自动修复：发现问题立即纠正，确保集群始终处于健康状态

就像学校里有专门的主管定期巡查，确保各部门按照学校政策执行工作

2️⃣ ReplicaSet Controller：楼管大妈的直属领导
```yaml
# 当总务处批准了宿舍申请后
apiVersion: apps/v1
kind: Deployment
spec:
  replicas: 3  # 总务处说：需要3个宿舍
```

Controller Manager的工作：
- 监控：发现只有2个宿舍在运行
- 决策：需要再创建1个宿舍
- 执行：通知楼管大妈（ReplicaSet）安排新宿舍
- 验证：确认新宿舍创建成功

3️⃣ Deployment Controller：总宿舍管理员的监督者
```bash
# 当需要升级宿舍设施时
kubectl set image deployment/nginx-deployment nginx=nginx:1.22
```

Controller Manager的工作：
1. 监听变化：发现Deployment配置更新了
2. 制定计划：创建新的ReplicaSet（新楼管）
3. 滚动更新：逐步替换旧宿舍为新宿舍
4. 清理工作：删除旧的ReplicaSet（辞退旧楼管）

4️⃣ Node Controller：宿舍楼管理员
```bash
# 监控宿舍楼（Node）的健康状态
kubectl get nodes
NAME     STATUS   ROLES    AGE   VERSION
node-1   Ready    worker   10d   v1.28.0
node-2   NotReady worker   10d   v1.28.0  # 这栋楼出问题了！
```

Controller Manager的应急处理：
1. 发现问题：node-2宿舍楼失联了
2. 标记状态：将该楼标记为"不可用"
3. 疏散住户：将楼内的Pod迁移到其他楼
4. 等待恢复：持续监控楼的状态


#实际的Controller Manager操作：

```bash
# 查看Controller Manager的状态
kubectl get componentstatuses
NAME                 STATUS    MESSAGE             ERROR
controller-manager   Healthy   ok

# 查看Controller Manager的日志
kubectl logs -n kube-system kube-controller-manager-master

# 查看所有正在运行的Controllers
kubectl get pods -n kube-system | grep controller-manager
```

5️⃣ Controller Manager 与 API Server 的协作：
👉 管理中枢时刻关注总务处的各种变化通知
👉 发现问题时，主动制定解决方案  
👉 通过总务处执行具体的管理操作

Controller Manager就像是学校里那些默默无闻但极其重要的中层管理者，他们不直接面对学生，但确保学校的各项政策能够被正确执行。如果没有它，整个宿舍管理体系就只是一堆静态的规章制度，无法自动化、智能化地运转。它是让K8S"活起来"的关键组件！没有它，K8s就只是一堆静态配置文件！

章节19: Kubernetes Scheduler：校园后勤部门

大家好！前面聊了校园总务处（API Server）和校园管理中枢（Controller Manager），今天来认识Scheduler，学校的后勤部门，专门负责给学生安排最合适的宿舍。
1️⃣ 什么是Scheduler❓
如果你是K8S大学新生，通过入学申请（即Pod创建）后需要宿舍。学校有很多宿舍楼（Node），条件各不相同
👉 A栋配置高但住满了
👉 B栋有空房但网络差
👉 C栋安静适合学习
👉 D栋设施新但位置远。
Scheduler就是帮你选择最佳宿舍的后勤部门！它综合考虑需求、资源情况和限制条件，安排最合适的住处。
2️⃣ Scheduler的工作流程 
👉 接收住宿申请
如图3，Controller Manager创建新Pod时，Pod处于"无家可归"状态，即还未分配到具体的Worker Node。
👉 筛选合适的宿舍楼（Filtering）
Scheduler筛选候选Node：检查资源是否充足（CPU、内存）、存储空间、端口冲突、Node选择器要求等。就像后勤部门问："学生需要2GB内存，A栋还有吗？"
👉 评分排序（Scoring）
Scheduler给候选Node打分：考虑资源利用率、亲和性、数据本地性、负载均衡等因素。
👉 最终决策
选择得分最高的Node，通知API Server，即为Pod配置指定nodeName字段
3️⃣ Scheduler的智能调度策略 🧠
👉 资源感知调度：
Scheduler避免将Pod调度到资源紧张的Node上。
👉 亲和性调度：如业务Pod希望与数据库Pod在同一个Node
👉 反亲和性调度：如多个Web服务器Pod不能在同一个Node，避免单点故障
👉 污点和容忍：某些Node有"污点"（如GPU专用），只有特定Pod能"容忍"
4️⃣ 调度失败怎么办？❌
Scheduler遇到"无房可分"时，Pod会处于Pending状态。常见原因：资源不足、亲和性冲突、污点限制、端口冲突等。
5️⃣ 一句话总结
Scheduler就像是学校里最懂学生需求的后勤部门主任——它不仅要考虑宿舍楼的硬件条件，还要平衡各种复杂的人际关系和特殊要求，最终为每个学生找到最合适的"家"。没有它，Pod就只能在集群里"流浪"，永远找不到属于自己的Node！

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: student-pod
spec:
  containers:
  - name: student-container
    image: nginx
    resources:
      requests:
        memory: "64Mi"
        cpu: "250m"
  # 注意：这里还没有nodeName字段
```

实际操作命令 🛠️

```bash
# 查看调度器状态
kubectl get pods -n kube-system | grep scheduler

# 查看调度事件
kubectl get events --sort-by=.metadata.creationTimestamp
```

章节20: Kubernetes etcd：校园档案馆

大家好！今天来说说K8S大学最神秘的地方——etcd，也就是我们的"校园档案馆"！如果说API Server是总务处，那etcd就是存放所有重要文件的档案馆，没有它，整个大学就要停摆了！

1️⃣ etcd是什么？
etcd和mysql，postgres，oracle一样，是一款数据库产品，作为K8S大学的"超级档案馆" 📚，etcd存放着K8S大学的所有重要信息：学生档案（Pod信息）、宿舍分配表（Pod调度）、课程安排（Service配置）、密码本（Secret）、校规校纪（RBAC规则）等。
它的特点：
👉 高度机密：只有总务处（API Server）能直接访问
👉 绝对可靠：分布式存储，多副本保证数据安全
👉 实时同步：任何变更立即记录并同步
👉 历史追溯：可查看任何时间点的数据状态

2️⃣ etcd的特殊身份：为什么在Service中"隐身"？ 🕵️

细心的同学可能发现了，运行`kubectl get svc -n kube-system`时，根本看不到etcd！这是因为：

etcd是静态Pod，由kubelet（设施维护员）直接管理，不需要Service。API Server直接通过localhost:2379连接etcd。就像档案馆太重要，不能随便让人访问一样。
```bash
# etcd直接监听在控制节点上
netstat -tlnp | grep 2379
tcp  0  0  127.0.0.1:2379  0.0.0.0:*  LISTEN  1234/etcd

# API Server直接连接etcd，不通过Service
cat /etc/kubernetes/manifests/kube-apiserver.yaml | grep etcd
--etcd-servers=https://127.0.0.1:2379
```

就像校园档案馆一样：
只有总务处（API Server）有钥匙
不对外开放：学生不能直接进入档案馆
内部直通：总务处有专门通道到档案馆

3️⃣ etcd的端口配置：档案馆的"两扇门" 🚪

etcd有两个重要端口：
2379端口：API Server通过这个端口读写数据（总务处专用通道）
2380端口：多个etcd实例之间同步数据（分馆间专用通道）

4️⃣ etcd的数据存储：分布式的"保险柜" 🔐

etcd采用Raft算法实现分布式一致性：选出一个"馆长"处理写操作，馆长把变更同步给其他分馆，只有大多数分馆确认后数据才算保存。

实际场景：
```bash
# 查看etcd集群状态
ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  endpoint health

# 输出：127.0.0.1:2379 is healthy: successfully committed proposal
```

5️⃣ etcd的性能特点：档案馆的"超能力" ⚡

读操作：闪电般快速，大部分可从本地副本完成，无需集群共识
写操作：安全第一，必须通过Leader，需要大多数节点确认

性能优化：使用SSD存储，降低网络延迟，通常3-5个节点的奇数集群最佳

6️⃣ etcd的备份与恢复：档案馆的"防灾演练" 🛡️

etcd存储了集群的所有状态，一旦丢失就像档案馆失火！备份策略：
```bash
# 创建etcd快照
ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  snapshot save /backup/etcd-snapshot-$(date +%Y%m%d).db
```

恢复流程：停止API Server → 恢复etcd数据 → 重启集群

7️⃣ etcd监控：档案馆的"安保系统" 👮‍♂️

关键监控指标：磁盘延迟、网络延迟、Leader变更频率、数据库大小

告警场景：
```bash
# 磁盘延迟过高
etcd_disk_wal_fsync_duration_seconds > 0.1

# Leader频繁变更
rate(etcd_server_leader_changes_seen_total[1h]) > 3

# 数据库过大
etcd_mvcc_db_total_size_in_bytes > 8GB
```

一句话总结 🎯

etcd就像K8S大学的超级档案馆——它默默地存储着集群的所有秘密，虽然你在Service中看不到它，但它却是整个大学正常运转的基石。没有etcd，就没有K8S；保护好etcd，就是保护好整个集群的灵魂！

*记住：在K8s的世界里，etcd不仅是数据存储，更是集群状态的唯一真相来源！* 📚✨

章节21: Cloud Controller Manager：校园云服务部门 ☁️

大家好！之前我们认识了K8S大学的各个机构，今天我们把目光从校内转向校外，来认识Cloud Controller Manager——学校的宣传处，专门负责与外部云平台对接的"外交官"。

1️⃣ 什么是Cloud Controller Manager❓

想象K8S大学决定"上云"——把服务外包给云服务商：宿舍楼租用阿里云虚拟机，网络使用云厂商负载均衡器，存储放在云盘中。

Cloud Controller Manager就是学校的宣传处，即"云服务部门"，负责与云厂商打交道，把校园需求转化为云服务配置。

2️⃣ 为什么需要它？🤔

本地集群只能提供基础设施，云平台作为扩展提供了更多能力：负载均衡器、云存储、网络安全等。

传统Controller Manager不懂云平台API。当学生申请LoadBalancer服务时：
👉 本地集群：Service永远显示`<pending>`
👉 云平台：有SLB，但不知道K8s要什么配置

Cloud Controller Manager就像宣传处主任，把校园需求翻译成云平台"方言"，让K8s享受云服务的便利。

3️⃣ 核心控制器 🎯

Node Controller（宿舍楼管理员）：
监控云主机状态，当云主机挂掉时，标记Node为NotReady，触发Pod重新调度。

Route Controller（网络管理员）：
管理云平台路由表，确保不同Node间Pod互通：
```bash
10.244.1.0/24 -> cloud-node-1
10.244.2.0/24 -> cloud-node-2
```

Service Controller（对外服务管理员）：
最重要的控制器，负责创建云负载均衡器：
```yaml
apiVersion: v1
kind: Service
metadata:
  name: my-app
spec:
  type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: my-app
```
自动调用云平台API创建负载均衡器，配置后端服务器组，返回外部IP。

4️⃣ 多云适配 🌍

每个云厂商API不同，Cloud Controller Manager需要适配：
- 阿里云：创建SLB负载均衡器
- 腾讯云：创建CLB负载均衡器
- AWS：创建ELB负载均衡器

就像多语言专家，流利地与各种云平台"对话"。

5️⃣ 架构对比 🏗️

传统架构：Controller Manager包含所有控制器
云原生架构：分离出Cloud Controller Manager专门处理云平台对接

```
Controller Manager     Cloud Controller Manager
├─ Deployment Ctrl     ├─ Node Controller
├─ ReplicaSet Ctrl     ├─ Service Controller
└─ Job Controller      └─ Route Controller
```

6️⃣ 实际场景：发布网站 🚀

学生社团申请LoadBalancer服务：
```yaml
apiVersion: v1
kind: Service
metadata:
  name: club-website
spec:
  type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: club-website
```

本地集群限制：无法获取公网IP，因为家庭/企业网络只有1个路由器公网IP，无负载均衡器资源。

云平台优势：Cloud Controller Manager工作流程：
👉 检测到LoadBalancer服务 → 调用云平台API创建负载均衡器 → 分配独立公网IP：47.96.123.45

结果：
```bash
kubectl get service club-website
NAME           TYPE           EXTERNAL-IP    PORT(S)
club-website   LoadBalancer   47.96.123.45   80:30080/TCP
```

7️⃣ 故障处理 ⚠️

云主机宕机：Node Controller自动标记NotReady，Pod重新调度，流量切换，用户无感知。

负载均衡器创建失败：Service显示`<pending>`状态，查看日志排查配额、权限等问题。

8️⃣ 云平台使用限制 💰

计时收费模式：
👉 负载均衡器：按小时计费，即使没有流量也收费
👉 公网IP：闲置IP也要付费，删除Service要及时释放
👉 流量费用：公网出流量按GB计费

成本控制建议：
- 开发环境使用NodePort，生产环境才用LoadBalancer
- 及时清理不用的Service，避免资源浪费
- 监控云账单，设置费用告警

权限和配额限制：
- Cloud Controller Manager需要云平台API权限
- 负载均衡器数量受配额限制
- 不同地域资源可用性不同

一句话总结 🎯

Cloud Controller Manager就像是K8S大学的宣传处主任——它精通各种云平台的"语言"，能够把校园内的需求无缝转化为云服务的配置。有了它，K8s集群才能真正享受到云计算的弹性和便利，但也要注意控制成本！

*记住：在云原生时代，Cloud Controller Manager不仅是K8s与云平台的桥梁，更是让应用真正"飞上云端"的关键推手！* ☁️✨

章节22: 
----- Chinese
kubelet基础篇：K8S大学勤劳的设施维护员 🔧
大家好！前面我们认识了K8S大学的各个管理部门，今天来认识一个默默无闻但极其重要的角色：kubelet，作为学校里那些勤劳的设施维护员，它们负责每栋宿舍楼的日常运营和维护。

1️⃣ 什么是kubelet❓
想象一下，K8S大学有很多栋宿舍楼（Worker Node），每栋楼都需要一个专职的设施维护员来管理：
👉 检查宿舍状态：哪些房间有人住，哪些空着
👉 维护基础设施：水电网络、空调暖气是否正常
👉 执行管理指令：总务处下达的各种任务
👉 上报楼栋情况：定期向总务处汇报楼栋状态
kubelet就是这样的设施维护员，运行在每个Worker Node上，负责该Node的Pod生命周期管理。

2️⃣ kubelet的核心职责和工作流程 🎯
Pod生命周期管理（宿舍房间管理）
在收到 API Server 总务处创建pod的指令以后：
```yaml
# 总务处下发住宿安排
apiVersion: v1
kind: Pod
metadata:
  name: student-k8s
spec:
  containers:
  - name: student
    image: college-student:v1.0
    resources:
      requests:
        memory: "1Gi"
        cpu: "0.5"
```
1. 接收任务
- 从API Server获取Pod规格
- 检查本地是否已有该Pod
- 对比期望状态和实际状态

2. 执行任务（如果需要创建新Pod）
- 拉取容器镜像 (准备生活用品)
- 创建Pod沙箱 (分配房间)
- 启动Init容器 (房间预处理)
- 启动主容器 (学生入住)
- 配置网络和存储 (接通水电网)

3. 持续监控（"巡楼"工作）
- 每10秒检查Pod健康状态
- 每20秒上报Node状态给API Server
- 实时监控资源使用情况
- 处理Pod的重启和故障恢复

节点状态上报（楼栋情况汇报）：kubelet定期向总务处汇报节点的实时状态

容器运行时管理（设施设备管理）：kubelet不直接管理容器，而是通过CRI（Container Runtime Interface）与Docker、containerd等容器运行时交互，就像维护员不直接修电器，而是找专业的电工师傅。
工作分工：
- kubelet：负责Pod级别的管理和调度
- 容器运行时：负责具体的容器创建和管理
- 就像：维护员负责整体规划，电工师傅负责具体施工

kubelet就像K8S大学里最勤劳的设施维护员——虽然不在管理层，但承担着最核心的执行工作。它们默默地管理着每个Node上的Pod，确保学生们（容器）能够正常"入住"和"生活"。理解kubelet的基本职责和工作流程，是掌握K8s运行机制的关键！

----- English

Chapter 22: kubelet Basics: The Diligent Facility Managers of K8S University 🔧

Hello everyone! We've previously met the various management departments of K8S University. Today, let's get to know an unsung but extremely important role: kubelet, the diligent facility managers responsible for the daily operations and maintenance of each dormitory building on campus.

1️⃣ What is kubelet? ❓

Imagine K8S University has many dormitory buildings (Worker Nodes), and each building needs a dedicated facility manager to handle:
👉 Check room status: Which rooms are occupied and which are vacant
👉 Maintain infrastructure: Ensure water, electricity, network, AC, and heating are working properly
👉 Execute management directives: Handle various tasks assigned by the General Affairs Office
👉 Report building status: Regularly report building conditions to the General Affairs Office

kubelet is exactly this kind of facility manager, running on each Worker Node and responsible for Pod lifecycle management on that Node.

2️⃣ kubelet's Core Responsibilities and Workflow 🎯

Pod Lifecycle Management (Dormitory Room Management)

When kubelet receives instructions from the API Server (General Affairs Office) to create a pod:

```yaml
# Housing assignment from General Affairs Office
apiVersion: v1
kind: Pod
metadata:
  name: student-k8s
spec:
  containers:
  - name: student
    image: college-student:v1.0
    resources:
      requests:
        memory: "1Gi"
        cpu: "0.5"
```

1. Receive Tasks
- Get Pod specifications from API Server
- Check if the Pod already exists locally
- Compare desired state with actual state

2. Execute Tasks (when creating a new Pod)
- Pull container images (prepare living supplies)
- Create Pod sandbox (allocate room)
- Start Init containers (room preprocessing)
- Start main containers (student move-in)
- Configure network and storage (connect utilities)

3. Continuous Monitoring ("Building Patrol" Work)
- Check Pod health status every 10 seconds
- Report Node status to API Server every 20 seconds
- Monitor resource usage in real-time
- Handle Pod restarts and failure recovery

Node Status Reporting (Building Status Reports): kubelet regularly reports the real-time status of nodes to the General Affairs Office

Container Runtime Management (Equipment Management): kubelet doesn't directly manage containers but interacts with container runtimes like Docker and containerd through CRI (Container Runtime Interface), just like how facility managers don't directly fix electrical equipment but call professional electricians.

Division of Labor:
- kubelet: Responsible for Pod-level management and coordination
- Container runtime: Responsible for specific container creation and management
- Analogy: Facility manager handles overall planning; electrician handles specific implementation

kubelet is like the most diligent facility manager at K8S University - while not part of the management hierarchy, it carries out the most critical execution work. It silently manages Pods on each Node, ensuring students (containers) can properly "move in" and "live." Understanding kubelet's basic responsibilities and workflow is key to mastering K8s operational mechanisms!

*Remember: In the K8s world, kubelet is the crucial bridge connecting the control plane with actual workloads - it's the true "executor"!* 🔧✨


章节23: kubelet配置与管理篇：维护员的工作手册 📋

大家好！上一章我们了解了kubelet这位勤劳的设施维护员的基本职责，今天来深入了解维护员是如何"上岗"的——从系统服务配置到工作参数设置，这些都是让kubelet正常工作的关键！
1️⃣ kubelet的系统服务身份 🔧
kubelet的双重身份：
虽然kubelet管理着容器化的Pod，但它自己却是"原住民"——直接运行在宿主机上的系统服务：

```bash
# kubelet作为systemd服务运行
systemctl status kubelet
# ● kubelet.service - kubelet: The Kubernetes Node Agent
#    Loaded: loaded (/lib/systemd/system/kubelet.service; enabled)
#    Active: active (running) since Mon 2024-01-15 09:00:00 UTC; 10 days ago

# 查看服务配置文件
cat /lib/systemd/system/kubelet.service
# [Unit]
# Description=kubelet: The Kubernetes Node Agent
# After=docker.service
#
# [Service]
# ExecStart=/usr/bin/kubelet \
#   --config=/var/lib/kubelet/config.yaml \
#   --kubeconfig=/etc/kubernetes/kubelet.conf
# Restart=always
```

形象比喻：
- kubelet程序 = 维护员这个人（提前招聘好的员工）
- systemd服务 = 维护员的工作岗位（确保他按时上班）
- 配置文件 = 维护员的工作手册（告诉他具体怎么干活）

2️⃣ kubelet的配置文件体系：维护员的"工具箱" 📁
kubelet有一套完整的配置文件"家族"：
主配置文件：记录着所有的工作规范和标准。这个文件告诉kubelet该怎么工作，比如监听哪个端口，健康检查怎么做。
认证配置文件：用来证明身份的"工作证件"，确保能和总务处（API Server）正常沟通。没有这个证件，它哪儿都去不了。
Static Pod目录：存放着"特殊住户"的档案，维护员不需要通过总务处分配可以直接安排他们入住。
工作目录和日志目录：记录着日常的工作情况和各种事件。

3️⃣ 资源管理配置：维护员的"精打细算"艺术 💰
kubelet必须学会"精打细算"，就像管理宿舍楼一样，不能把所有资源都分给Pod：
系统资源预留：为操作系统的基础进程预留CPU和内存。不然系统进程"饿死"了，整栋楼都要停摆。
K8s组件资源预留：kubelet自己也要"吃饭"，还有其他K8s组件也需要资源。
驱逐策略：当资源紧张时，kubelet就像严格的宿管阿姨：
- 硬驱逐：情况紧急时，立即让学生搬走，就像火灾时的紧急疏散
- 软驱逐：给学生时间收拾行李，优雅地搬离，就像期末宿舍调整

4️⃣ Pod管理配置：宿舍管理的"黄金法则" 🏠
作为宿舍楼的管理员，kubelet需要制定一些基本的管理规则：
住户数量限制：每栋宿舍楼最多110个Pod，每个CPU核心最多管理10个Pod。就像宿舍楼有消防安全限制一样。
进程数量控制：每个住户（Pod）最多运行4096个进程。就像宿舍里不能养太多宠物一样。
巡楼频率：维护员每分钟巡楼一次，每10秒更新节点状态，每5分钟向总务处汇报。
日志管理：每个容器日志文件最大10MB，最多保留5个，避免占用过多存储空间。

从基本的通信方式到资源管理策略，从安全认证到日志记录，合理的配置是kubelet高效工作的基础，也是整个K8s集群稳定运行的保障！

章节24: kubelet运维实战篇：维护员的进阶技能 🛠️

大家好！前面我们了解了kubelet的基本职责和配置方法，今天来学习维护员的"进阶技能"——Static Pod管理、故障排查、监控告警等实战技巧。这些技能是成为K8s运维专家的必备武器！

1️⃣ Static Pod：特殊住户管理 👑

什么是Static Pod？
Static Pod是kubelet直接管理的特殊Pod，不经过API Server调度，就像宿舍楼里的"常住管理员"：

```yaml
# Static Pod示例 - 放在 /etc/kubernetes/manifests/ 目录
# /etc/kubernetes/manifests/my-static-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: my-static-pod
  namespace: kube-system
  labels:
    component: my-component
spec:
  containers:
  - name: web-server
    image: nginx:1.20
    ports:
    - containerPort: 80
    resources:
      requests:
        memory: "64Mi"
        cpu: "50m"
      limits:
        memory: "128Mi"
        cpu: "100m"
```

Static Pod的特殊权限：
👉 直接管理：kubelet直接监控，不经过Scheduler
👉 自动重启：Pod挂了kubelet会立即重启，比普通住户更"特权"
👉 删除保护：无法通过kubectl删除，只能删除manifest文件
👉 节点绑定：永远运行在特定节点上，不会被调度到其他地方

常见应用场景：
```bash
# 控制平面组件（Master节点）
ls /etc/kubernetes/manifests/
# etcd.yaml                    # 数据库服务
# kube-apiserver.yaml         # API服务器
# kube-controller-manager.yaml # 控制器管理器
# kube-scheduler.yaml         # 调度器

# 系统级服务（Worker节点）
# kube-proxy.yaml             # 网络代理
# monitoring-agent.yaml       # 监控代理
```

重要提醒：kubelet本身不是Static Pod！它是运行在宿主机上的系统服务，就像宿舍楼的"原住民管理员"。

2️⃣ 故障排查实战：维护员的诊断技能 🔍

常见问题场景与排查：

问题1：Pod一直处于Pending状态
```bash
# 查看Pod详细信息
kubectl describe pod problematic-pod
# Events:
# Warning  FailedScheduling  pod has unbound immediate PersistentVolumeClaims

# 排查步骤
1. 检查存储卷状态：kubectl get pv,pvc
2. 确认Node资源充足：kubectl describe node
3. 查看调度器日志：kubectl logs -n kube-system kube-scheduler-xxx
4. 检查污点和容忍：kubectl get nodes -o yaml | grep -A5 -B5 taint
```

问题2：Pod频繁重启（CrashLoopBackOff）
```bash
# 查看Pod状态
kubectl get pods
# NAME        READY   STATUS             RESTARTS   AGE
# my-app      0/1     CrashLoopBackOff   5          10m

# 排查步骤
1. 查看Pod日志：kubectl logs my-app --previous
2. 查看Pod事件：kubectl describe pod my-app
3. 检查健康检查配置：kubectl get pod my-app -o yaml | grep -A10 livenessProbe
4. 进入容器调试：kubectl exec -it my-app -- /bin/sh
```

问题3：Node状态NotReady
```bash
# 查看Node状态
kubectl get nodes
# NAME           STATUS     ROLES    AGE
# worker-node-1  NotReady   worker   10d

# 排查步骤
1. 检查kubelet服务：systemctl status kubelet
2. 查看kubelet日志：journalctl -u kubelet -f
3. 检查网络连通性：ping <api-server-ip>
4. 查看系统资源：df -h && free -h
5. 检查容器运行时：systemctl status containerd
```

3️⃣ 监控指标：维护员的仪表盘 📊

核心监控指标：
```bash
# Pod相关指标
kubelet_running_pods{node="worker-1"}           # 运行中的Pod数量
kubelet_pod_start_duration_seconds              # Pod启动耗时
kubelet_pod_worker_duration_seconds             # Pod工作耗时

# 资源相关指标
kubelet_node_cpu_usage_seconds_total            # CPU使用情况
kubelet_node_memory_working_set_bytes           # 内存使用情况
kubelet_volume_stats_used_bytes                 # 存储卷使用情况

# 容器运行时指标
kubelet_runtime_operations_total                # 容器操作次数
kubelet_runtime_operations_duration_seconds     # 容器操作耗时
kubelet_runtime_operations_errors_total         # 容器操作错误次数

# 网络相关指标
kubelet_cgroup_manager_duration_seconds         # cgroup管理耗时
kubelet_pleg_relist_duration_seconds           # PLEG重新列举耗时
```

告警规则配置：
```yaml
# kubelet关键告警规则
groups:
- name: kubelet-alerts
  rules:
  # kubelet服务异常
  - alert: KubeletDown
    expr: up{job="kubelet"} == 0
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "Kubelet is down on node {{$labels.node}}"

  # Pod启动缓慢
  - alert: PodStartupSlow
    expr: kubelet_pod_start_duration_seconds > 60
    for: 2m
    labels:
      severity: warning
    annotations:
      summary: "Pod startup is taking too long on {{$labels.node}}"

  # 节点资源不足
  - alert: NodeResourceExhaustion
    expr: kubelet_node_memory_working_set_bytes / kubelet_node_memory_capacity_bytes > 0.9
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Node {{$labels.node}} memory usage is above 90%"
```

4️⃣ 性能优化：让维护员更高效 ⚡

资源优化配置：
```yaml
# 性能优化配置
maxPods: 110                    # 根据节点规格调整
podsPerCore: 10                 # 避免单核负载过重
kubeAPIQPS: 50                  # 根据集群规模调整
kubeAPIBurst: 100               # 处理突发请求

# 并发控制
maxParallelImagePulls: 5        # 并发拉取镜像数
serializeImagePulls: false      # 允许并发拉取

# 垃圾回收优化
imageGCHighThresholdPercent: 85 # 镜像GC高阈值
imageGCLowThresholdPercent: 80  # 镜像GC低阈值
imageMinimumGCAge: 2m          # 镜像最小GC年龄
```

系统调优：
```bash
# 系统级优化
# 增加文件描述符限制
echo "* soft nofile 65536" >> /etc/security/limits.conf
echo "* hard nofile 65536" >> /etc/security/limits.conf

# 优化内核参数
echo "net.core.somaxconn = 32768" >> /etc/sysctl.conf
echo "net.ipv4.ip_local_port_range = 1024 65000" >> /etc/sysctl.conf
sysctl -p
```

5️⃣ 日志管理与调试：维护员的记录本 📝

日志配置优化：
```bash
# kubelet日志配置
--log-file-max-size=100         # 单个日志文件最大100MB
--log-max-files=5               # 最多保留5个日志文件
--v=2                           # 日志级别（0-10，2为推荐值）
--log-flush-frequency=5s        # 日志刷新频率
```

常用调试命令：
```bash
# 查看kubelet状态
systemctl status kubelet
systemctl is-active kubelet

# 查看kubelet日志
journalctl -u kubelet -f                    # 实时日志
journalctl -u kubelet --since "1 hour ago" # 最近1小时日志
journalctl -u kubelet -p err               # 只看错误日志

# 检查kubelet配置
kubelet --help | grep config
kubelet --print-join-defaults

# 查看Node详细信息
kubectl describe node <node-name>
kubectl get node <node-name> -o yaml

# 检查系统资源
df -h                           # 磁盘使用情况
free -h                         # 内存使用情况
top -p $(pgrep kubelet)         # kubelet进程资源使用
```

6️⃣ 最佳实践：维护员的经验总结 💡

安全最佳实践：
```yaml
# 安全配置建议
readOnlyPort: 0                 # 禁用只读端口
authentication:
  webhook:
    enabled: true               # 启用认证
authorization:
  mode: Webhook                 # 启用授权
protectKernelDefaults: true     # 保护内核默认值
```

运维最佳实践：
```bash
# 定期维护任务
1. 监控磁盘空间使用情况
2. 定期清理无用镜像：docker system prune
3. 检查kubelet日志大小
4. 验证Static Pod配置
5. 测试节点网络连通性

# 应急预案
1. 准备kubelet配置备份
2. 建立节点快速替换流程
3. 制定Pod迁移策略
4. 准备常用调试工具
```

容量规划建议：
```yaml
# 资源预留建议（根据节点规格调整）
# 小型节点（2核4GB）
systemReserved:
  cpu: "100m"
  memory: "500Mi"

# 中型节点（4核8GB）
systemReserved:
  cpu: "200m"
  memory: "1Gi"

# 大型节点（8核16GB）
systemReserved:
  cpu: "300m"
  memory: "2Gi"
```

一句话总结 🎯

掌握kubelet的运维实战技能，就像培养一个经验丰富的资深维护员——不仅要会基本操作，还要能处理各种突发情况，优化性能，预防问题。Static Pod管理、故障排查、监控告警、性能优化这些技能，是成为K8s运维专家的核心竞争力！

*记住：kubelet的稳定运行是整个K8s集群健康的基石，掌握这些实战技能让你在运维路上游刃有余！* 🛠️✨

