章节1:
大家好，今天来聊一聊在Kubernetes集团中，Pod的直属领导们：ReplicaSet 和 Deployment。
Pod：共享宿舍
在 K8S 的世界里，Pod 是最小的调度单元，类似一间共享宿舍，里面住着一群舍友（容器）。
Pod 的特色：
共享资源：Pod 里的舍友共享插座、WiFi、饮水机等资源，就像容器共享网络和存储。
生死与共：Pod 是一个整体，如果宿舍房间出问题了（Pod 挂了），所有舍友都会一起搬家（全部容器被清理）。
短期租赁：Pod 的寿命通常很短，今天住的是一群考研党，明天可能就换成了游戏开黑团队。

ReplicaSet：宿舍楼管大妈

K8S 中的宿舍管理器，负责确保宿舍房间（Pod）的数量始终符合要求。

宿管职责：
确保宿舍满员：楼管每天统计宿舍的人数。如果某个宿舍的舍友“突然退学”（Pod 掉了），楼管会立刻安排新舍友入住，确保宿舍不会空着。
扩招/缩招：学校突然扩招了？楼管会迅速联系后勤，安排新宿舍上线；如果学校缩招了，楼管也会负责清退多余的宿舍。
服从上级：虽然楼管负责执行具体管理，但她并不决定宿舍楼的总房间数，她的所有工作听命于总宿舍管理员（Deployment）。

Deployment——总宿舍管理员
K8S 中用来管理无状态应用的控制器，它负责整个宿舍楼的规划和管理，楼管（ReplicaSet）只是她的“执行工具人”。

管理员职责：
分配宿舍数量
总管理员会告诉楼管：“我们需要 5 间宿舍，每间住 4 个人。” 楼管就根据这个要求安排宿舍数量，并持续保证每间宿舍的人数满员。
宿舍升级改造
发现宿舍设施老旧（容器版本过时了）？总管理员会安排宿舍升级，比如换新床、装空调。这种升级通常是滚动进行，一个宿舍一个宿舍地改造，避免所有宿舍同时停用导致全楼瘫痪。
快速回滚
如果新宿舍的设施有问题，比如空调漏水，总管理员会迅速撤销升级计划，把宿舍恢复到之前的样子（旧版本）。
新增策略支持
总管理员还可以制定各种“高级策略”，比如：
蓝绿部署：新宿舍和旧宿舍同时存在一段时间，看看哪个更受舍友喜爱。
金丝雀部署：先试点升级一两个宿舍，确保没问题再推广到全楼。

章节2:
大家好，昨天介绍了Kubernetes中Pod的直属领导们：ReplicaSet 和 Deployment，今天来说说为什么需要它们。
pod是易失的：作为K8S中的最小部署单元，它的存在是短暂的，可能因为节点故障、更新操作等原因被销毁或重启。
难以保证应用的高可用性：手动管理多个 Pod 时，很难实时监控并确保应用始终有足够的实例在线。
复杂的版本管理需求：在实际生产环境中，应用需要不断更新（如升级到新版本的容器镜像、修改配置），Pod 无法应对

引入 ReplicaSet 的原因：

ReplicaSet 的核心目标是确保指定数量的 Pod 持续运行，自动补充缺失的 Pod，实现高可用性。

自动扩缩容：

通过设置副本数（replicas），ReplicaSet 可以自动创建或删除 Pod，确保集群中运行的 Pod 数量符合需求。
自愈能力（自动修复）：当 Pod 因节点故障或其他问题挂掉时，ReplicaSet 会自动检测并重新创建新的 Pod。


虽然 ReplicaSet 能确保 Pod 的数量，但它无法管理 Pod 的版本更新

引入 Deployment 的原因：

Deployment 是在 ReplicaSet 的基础上增加了一层抽象，专注于声明式管理应用的生命周期，解决了复杂场景下的版本管理和扩缩容需求。

声明式管理：
管理员只需要声明期望的目标状态（如 Pod 的副本数、版本等），Deployment 会自动创建或更新底层的 ReplicaSet，使集群达到目标状态。
滚动更新：
Deployment 支持逐步替换 Pod 的版本，在不中断服务的情况下完成升级。例如，逐个替换旧版本的 Pod，直到所有 Pod 都更新为新版本。
快速回滚：
如果新版本的应用出现问题，可以快速回滚到之前的版本，避免服务长时间不可用。它记录历史版本，支持一键恢复。
扩展发布策略：
支持复杂的发布策略，比如：蓝绿部署/金丝雀发布

与 ReplicaSet 的协作：
Deployment 内部会自动创建和管理 ReplicaSet，确保底层 Pod 的数量和状态。
每次 Deployment 更新时，会创建一个新的 ReplicaSet 来管理新版本的 Pod。

一句话总结

ReplicaSet：解决了 Pod 的自动扩缩容和自愈问题，让应用始终保持高可用。
Deployment：在 ReplicaSet 之上，提供了声明式管理、滚动更新和回滚能力，满足了复杂应用场景下的运维需求。

通过引入这两者，Kubernetes 构建了一个高效、可靠、自动化的应用管理系统，让开发和运维人员的工作更加轻松！

章节3:
大家好！今天我们来聊聊 Kubernetes 中的 Service 和 Endpoint。在有了 Pod 的自动管理系统（Deployment 和 ReplicaSet）之后，我们还需要解决一个重要问题：如何高效地访问 Pod？

这时，就需要 Service 出场了！

Service：外卖配送调度中心
由于共享宿舍（Pod）的“宿舍房间”随时可能发生变化（如扩容、缩容、或者翻修换房间），外卖员（客户端或外部流量）不可能直接找到这些动态变化的房间地址。
这时候，Service 就像一个“外卖配送调度中心”，它负责：
固定入口地址：
Service 给整个宿舍分配了一个固定的“调度中心地址”（IP 或域名）。外卖员只需要记住这个固定地址，无需关心宿舍房间（Pod）具体地址的变化。
智能分单：
调度中心会把外卖订单分配给正在营业的房间（Pod），确保任务被合理分发。


Endpoint：调度中心的动态地址簿
Endpoint 是 Service 的“地址簿”，它会自动监控 Pod 的变化，动态更新活跃房间的名单：
动态更新：
如果某个 Pod（房间）挂了，Endpoint 会立刻将其从地址簿中移除；如果新增了 Pod，Endpoint 会自动添加它到名单中。
分单逻辑：
每次接到外卖订单时，调度中心（Service）都会翻看最新的地址簿（Endpoint），将订单分配给还在“营业”的房间。挂掉的房间不会再接到任务。


如何创建 Service？
如下图（示例图 2），我们可以通过 YAML 文件定义一个 Service 对象，并将其与一组 Pod 关联起来。

注意事项：
示例中的 Pod 使用的是 busybox 镜像，它默认不会监听 80 端口。因此，YAML 中定义的 port 和 targetPort 只是演示用，实际上这个 Pod 没有任何端口可以被 Service 访问。

当执行以下命令后，Kubernetes 会自动创建相应的 Endpoint：
kubectl apply -f <SERVICE>.yaml

章节4:
大家好，之前我用形象的类比介绍了 pod 这个共享宿舍和它的直属领导，今天来介绍 deployment 的伙伴们。

StatefulSet：辅导员专属宿舍，房间永远不变
如果把K8S的 worker 节点想象一栋宿舍楼，这栋楼一定会有几件“辅导员宿舍”，每个房间门口都挂着专属门牌号：101、102、103。房间号一旦分配，永远不变，学生换了也不影响房间的“身份”，送走了毕业生，辅导员还等着新人来报道呢。
StatefulSet 就是给那些需要“专属房间号”的应用准备的，比如数据库、分布式存储等。虽然辅导员宿舍（Pod）可以换，但宿舍（Pod 名字和存储）永远不变，数据和身份都稳稳当当！

DaemonSet：每栋楼的配电间
你以为每栋宿舍楼只有房间和楼管？错！还有一个神秘存在——配电间。DaemonSet 就像是每栋楼的配电间，无论新盖几栋楼，每栋都必须有一个，负责全楼供电、网络、安防等基础设施。
只要新楼一建好，唯一的配电间（DaemonSet Pod）就会自动安排到位，绝不缺席。
比如日志收集、监控探针、杀毒软件这些“全楼服务”，都靠 DaemonSet 的“配电间”默默守护。它要是罢工了，那整栋楼就不能正常运转了，要么缺了监控，要么漏了日志，总之必须有它，没有之一。


Cronjob：定时来打扫卫生的清洁阿姨
还有一位默默奉献的“清洁阿姨”。她每天（或每周、每月）定点来宿舍楼打扫卫生、收拾垃圾、消毒杀菌。
清洁阿姨不会一直待在楼里，只有到点才会出现，任务完成后就离开，等下次再来。
有时候是早上7点来扫地，有时候是晚上来收垃圾，时间表全靠提前安排好。
有了她，宿舍楼（集群）才能一直干净整洁、井井有条！

Job：水电工
作为 K8S 里的支持人员，他们专门负责定时执行一次性任务，比如备份、清理、导入/到处数据等。
水电工来了就干活，干完就走，不会常驻。


好，到现在为止，我介绍了kubernetes里创建 pod 的主要控制器，来看看它们是如何创建 pod 的。如图2，3，4

K8S 的这些‘宿舍角色’，让集群管理像生活一样井井有条、充满烟火气。

章节5:
再探Kubernetes Pod访问：一招鲜吃遍天
大家好，之前我们聊过了在使用deployment 控制器生成 pod 的场合，如何高效访问 Pod，今天我来补充一下其他控制器访问 pod 的方式，来看看有啥新鲜的玩意。

核心真相：Service是万能钥匙
99%的情况： 不管什么控制器创建的Pod，都用Service访问！
Deployment的Pod → Service访问 ✅
DaemonSet的Pod → Service访问 ✅
Job的Pod → Service访问 ✅
ReplicaSet的Pod → Service访问 ✅

唯一例外：StatefulSet - 这个特立独行的家伙！
为什么特殊？ 因为它的Pod有身份证（固定hostname），毕竟作为“辅导员”的 statefulset 房间标识还是要特殊一些，必须要好找。访问方式如图2，需要创建 headless service：
无虚拟IP - K8s不会分配ClusterIP
直接解析 - DNS直接返回Pod IP列表
访问特色：
实名访问：pod-0.service-name.namespace.svc.cluster.local
点对点：想找mysql-0就找mysql-0，想找mysql-1就找mysql-1
有状态专用：数据库、消息队列的最爱

章节6:
大家好！前面我们搞定了K8s内部的"宿舍管理"，现在该让外面的人进来参观、使用了。

1️⃣ 核心问题：怎么访问K8S的服务 ？
内部访问： 住在宿舍里的同学互相串门 - ClusterIP 就够了
外部访问： 校外的朋友要来玩 - 需要特殊的"通行证"！
Service的外部访问只有两条路：NodePort 和 LoadBalancer 😂

2️⃣ NodePort：每栋楼开个小门
工作原理： 在每个节点上开个固定端口，像每栋宿舍楼都开个侧门，门牌号30000-32767
访问方式： 任意节点IP + 端口号，比如 192.168.1.100:30080

✅ 免费使用 - 不花钱，学生的最爱
✅ 多个入口 - 每个节点都有门，坏了一个还有其他的
✅ 简单粗暴 - 配置简单，立马能用

❌ 端口奇葩 - 谁记得住30080是啥服务？像古代暗号
❌ 用户困惑 - 普通用户看到端口号就懵了

3️⃣ LoadBalancer：请个专业门卫
工作原理： 云厂商提供专业的负载均衡器，像请了个保安来指路
访问方式： 直接用外部IP或域名，比如 http://my-app.com

云部署集群（有钱人的选择）：
✅ 专业服务 - 云厂商的负载均衡器，高大上
✅ 真正的外部IP - 不用记奇葩端口号，用户体验满分
✅ 智能分发 - 自动负载均衡，井然有序
但是要花钱 - 云厂商服务按小时计费

本地部署集群（高性价比）：
✅ 局域网内有效 - 只能在内网访问
✅ EXTERNAL-IP永远pending - 没有云厂商撑腰，没有外部IP可以直接通过浏览器访问
想体验LoadBalancer，需要额外方案 - 要么装MetalLB模拟IP资源池（只能在内网访问），要么老实用NodePort

4️⃣ 小结
NodePort = 自己开门 - 便宜但端口奇葩，而且不安全
LoadBalancer = 请门卫 - 专业且让人放心，但要花钱

Service外部访问就两招：NodePort自己开门，LoadBalancer请门卫，选哪个看钱包！

到此篇为止，我已经完整介绍了 POD 相关的创建，控制器，访问，对外暴露等。有了这些基本知识，服务基本可以上线了 👍

章节7:
Pod装修指南：给共享宿舍来个精装修 🏠

大家好！前面我们聊了Pod这个"共享宿舍"和它的管理员们，也知道了怎么开门迎客。今天该给这个宿舍好好装修一下了！毕竟，光有房子不行，还得有密码锁、限电器这些基础设施，不然舍友们住得不安全也不舒服。

核心问题：宿舍需要哪些装修？

基础装修清单
Secret（保险箱）：存放重要的密码和证件
Resource Limits（限电器）：防止某个舍友用电过度
ConfigMap（公告板）：贴一些公共配置信息
Volume（储物柜）：数据持久化存储

今天重点聊前两个：保险箱和限电器！

Secret：宿舍里的小保险箱 🔐
就像宿舍里放个小保险箱，专门存放重要的密码、证件、私房钱

必要性：
数据库密码不能明文写在代码里（就像不能把银行卡密码贴在卡上）
API密钥需要安全存储（泄露了就完蛋）
证书文件要保密（HTTPS证书很重要）
请参考前篇文章 “Kubernetes Secret 踩坑案例”，具体了解 secret 的配置

举个栗子：
Secret就像宿舍的小保险箱，只有知道密码的舍友才能打开
环境变量就像把保险箱里的东西悄悄递给需要的人
base64编码就像给密码穿了件"隐身衣"，不是真正的加密但至少不会一眼看穿

Resource Limits：宿舍限电器
就像宿舍装个智能限电器，防止某个舍友开太多电器把整个宿舍电闸跳了

必要性：
防止"电老虎"舍友：某个容器占用过多CPU/内存
保证公平使用：每个容器都有自己的资源配额
避免宿舍"停电"：防止整个节点资源耗尽

举个栗子：
requests：就像宿舍保证每个人至少有1000W的用电额度
limits：就像限电器设置最高不能超过3000W，超了就跳闸
没有limits：就像某个舍友开了10个暖风机，把整栋楼都搞停电了

来一个给nginx宿舍装修的案例：

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-luxury-edition
spec:
  replicas: 2
  selector:
    matchLabels:
      app: nginx-luxury
  template:
    metadata:
      labels:
        app: nginx-luxury
    spec:
      containers:
      - name: nginx
        image: nginx:latest
        # 🔐 从保险箱里拿密码
        env:
        - name: ADMIN_USER
          valueFrom:
            secretKeyRef:
              name: nginx-secrets
              key: admin-user
        - name: ADMIN_PASS
          valueFrom:
            secretKeyRef:
              name: nginx-secrets
              key: admin-pass
        # ⚡ 安装限电器
        resources:
          requests:
            memory: "64Mi"    # 基础内存：64MB
            cpu: "50m"        # 基础CPU：0.05核心
          limits:
            memory: "128Mi"   # 最大内存：128MB
            cpu: "100m"       # 最大CPU：0.1核心
        ports:
        - containerPort: 80
```

装修前（毛坯）：
❌ 密码明文写在配置里，一眼就被看穿
❌ 没有资源限制，某个容器可能把节点搞崩
❌ 就像住在毛坯房里，啥都没有

装修后（像模像样了）：
✅ 密码安全存储在Secret里，需要授权才能访问
✅ 资源使用有序，每个容器都有自己的"用电额度"
✅ 就像住进了精装修公寓，安全又舒适

Pod装修就像宿舍装修：Secret是保险箱保护隐私，Resource Limits是限电器保证公平，装修到位了大家住得才安心！



章节8:
Pod装修进阶：公告板和储物柜的艺术 📋

大家好！上次我们给共享宿舍装了保险箱和限电器，今天继续装修大业，聊聊另外两个必备神器：公告板和储物柜！毕竟，一个完美的宿舍不仅要安全（Secret），还要有序（ConfigMap）和实用（Volume）。

今日装修清单：
ConfigMap（公告板）：贴一些公共配置信息，让大家都知道WiFi密码，啥时候停水停电等
Volume（移动储物柜）：数据持久化存储，重要东西不能丢

ConfigMap：宿舍里的智能公告板，就像宿舍门口贴个智能公告板，写着WiFi密码、垃圾分类规则、外卖电话等公共信息
配置信息要公开透明（不像Secret那么神秘）
多个应用共享配置（大家都用同一个WiFi）
配置变更要方便（换个WiFi密码不用重装系统）

注意点：
不要把敏感信息放公告板上（那是保险箱的活）
配置变更后记得重启Pod（公告板更新了要通知大家）
合理组织配置结构（不要把公告板贴得乱七八糟）

ConfigMap的创建：
```yaml
# 智能公告板配置
apiVersion: v1
kind: ConfigMap
metadata:
  name: dorm-config
data:
  wifi-password: "dorm123456"
  takeout-phone: "400-123-4567"
  cleaning-schedule: "Monday: 小明, Tuesday: 小红"
  app.properties: |
    server.port=8080
    database.url=jdbc:mysql://localhost:3306/mydb
    log.level=INFO
```


Volume：宿舍里的移动储物柜
工作原理： 就像给宿舍配个移动储物柜，重要东西放里面，搬家时也能带走
数据要持久化（Pod重启数据不能丢）
多个容器共享数据（舍友之间共享文件）
数据要备份（重要文件不能只存一份）

注意点：
选择合适的存储类型（临时用emptyDir，持久用PVC）
注意挂载路径冲突（不要把两个储物柜放同一个位置）
定期备份重要数据（储物柜也可能丢失）

Volume的类型：
emptyDir：临时储物箱，Pod删除时数据就没了；缓存文件、临时日志
hostPath：就像借用房东家的储物间，数据存在节点上；需要访问节点文件系统
persistentVolumeClaim：就像租个专业储物柜，数据永久保存；数据库文件、用户上传文件


实际装修案例：给nginx宿舍升级装修

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-decorate-v2
spec:
  replicas: 2
  selector:
    matchLabels:
      app: nginx-decorate
  template:
    metadata:
      labels:
        app: nginx-decorate
    spec:
      containers:
      - name: nginx
        image: nginx:latest
        # 从公告板读取配置
        env:
        - name: SERVER_NAME
          valueFrom:
            configMapKeyRef:
              name: nginx-config
              key: server-name
        # 挂载储物柜
        volumeMounts:
        - name: nginx-config-volume
          mountPath: /etc/nginx/conf.d
        - name: nginx-data-volume
          mountPath: /usr/share/nginx/html
        - name: nginx-logs-volume
          mountPath: /var/log/nginx
        ports:
        - containerPort: 80
      volumes:
      # 公告板挂载
      - name: nginx-config-volume
        configMap:
          name: nginx-config
      # 数据储物柜
      - name: nginx-data-volume
        persistentVolumeClaim:
          claimName: nginx-data-pvc
      # 日志储物柜
      - name: nginx-logs-volume
        emptyDir: {}
```

装修后：
- ✅ 智能公告板（ConfigMap）：配置管理
- ✅ 移动储物柜（Volume）：数据持久化
- ✅ 就像从经济型酒店升级到五星级套房

总结
第二轮装修后：Secret保险箱守护隐私，ConfigMap公告板分享信息，Volume储物柜保存数据，Resource Limits限电器控制资源，宿舍生活越来越好了！


章节9:
Pod装修终极版：选宿舍楼的艺术与准入指南 �

大家好！前面我们给共享宿舍装了保险箱、限电器、公告板、储物柜，今天来聊聊最高级的装修技能：选宿舍楼！住哪栋楼直接影响居住体验。

选楼指南

Node Affinity（选楼偏好）：我想住哪种类型的楼
Pod Affinity（邻居偏好）：我想住在某些服务附近
Pod Anti-Affinity（避坑指南）：我绝对不和某些应用住同楼
Taint/Toleration（准入制度）：特殊楼栋需要通行证

Node Affinity：选楼类型 🏢

工作原理： 根据楼栋设施选择，就像"我只住有空调的楼"

使用场景：
- GPU应用要住有显卡的楼
- 就近部署，降低网络延迟

Pod Affinity：选邻居 🤝

工作原理： 选择"靠近图书馆的宿舍"，希望和某些服务住得近

使用场景：
- Web应用住在缓存服务附近，降低延迟
- 相关服务住一起，方便数据共享

Pod Anti-Affinity：避坑指南 ⚡

工作原理： "避雷指南"，绝对不和某些应用住同一栋楼

使用场景：
- 多个副本分散部署，避免"一锅端"
- CPU密集型应用分开，避免抢资源

Taint & Toleration：准入制度 🏷️

工作原理： 给楼栋贴标签"仅限研究生"，只有符合条件的应用才能入住

Taint（楼栋标签）： 管理员设置准入要求，即接受哪些人入住
Toleration（通行证）： 应用声明资格，即声明我愿意入住一定标准的楼。
二者结合，才能双向奔赴，达到最好的匹配状态

选楼策略对比 📊

| 策略类型 | 作用 | 使用场景 |
|---------|------|----------|
| Node Affinity | 选楼栋类型 | 选择特定硬件节点 |
| Pod Affinity | 选邻居 | 服务间协作优化 |
| Pod Anti-Affinity | 避坑指南 | 高可用分散部署 |
| Taint/Toleration | 准入制度 | 专用节点管理 |

调度策略优先级 🎯

Required（硬性要求）： 必须满足，否则不能入住
Preferred（软性偏好）： 尽量满足，但不是必须的

实际选楼案例 🏗️

高可用Web应用需求：
- 3个副本分散部署（避免单点故障）
- 希望住在缓存服务附近（提高性能）
- 只能住计算型节点（硬件要求）

选楼策略：
- Node Affinity：只选计算型楼栋
- Pod Affinity：优先靠近Redis缓存
- Pod Anti-Affinity：3个副本住不同楼栋

常见搭配 🏠

黄金搭配：
- Web服务靠近缓存：降低延迟
- 数据库主从分离：提高可用性

避坑组合：
- 多个数据库副本：绝对不能住同楼
- CPU密集型应用：要分散部署

选楼小贴士 💡

- 不要设置过多限制：可能导致无楼可住
- 专用楼栋才设Taint：普通楼不要随便贴标签
- 考虑集群规模：小集群别设太严格要求

一句话总结 🎯

Pod终极装修：Node Affinity选楼栋，Pod Affinity/Anti-Affinity管邻居，Taint/Toleration控门槛，科学选楼让每个Pod都住得舒心！ 🏠

章节10:
Pod装修收尾工程：软装让生活更美好 ✨

大家好！前面我们给共享宿舍进行了3轮硬装修，今天该来聊聊软装了！毕竟，硬装解决"能住"的问题，软装解决"住得好"的问题。

软装三件套：
Health Checks（健康监测）：安装智能健康检测设备
Security Context（安全管家）：设置门禁权限和安全规则
Init Containers（保洁阿姨）：入住前的清洁和准备工作

Health Checks：就像给宿舍装个智能健康监测系统，随时检查宿舍的各种状况
- 及时发现问题：应用挂了要马上知道
- 避免服务中断：不健康的Pod不接收流量
- 自动恢复：检测到问题自动重启

三种健康检查：
1. Liveness Probe（生命体征检查）
```yaml
livenessProbe:
  httpGet:
    path: /health
    port: 8080
  initialDelaySeconds: 30
  periodSeconds: 10
```
就像每10秒问一次："你还活着吗？"如果连续几次没回应，就重启Pod

2. Readiness Probe（就绪检查）
```yaml
readinessProbe:
  httpGet:
    path: /ready
    port: 8080
  initialDelaySeconds: 5
  periodSeconds: 5
```
就像问："你准备好接客了吗？"没准备好就不给你分配流量

3. Startup Probe（启动检查）
```yaml
startupProbe:
  httpGet:
    path: /startup
    port: 8080
  failureThreshold: 30
  periodSeconds: 10
```
就像问："你启动完了吗？"给慢启动的应用更多时间

生动比喻：
- Liveness = 心跳检测器，确保还活着
- Readiness = 门铃，确认是否可以接待客人
- Startup = 闹钟，确认是否已经起床

Security Context：宿舍安全管家

工作原理： 就像给宿舍配个专业安全管家，管理谁能进、谁能做什么

为什么需要Security Context？
- 权限控制：不是谁都能当管理员
- 安全隔离：防止恶意操作
- 合规要求：满足安全审计标准

主要安全设置：
1. 用户权限控制
```yaml
securityContext:
  runAsUser: 1000        # 以普通用户身份运行
  runAsGroup: 3000       # 设置用户组
  runAsNonRoot: true     # 禁止以root运行
```

2. 文件系统权限
```yaml
securityContext:
  fsGroup: 2000          # 文件系统用户组
  readOnlyRootFilesystem: true  # 根文件系统只读
```

3. 特权控制
```yaml
securityContext:
  allowPrivilegeEscalation: false  # 禁止提权
  privileged: false                # 禁止特权模式
  capabilities:
    drop:
    - ALL                          # 删除所有特权
```

生动比喻：
- runAsUser = 给每个住户发身份证
- readOnlyRootFilesystem = 把重要文件锁在保险柜里
- capabilities = 限制住户的"超能力"

Init Containers：专业保洁阿姨 🧹

工作原理： 就像请专业保洁阿姨，在正式入住前把房间打扫干净、准备就绪

为什么需要Init Containers？
- 环境准备：数据库初始化、文件下载
- 依赖检查：确保依赖服务已启动
- 权限设置：修改文件权限、创建目录

Init Container配置：
```yaml
initContainers:
- name: init-myservice
  image: busybox:1.28
  command: ['sh', '-c', 'until nslookup myservice; do echo waiting for myservice; sleep 2; done;']
- name: init-mydb
  image: busybox:1.28
  command: ['sh', '-c', 'until nslookup mydb; do echo waiting for mydb; sleep 2; done;']
```

工作流程：
1. Init Container 1 执行完成
2. Init Container 2 执行完成
3. 主容器才开始启动

生动比喻：
- 像酒店的客房服务，客人入住前把一切准备好
- 按顺序工作，前一个保洁阿姨干完，下一个才开始
- 所有准备工作完成，客人才能入住

软装配置实战案例 

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webapp-deluxe
spec:
  replicas: 2
  selector:
    matchLabels:
      app: webapp-deluxe
  template:
    metadata:
      labels:
        app: webapp-deluxe
    spec:
      # 保洁阿姨先工作
      initContainers:
      - name: init-db-check
        image: busybox
        command: ['sh', '-c', 'until nc -z db-service 5432; do sleep 1; done']

      containers:
      - name: webapp
        image: nginx:latest
        ports:
        - containerPort: 80

        # 健康监测设备
        livenessProbe:
          httpGet:
            path: /
            port: 80
          initialDelaySeconds: 30
          periodSeconds: 10

        readinessProbe:
          httpGet:
            path: /
            port: 80
          initialDelaySeconds: 5
          periodSeconds: 5

        # 安全管家设置
        securityContext:
          runAsNonRoot: true
          runAsUser: 1000
          readOnlyRootFilesystem: true
          allowPrivilegeEscalation: false
```

软装效果对比 📊

硬装完成后：
- ✅ 有地方住（基础功能）
- ✅ 安全存储（Secret、ConfigMap）
- ✅ 资源管理（Resource Limits）
- ✅ 位置优化（Affinity）

软装完成后：
- ✅ 健康监控（随时知道状态）
- ✅ 安全防护（权限控制到位）
- ✅ 环境整洁（Init Container准备）
- ✅ 生活品质大幅提升

软装小贴士 💡

Health Checks建议：
- 设置合理的检查间隔：太频繁浪费资源，太稀疏发现问题慢
- 区分不同检查类型：Liveness检查要宽松，Readiness检查要严格
- 提供专门的健康检查接口：不要用业务接口做健康检查

Security Context建议：
- 最小权限原则：能用普通用户就别用root
- 只读文件系统：能只读就别可写
- 定期审计：检查是否有不必要的权限

Init Container建议：
- 保持简单：Init Container应该做简单的准备工作
- 快速执行：不要让Init Container运行太久
- 幂等操作：多次执行结果应该一样

一句话总结 🎯

Pod软装三件套：Health Checks当健康管家随时体检，Security Context当安全管家控制权限，Init Containers当保洁阿姨提前准备，让你的Pod住得安全、舒适、放心！ 🏠

*记住：硬装让Pod能跑起来，软装让Pod跑得更好！* ✨

章节11:
Kubernetes Namespace: 大学校园的院系管理

大家好！Kubernetes Pod 这个共享宿舍，在多篇连载的持续介绍下，加上操作视频的展示，基本介绍完毕。现在宿舍越来越多了，该考虑如何科学地分区管理了。就像一所大学有不同的院系一样，K8S 也需要用 Namespace 来划分不同的"院系"，让资源管理更有序。

1️⃣ 核心问题：宿舍多了怎么管理？ 🤔
按照常理，都需要按照专业职能来进行如下区分：
👉 计算机科学系的学生住一片区域
👉 物理系的学生住另一片区域
👉 每个院系有自己的管理制度和资源

2️⃣ Namespace：院系分区管理，就像大学把学生按院系分区管理，Namespace 把 K8s 资源按用途分组管理
👉 资源隔离：计算机系的实验不会影响物理系
👉 权限管理：每个院系有自己的管理员
👉 命名冲突：不同院系可以有同名的实验室
👉 资源配额：每个院系有自己的预算和资源限制

3️⃣ 集群预置 Namespace
👉 default（主校区）：就像大学的主校区，没有特别指定院系的都在这里
“kubectl get pods” 默认显示default 命名空间的Pod

👉 kube-system（行政管理区）：就像大学的行政楼，校长、教务处、后勤部门都在这里
“kubectl get pods -n kube-system” 

👉 kube-public（公共区域）：就像大学的图书馆、体育馆等公共设施
kubectl get pods -n kube-public

4️⃣ 创建 Namespace
如图2，创建命名空间可使用命令行或者 yaml 文件，有了自定义的命名空间以后，就可以在它的地盘通过控制器来创建 Pod 了，如图3。

5️⃣ Namespace的各项操作
如图4：
👉 跨院系的访问需要指定服务所在的 namespace 并符合特定的格式
👉 查看各院系的部署对象、资源等（当然，你得有足够的权限才可以）
👉 清理命名空间（这可是删库跑路的节奏🎵，需谨慎）

6️⃣ Namespace就像大学的院系管理：不同院系有各自的学生、老师、实验室和预算，既保持独立又能协作，让整个校园井然有序！合理的院系划分是大学管理的基础，也是K8s集群管理的关键！


1️⃣ 创建新院系
👉 创建计算机科学系 (computer-science.yaml)：
```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: computer-science
  labels:
    department: "cs"
    budget: "high"
```
kubectl apply -f computer-science.yaml

👉 命令行快速创建：
```bash
kubectl create namespace computer-science
```

2️⃣ 在指定院系部署应用 
在计算机科学系部署Web应用：
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-app
  namespace: computer-science  # 指定院系
spec:
  replicas: 3
  selector:
    matchLabels:
      app: web-app
  template:
    metadata:
      labels:
        app: web-app
    spec:
      containers:
      - name: nginx
        image: nginx:latest
        ports:
        - containerPort: 80
```

1️⃣ 跨院系访问 
👉 院系内部访问（简单）：
```bash
# 在computer-science院系内
curl http://web-service
```
👉 跨院系访问（需要全名）：
```bash
# 从computer-science访问physics的服务
curl http://database-service.physics.svc.cluster.local
```

2️⃣ Kubernetes DNS命名规则：
```
<service-name>.<namespace>.<service-type>.cluster.local
```

3️⃣ 院系资源管理 
查看各院系资源：
```bash
# 查看所有院系
kubectl get namespaces
# 查看计算机科学系的所有资源
kubectl get all -n computer-science
# 查看物理系的Pod
kubectl get pods -n physics
```

4️⃣ 设置默认院系：
```bash
# 设置默认工作在computer-science院系
kubectl config set-context --current --namespace=computer-science
# 现在所有命令默认在CS院系执行
kubectl get pods  # 等同于 kubectl get pods -n computer-science
```

5️⃣ 资源清理：
```bash
# 删除整个院系（谨慎操作！）
kubectl delete namespace computer-science
# 这会删除该院系下的所有资源
```

章节12:
ResourceQuota：院系预算管理办公室 🏛️

大家好！今天接着上篇的namespace，继续Kubernetes的资源管理话题。大学的院系（namespace）分好了，但问题来了：如果不加管理，计算机科学系可能把整个学校的电费都用光！今天聊聊如何给院系设预算。

核心问题：如何防止某个院系把学校搞破产？ 🤔

精细管理是稳步发展的关键，来看看下面的情况：

👉 计算机科学系：挖矿、AI训练，电费爆表
👉 物理系：粒子加速器24小时运转
👉 化学系：各种设备全开，水电费飞涨
👉 学校财务：这个月预算超支500%！😱

这时候，需要ResourceQuota（院系预算管理办公室）来把关了！

ResourceQuota工作原理 💰

就像给每个院系设置年度预算，超了就不能再申请资源，防止某个院系把所有资源都占了。

为什么需要ResourceQuota？
- 防止资源垄断：某个院系不能把所有资源都占了
- 成本控制：每个院系有明确的资源预算
- 公平分配：确保各院系都能获得合理资源
- 避免雪崩：防止某个院系的问题影响整个集群

回顾：Pod的个人资源设置
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: student-pod
spec:
  containers:
  - name: app
    image: nginx
    resources:
      requests:          # 保底需求 ⬇️
        memory: "128Mi"   # 至少要128MB内存
        cpu: "100m"       # 至少要0.1个CPU核心
      limits:            # 最高上限 ⬆️
        memory: "512Mi"   # 最多用512MB内存
        cpu: "500m"       # 最多用0.5个CPU核心
```
- requests = 保证的基础用电量（1000W）
- limits = 断路器设置（3000W，超了就跳闸）

ResourceQuota：院系预算管理办公室 🏛️

给每个院系设置年度预算，超了就不能再申请资源，防止某个院系把所有资源都占了。

计算机科学系的预算设置：
```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: cs-budget
  namespace: computer-science
spec:
  hard:
    # CPU和内存总预算
    requests.cpu: "10"           # 总共10个CPU核心
    requests.memory: 20Gi        # 总共20GB内存
    limits.cpu: "20"             # 最高峰值20个CPU核心
    limits.memory: 40Gi          # 最高峰值40GB内存

    # 对象数量限制
    pods: "50"                   # 最多50个Pod（50个学生）
    services: "10"               # 最多10个Service
    secrets: "20"                # 最多20个Secret
    configmaps: "30"             # 最多30个ConfigMap

    # 存储预算
    persistentvolumeclaims: "10" # 最多10个存储申请
    requests.storage: 100Gi      # 总共100GB存储
```

物理系的预算设置（预算较少）：
```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: physics-budget
  namespace: physics
spec:
  hard:
    requests.cpu: "5"            # 总共5个CPU核心
    requests.memory: 10Gi        # 总共10GB内存
    limits.cpu: "10"             # 最高峰值10个CPU核心
    limits.memory: 20Gi          # 最高峰值20GB内存
    pods: "20"                   # 最多20个Pod
    services: "5"                # 最多5个Service
```

实际效果演示 🎬

创建超预算的Pod会怎样？
```yaml
# 尝试在physics系创建大内存Pod
apiVersion: v1
kind: Pod
metadata:
  name: big-pod
  namespace: physics
spec:
  containers:
  - name: app
    image: nginx
    resources:
      requests:
        memory: 15Gi  # 超过physics系的总预算10Gi
```

结果：
```
Error: exceeded quota: physics-budget, requested: requests.memory=15Gi,
used: requests.memory=8Gi, limited: requests.memory=10Gi
```
*翻译：物理系预算不够了！你要15GB，但总预算只有10GB，已经用了8GB*

查看和管理
```bash
# 查看所有院系配额
kubectl get quota --all-namespaces

# 查看预算使用情况
kubectl describe quota cs-budget -n computer-science

# 实时监控资源使用
kubectl top pods -n computer-science
```

最佳实践 💡

- 生产环境：设置严格预算防止资源滥用
- 开发环境：可以宽松一些便于调试和实验
- 监控告警：预算使用超过80%时告警
- 定期审查：根据实际使用情况调整配额
- 分层管理：不同重要级别的院系设置不同预算

一句话总结 🎯

ResourceQuota就像大学的财务处，给每个院系设置明确的资源预算，防止某个院系把学校搞破产，确保资源公平分配！ 🏛️

*记住：没有预算管理的大学会破产，没有ResourceQuota的K8s集群会崩溃！* 💰

章节13:
LimitRange：宿舍用电用水标准 🏠

大家好！上一章我介绍了给院系设置预算（ResourceQuota），今天来聊聊如何给每个宿舍制定用电用水标准。有了院系总预算这个大盘，还需要管好每个宿舍的具体使用情况，这样才能对 K8S 集群进行更精细的管理，一句话，省💰！尤其是你在用云端资源的场合😂

1️⃣ LimitRange：就像宿舍管理规定，给每个宿舍设置用电用水的标准和上限，防止某个学生把整栋楼电用光，同时给新学生设置标准配置。
想象一下这些场景：
👉 计算机系学生：在宿舍挖矿，功率拉满，整栋楼跳闸
👉 物理系研究生：实验设备24小时运转，电费爆表
👉 新入学学生：不知道标准配置，申请资源时一脸懵
👉 宿舍管理员：每天处理各种资源纠纷，头疼不已
这时候，需要LimitRange（宿舍用电用水标准）来规范管理！

2️⃣ 为什么需要LimitRange？
👉 防止个别宿舍过度消耗：某个学生不能把整栋楼电用光
👉 设置合理默认值：新学生入住时有标准配置
👉 强制最低标准：确保每个应用都有基本资源
👉 简化管理：统一的标准减少管理复杂度

3️⃣ 与ResourceQuota的关系 🔗
ResourceQuota = 院系总预算（计算机系总共100kW电力配额）
LimitRange = 宿舍标准（每个宿舍最多3kW，默认1kW）

两者配合使用：
👉 ResourceQuota控制总量
👉 LimitRange控制单体
👉 双重保险，确保资源合理分配

4️⃣ LimitRange的创建、使用以及维护等场景，请查看图2，3，4

5️⃣ 使用建议
👉 设置合理默认值：新应用有标准配置，减少配置错误
👉 上限不要太严格：给特殊需求留空间
👉 下限要保证基本运行：避免资源不足导致故障
👉 配合ResourceQuota使用：双重保险确保资源合理分配
👉 定期审查标准：根据实际使用情况调整标准

良好管理的宿舍标准让大家住得安心，好的 LimitRange 让 Pod 跑得稳定！


宿舍标准配置实战 📋

计算机科学系的宿舍标准：
```yaml
apiVersion: v1
kind: LimitRange
metadata:
  name: cs-dorm-standards
  namespace: computer-science
spec:
  limits:
  # Pod级别的限制（整个宿舍）
  - type: Pod
    max:
      cpu: "2"                   # 单个Pod最多2个CPU核心
      memory: 4Gi                # 单个Pod最多4GB内存
    min:
      cpu: "100m"                # 单个Pod至少0.1个CPU核心
      memory: 128Mi              # 单个Pod至少128MB内存

  # Container级别的限制（单个学生）
  - type: Container
    default:                     # 默认配置（新学生标准）
      cpu: "200m"                # 默认0.2个CPU核心
      memory: 256Mi              # 默认256MB内存
    defaultRequest:              # 默认请求（保底配置）
      cpu: "100m"                # 保底0.1个CPU核心
      memory: 128Mi              # 保底128MB内存
    max:                         # 单个容器上限
      cpu: "1"                   # 最多1个CPU核心
      memory: 2Gi                # 最多2GB内存
    min:                         # 单个容器下限
      cpu: "50m"                 # 至少0.05个CPU核心
      memory: 64Mi               # 至少64MB内存
```

物理系的宿舍标准（更严格）：
```yaml
apiVersion: v1
kind: LimitRange
metadata:
  name: physics-dorm-standards
  namespace: physics
spec:
  limits:
  - type: Container
    default:
      cpu: "100m"                # 默认更少的CPU
      memory: 128Mi              # 默认更少的内存
    defaultRequest:
      cpu: "50m"
      memory: 64Mi
    max:
      cpu: "500m"                # 上限也更低
      memory: 1Gi
```

实际效果演示 🎬

新学生入住时的自动配置：
```yaml
# 新学生创建Pod时没有指定resources
apiVersion: v1
kind: Pod
metadata:
  name: new-student-pod
  namespace: computer-science
spec:
  containers:
  - name: app
    image: nginx
    # 没有指定resources，LimitRange会自动设置默认值
```

K8s会自动添加：
```yaml
resources:
  requests:
    cpu: "100m"      # 来自defaultRequest
    memory: 128Mi
  limits:
    cpu: "200m"      # 来自default
    memory: 256Mi
```

查看和管理
```bash
# 查看宿舍标准
kubectl describe limitrange cs-dorm-standards -n computer-science

# 查看所有院系的宿舍标准
kubectl get limitrange --all-namespaces

# 检查Pod的实际资源配置
kubectl describe pod new-student-pod -n computer-science
```

一句话总结 🎯

LimitRange就像宿舍管理规定，给每个宿舍设置用电用水标准，防止个别学生作妖，同时给新学生提供标准配置！ 🏠

*记住：好的宿舍标准让大家住得安心，好的LimitRange让Pod跑得稳定！* ⚡

章节14:
PV & PVC：大学的储物柜管理系统 📦
大家好！前面聊完了院系预算和宿舍标准，今天来聊校园的储物管理！大学有储物管理系统，K8S也有存储管理体系，毕竟 Pod 这个共享宿舍如果总是重启的话，数据丢失就说不过去了😂

1️⃣ 数据放哪里才不会丢？ 🤔
👉 网站用户上传的图片，Pod重启后全没了 😭
👉 数据库存储的订单信息，容器挂了就消失了 😱
👉 应用生成的日志文件，重新部署就找不到了 😂
这时候，需要PV（储物柜）和PVC（储物申请）来拯救数据，对数据做持久化处理！

2️⃣ K8S 存储的优势：
👉 数据和应用分离：应用挂了，数据还在
👉 自动管理：K8s帮你处理绑定和挂载
👉 灵活调度：数据可以跟着Pod走

3️⃣ PV (PersistentVolume)：大学的储物柜系统 🗄️
如图2，PV 就像大学提供的储物柜，独立于学生存在，学生毕业了储物柜还在。
👉 独立存在
👉 集群共享
👉 有生命周期
👉 类型丰富

4️⃣ PVC (PersistentVolume Claim)：储物柜申请单 📝
如图3，就像学生填写储物柜申请表，告诉管理员需要什么样的储物柜。
👉 需求描述
👉 自动匹配
👉 抽象层：Pod不需要知道具体是本地硬盘还是网络存储，只管用就行

5️⃣ PV与PVC的相亲匹配 🔗
提交申请 → 自动匹配 → 绑定成功 → Pod使用，如图4

6️⃣ 存储访问模式
如图5，存储访问一共有三种模式，根据使用场景进行合理选择 🔑，举几个 🌰 如下：
👉 数据库用RWO，确保数据一致性
👉 静态文件用ROX，多Pod共享读取
👉 日志收集用RWX，多Pod写入
👉 定期备份重要数据，设置合理回收策略，毕竟PV、PVC也会被意外删除

7️⃣ 存储回收策略
👉 Retain：手动回收，数据保留
👉 Delete：自动删除，数据清空
👉 Recycle：清理重用（由于安全性和兼容性问题，已废弃）

对于SRE来说，存储管理是保障业务连续性的关键环节。没有持久化存储，应用重启后数据就丢失了，这在生产环境是不可接受的。K8S 提供的 PV 储物柜和 PVC 申请单的完整体系，让数据和应用分离，确保Pod重启后数据依然安全！

本地储物柜（HostPath PV）：
```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: local-storage-pv
spec:
  capacity:
    storage: 10Gi                    # 储物柜容量：10GB
  accessModes:
  - ReadWriteOnce                    # 单人专用
  persistentVolumeReclaimPolicy: Retain  # 手动回收
  storageClassName: local-storage    # 本地存储类型
  hostPath:
    path: /data/storage              # 节点本地路径
    type: DirectoryOrCreate          # 目录不存在就创建
```

网络共享储物柜（NFS PV）：
```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfs-storage-pv
spec:
  capacity:
    storage: 50Gi                    # 共享储物柜：50GB
  accessModes:
  - ReadWriteMany                    # 多人共享
  persistentVolumeReclaimPolicy: Recycle  # 自动清理重用
  storageClassName: nfs-storage      # NFS存储类型
  nfs:
    server: 192.168.1.100           # NFS服务器地址
    path: /shared/storage           # 共享目录路径
```

个人储物柜申请：
```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-personal-storage
  namespace: computer-science
spec:
  accessModes:
  - ReadWriteOnce                    # 个人专用
  storageClassName: local-storage    # 匹配本地存储PV
  resources:
    requests:
      storage: 5Gi                   # 申请5GB空间
```

共享储物柜申请：
```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: team-shared-storage
  namespace: computer-science
spec:
  accessModes:
  - ReadWriteMany                    # 团队共享
  storageClassName: nfs-storage      # 匹配NFS存储PV
  resources:
    requests:
      storage: 20Gi                  # 申请20GB共享空间
```


匹配条件： 容量匹配、访问模式匹配、存储类型匹配
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: my-app-pod
spec:
  containers:
  - name: app
    image: nginx
    volumeMounts:
    - name: data-volume
      mountPath: /usr/share/nginx/html  # 挂载到容器内
  volumes:
  - name: data-volume
    persistentVolumeClaim:
      claimName: my-personal-storage    # 使用申请的储物柜
```

RWO示例（个人储物柜）：
```yaml
# 数据库Pod专用存储
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mysql-storage
spec:
  accessModes:
  - ReadWriteOnce      # 只有一个Pod能写入
  resources:
    requests:
      storage: 10Gi
```

RWX示例（共享储物柜）：
```yaml
# 多个Pod共享的日志存储
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: shared-logs
spec:
  accessModes:
  - ReadWriteMany      # 多个Pod都能写入
  resources:
    requests:
      storage: 50Gi
```

实际操作 🔧

```bash
# 查看储物柜状态
kubectl get pv

# 查看储物申请
kubectl get pvc -n computer-science

# 查看绑定详情
kubectl describe pvc my-personal-storage -n computer-science
```

章节15:
StorageClass：大学的智能储物管理员 🤖

大家好！上一章聊了 PV(PersistentVolume) 储物柜和 PVC(PersistentVolumeClaim) 申请单，细心的小红薯可能注意到了，每次需要使用存储时，都要手动创建 PV ，这看着很麻烦，其实一点都不方便。今天来聊一个创建 PV 更好的选项： Kubernetes StorageClass(SC)，大学里的"智能储物管理员"。

1️⃣ 为什么需要SC：因为手动管理太累了🥱
👉 学生申请储物柜，管理员要手动去买、装、配置
👉 学生多了，管理员忙不过来
StorageClass解决方案：学生提交申请，系统自动采购、安装、配置！

2️⃣ 工作原理：动态供应流程：
PVC申请 → StorageClass响应 → 调用Provisioner → 自动创建PV → 自动绑定

3️⃣ 三种智能管理员：
👉 本地存储管理员：使用本地存储创建，依赖于集群的各个节点的磁盘存储。
👉 云存储管理员：使用云端存储，虽然选项很多：aws云，阿里云，腾讯云。。这带个云的存储啥都好，就是对钱包不好😂
👉 网络存储管理员：使用公司/部门的网络存储：Network File System，免费的才是真香👍
三种存储请参考图2、3和4，主要参数说明如下：
Provisioner（供应商）：决定用哪家"厂商"制造储物柜
VolumeBindingMode（绑定模式）：Immediate：立即分配储物柜；WaitForFirstConsumer：等学生来了再分配
ReclaimPolicy（回收策略）：Delete：用完自动回收；Retain：保留数据，手动清理
AllowVolumeExpansion（扩容支持）：储物柜是否可以扩大

4️⃣ 使用示例，请参考图5，在创建 PVC 时指定 SC

5️⃣ SC 使用建议：
👉 为不同业务场景创建不同StorageClass
👉 生产环境用高性能存储，开发环境用标准存储
👉 定期清理不用的动态PV，避免成本浪费
👉 敏感数据使用加密存储

StorageClass就像大学里的智能储物管理员，学生只需要提交申请，它就能自动采购、安装、配置储物柜，让存储管理从手工作坊升级为智能工厂！在 K8S 的世界，有了StorageClass，SRE再也不用半夜起来手动创建 PV 了，就它了！


三种智能管理员 🎯

本地存储管理员：
```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: local-fast-storage
provisioner: kubernetes.io/no-provisioner  # 不自动创建，需要手动准备PV
volumeBindingMode: WaitForFirstConsumer    # 等学生来了再分配
allowVolumeExpansion: false                # 不支持扩容
reclaimPolicy: Delete                      # 用完就删除
```

云存储管理员：
```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: aws-gp3-storage
provisioner: ebs.csi.aws.com              # AWS EBS供应商
parameters:
  type: gp3                                # SSD类型
  fsType: ext4                             # 文件系统
  encrypted: "true"                        # 加密存储
allowVolumeExpansion: true                 # 支持扩容
reclaimPolicy: Delete                      # 自动清理
volumeBindingMode: Immediate               # 立即分配
```

网络存储管理员：
```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: nfs-shared-storage
provisioner: nfs.csi.k8s.io               # NFS供应商
parameters:
  server: nfs-server.example.com          # Network File System服务器
  path: /shared/storage                    # 共享路径
  mountOptions: "vers=4.1,rsize=1048576"  # 挂载选项
allowVolumeExpansion: true                 # 支持扩容
reclaimPolicy: Retain                      # 保留数据
volumeBindingMode: Immediate               # 立即分配
```


使用示例
```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-dynamic-storage
  namespace: computer-science
spec:
  accessModes:
  - ReadWriteOnce
  storageClassName: aws-gp3-storage        # 指定智能管理员
  resources:
    requests:
      storage: 10Gi                        # 申请10GB
```


常用命令 📋

```bash
# 查看所有StorageClass
kubectl get storageclass

# 设置默认StorageClass
kubectl patch storageclass aws-gp3-storage -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'
```
killercode:
controlplane:~/k8s$ kubectl get sc
NAME                   PROVISIONER             RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE
local-path (default)   rancher.io/local-path   Delete          WaitForFirstConsumer   false                  13d
test-path              rancher.io/local-path   Retain          WaitForFirstConsumer   false                  4s

1. sc.yaml 
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  annotations:
    storageclass.kubernetes.io/is-default-class: "false"
  name: test-path
provisioner: rancher.io/local-path
reclaimPolicy: Retain
volumeBindingMode: WaitForFirstConsumer

2. create pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: test-pod
spec:
  containers:
  - name: test-container
    image: nginx
    volumeMounts:
    - name: test-volume
      mountPath: /usr/share/nginx/html
  volumes:
  - name: test-volume
    persistentVolumeClaim:
      claimName: local-pvc

3. create pvc
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: local-pvc
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
  storageClassName: local-path

4. pod status become from pending to running
5. kubectl delete pvc local-pvc
6. kubectl get pvc : local-pvc is still there but status is terminating
7. kubectl patch pvc local-pvc --type='merge' -p='{"metadata":{"deletionTimestamp":null}}'
persistentvolumeclaim/local-pvc patched (no change)
kubectl get pvc local-pvc -oyaml | grep -A10 finalizer

8. kubectl patch pvc local-pvc -p '{"metadata":{"finalizers":null}}'
persistentvolumeclaim/local-pvc patched
pvc is deleted -- same effect as to remove whole finalizer section in yaml

9 kubectl exec test-pod -- mkdir -p /usr/share/nginx/html/test
mkdir: cannot create directory '/usr/share/nginx/html/test': No such file or directory
command terminated with exit code 1

10 kubectl get pv : pv had been deleted

章节16:
大家好！前面我们从下往上聊了Kubernetes的共享宿舍（Pod）、楼管大妈（ReplicaSet）、总宿舍管理员（Deployment）、外卖调度中心（Service），今天来聊整个校园的核心——学校总务处（API Server）。

1️⃣ 想象一下，如果你是学生，想要申请宿舍、查询宿舍信息、投诉宿舍问题，你会去找谁？没错，就是学校总务处！在K8S的世界里，API Server就是这个"总务处"，它是整个集群的统一服务窗口。

2️⃣ 请求统一受理
无论你是想创建新的共享宿舍（Pod），还是想调整宿舍数量（修改Deployment），都必须通过总务处。它是唯一的官方渠道：
想申请宿舍？想退宿？想投诉楼管？找总务处！

3️⃣ 身份验证和权限管理
总务处可不是谁来都给办事的。首先，你得出示学生证（身份认证），然后检查你的权限：
👉 研究生可以申请研究生公寓
👉 教职工才能申请教师宿舍
如果你是个校外人员，想混进来申请宿舍？No Door!

4️⃣ 统一调度协调
总务处接到申请后，开始协调各个相关部门：
👉 通知楼管大妈（ReplicaSet）："给他安排个床位"
👉 联系外卖调度中心（Service）："新宿舍的外卖地址记得更新"
👉 协调后勤部门（Scheduler）："看看哪栋楼还有空房间"
就像一个指挥中心，把需求传达给各个执行部门。

5️⃣ 档案管理和信息存储
总务处最重要的职责就是记录存档。所有的宿舍分配信息都存在档案室（etcd）里：
👉 张三住在3号楼201室
👉 李四的宿舍申请在审批中
这些信息必须准确无误，因为其他部门都要依赖这些数据工作。

6️⃣ 标准化服务流程 
为了提高效率，总务处制定了标准的API：
👉 GET：查询信息，查看我的宿舍信息
👉 POST：创建资源，申请新宿舍
👉 PUT：更新信息，我要更换宿舍
👉 DELETE：删除资源，我要退宿
全校师生都按照统一的流程办事。

7️⃣ 高可用性
如果总务处突然关门了，整个宿舍管理系统都会瘫痪！所以，API Server通常会部署多个副本，就像总务处会安排多个值班人员一样。

记住哈，当你使用kubectl命令时，请注意你其实是在和这个"总务处"打交道！


实战演示：如何与总务处（API Server）打交道 💻

虽然我们不能"创建"总务处，但可以通过各种方式与它交互：

基础的总务处业务办理（kubectl 命令）
```bash
# 查看总务处的基本信息
kubectl cluster-info

# 查看总务处支持哪些业务（API 版本）
kubectl api-versions

# 查看总务处能管理哪些资源类型
kubectl api-resources

# 向总务处申请创建宿舍
kubectl apply -f nginx-pod.yaml

# 向总务处查询宿舍状态
kubectl get pods

# 向总务处查询详细的宿舍信息
kubectl describe pod nginx-pod

# 向总务处申请删除宿舍
kubectl delete pod nginx-pod
```

直接与总务处对话（REST API 调用）
```bash
# 获取总务处的地址
APISERVER=$(kubectl config view --minify -o jsonpath='{.clusters[0].cluster.server}')

# 获取访问令牌（学生证）
TOKEN=$(kubectl get secret $(kubectl get serviceaccount default -o jsonpath='{.secrets[0].name}') -o jsonpath='{.data.token}' | base64 --decode)

# 直接向总务处查询所有宿舍信息
curl -X GET $APISERVER/api/v1/pods \
  --header "Authorization: Bearer $TOKEN" \
  --insecure

# 向总务处提交宿舍申请
curl -X POST $APISERVER/api/v1/namespaces/default/pods \
  --header "Authorization: Bearer $TOKEN" \
  --header "Content-Type: application/json" \
  --data '{
    "apiVersion": "v1",
    "kind": "Pod",
    "metadata": {"name": "api-test-pod"},
    "spec": {
      "containers": [{
        "name": "nginx",
        "image": "nginx:latest"
      }]
    }
  }' \
  --insecure
```

编程方式与总务处交互（Python 客户端）
```python
from kubernetes import client, config

# 加载配置（相当于出示学生证）
config.load_kube_config()

# 创建API客户端（连接总务处）
v1 = client.CoreV1Api()

# 向总务处查询所有宿舍
print("向总务处查询宿舍列表:")
pods = v1.list_pod_for_all_namespaces()
for pod in pods.items:
    print(f"宿舍: {pod.metadata.name}, 状态: {pod.status.phase}")

# 向总务处申请新宿舍
pod_manifest = {
    "apiVersion": "v1",
    "kind": "Pod",
    "metadata": {"name": "python-test-pod"},
    "spec": {
        "containers": [{
            "name": "busybox",
            "image": "busybox",
            "command": ["sleep", "3600"]
        }]
    }
}

print("向总务处申请新宿舍...")
v1.create_namespaced_pod(namespace="default", body=pod_manifest)
print("宿舍申请已提交！")
```

监控总务处的工作状态
```bash
# 查看总务处的健康状态
kubectl get componentstatuses

# 查看总务处的详细信息
kubectl get endpoints kubernetes

# 查看总务处的访问日志（如果有权限）
kubectl logs -n kube-system kube-apiserver-<node-name>

# 查看总务处处理的事件
kubectl get events --sort-by=.metadata.creationTimestamp
```

总务处的权限验证演示
```bash
# 创建一个受限用户（普通学生）
kubectl create serviceaccount student-user

# 给学生分配有限权限（只能查看，不能创建）
kubectl create clusterrolebinding student-binding \
  --clusterrole=view \
  --serviceaccount=default:student-user

# 使用学生身份访问总务处
kubectl --as=system:serviceaccount:default:student-user get pods
# ✅ 可以查看宿舍

kubectl --as=system:serviceaccount:default:student-user delete pod nginx-pod
# ❌ 没有权限删除宿舍
```

总务处的负载均衡演示
```yaml
# 如果你有多个API Server（多个总务处窗口）
apiVersion: v1
kind: Endpoints
metadata:
  name: kubernetes
subsets:
- addresses:
  - ip: 192.168.1.100  # 总务处窗口1
  - ip: 192.168.1.101  # 总务处窗口2
  - ip: 192.168.1.102  # 总务处窗口3
  ports:
  - port: 6443
    protocol: TCP
```

总务处的幕后工作流程 🔄

当你执行 `kubectl apply -f pod.yaml` 时，总务处的工作流程：

1. 接收请求：总务处收到你的宿舍申请
2. 身份验证：检查你的学生证是否有效
3. 权限检查：确认你有申请这类宿舍的权限
4. 数据验证：检查申请表是否填写正确
5. 存储记录：将申请信息存入档案室（etcd）
6. 通知相关部门：告诉调度器（Scheduler）安排宿舍
7. 返回结果：告诉你申请已受理

这就是为什么 API Server 被称为 Kubernetes 的"心脏"——所有的操作都要经过它！

章节17:
Kubernetes API Server的特殊身份：静态pod
大家好！之前我们聊了API Server这个"学校总务处"，在操作视频中也看到了API Server其实是kube-system命名空间下的一个Pod。
今天来揭开它的神秘面纱——为什么API Server这么特殊？答案就在于它的"特殊身份"：静态Pod！

1️⃣ 什么是静态Pod 🤔
想象一下，在我们的K8S大学里，大部分学生住在由楼管大妈（ReplicaSet）管理的普通共享宿舍（Pod）里。但是，总务处的工作人员却住在特殊的"VIP宿舍"里——这就是静态Pod！

2️⃣ 静态Pod的特点：
👉 不需要通过总务处申请
👉 直接由设施维护员(kubelet)管理
👉 通常运行在控制平面节点(master节点)上
👉 配置文件就放在本地，随时可以查看
👉 一旦配置文件改变，立即"搬家"重建

3️⃣ 为什么要用静态Pod？🏠
这里有个"鸡生蛋，蛋生鸡"的问题：普通Pod需要通过API Server创建，但如果API Server本身也需要通过API Server来创建，那谁来创建它呢？🤯

K8s设计者想出了绝妙方案：让API Server自力更生，住进"静态Pod"这种特殊宿舍！
工作原理：
👉 设施维护员有特殊文件夹，里面放着"宿舍设计图"
👉 kubelet每隔几秒检查这个文件夹
👉 发现设计图变化，立即重建宿舍

4️⃣ 静态Pod的神奇特性
👉 配置即生效
修改配置文件 → 保存 → 几秒后Pod自动重建 → 新配置生效。就像魔法一样！
👉 本地文件管理，如图2
👉 自动恢复机制：即使API Server挂了，kubelet 仍能管理API Server这个静态Pod本身，会自动尝试重启它。
👉 无法直接删除：用kubectl删除API Server的Pod会发现几秒后又出现了！因为kubelet看到配置文件还在，会立即重建。

5️⃣ 静态Pod家族成员
K8s集群中的控制平面组件都是静态Pod，除了API Server以外，后续我会继续介绍其他组件

API Server使用静态Pod这种特殊身份，就像住在"VIP宿舍"里的总务处工作人员——不需要通过自己来申请宿舍，直接由设施维护员（kubelet）根据本地配置文件管理，实现了"自力更生"的完美闭环！


实际操作体验 🛠️

想体验静态Pod的神奇吗？试试这个：

```bash
# 1. 查看当前API Server配置
sudo cat /etc/kubernetes/manifests/kube-apiserver.yaml

# 2. 备份原文件
sudo cp /etc/kubernetes/manifests/kube-apiserver.yaml /tmp/backup.yaml

# 3. 修改一个无关紧要的标签
sudo vim /etc/kubernetes/manifests/kube-apiserver.yaml
# 在metadata.labels下加一行：test: "modified"

# 4. 保存后观察Pod重建
watch kubectl get pods -n kube-system | grep apiserver

# 5. 恢复原配置
sudo cp /tmp/backup.yaml /etc/kubernetes/manifests/kube-apiserver.yaml
```

