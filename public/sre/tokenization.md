----- Chinese
Tokenization: LLMç†è§£äººç±»è¯­è¨€çš„ç¬¬ä¸€æ­¥
å¤§å®¶å¥½ï¼Œä»Šå¤©æ¥èŠèŠLLMï¼ˆLarge Language Modelï¼Œå¤§è¯­è¨€æ¨¡å‹ï¼‰ä¸­çš„ä¸€ä¸ªé‡è¦æ¦‚å¿µâ€”â€”åˆ†è¯ï¼ˆTokenizationï¼‰ã€‚
## ğŸ¯ æ ¸å¿ƒæ¦‚å¿µè§£é‡Š

åœ¨æ·±å…¥åˆ†è¯æœºåˆ¶ä¹‹å‰ï¼Œè®©æˆ‘ä»¬å…ˆç†è§£ä¸‰ä¸ªå…³é”®æ¦‚å¿µï¼š
ğŸ¤– AI (Artificial Intelligence)ï¼šäººå·¥æ™ºèƒ½ï¼Œè®©æœºå™¨èƒ½å¤Ÿæ¨¡æ‹Ÿäººç±»æ™ºèƒ½è¡Œä¸ºçš„æŠ€æœ¯
ğŸ“ NLP (Natural Language Processing)ï¼šè‡ªç„¶è¯­è¨€å¤„ç†ï¼ŒAIçš„ä¸€ä¸ªåˆ†æ”¯ï¼Œä¸“é—¨å¤„ç†äººç±»è¯­è¨€çš„ç†è§£å’Œç”Ÿæˆ
ğŸ§  LLM (Large Language Model)ï¼šå¤§è¯­è¨€æ¨¡å‹ï¼ŒåŸºäºæ·±åº¦å­¦ä¹ çš„NLPç³»ç»Ÿï¼Œå¦‚GPTã€Geminiç­‰
å®ƒä»¬çš„å…³ç³»ï¼šAI > NLP > LLMï¼Œåˆ†è¯æ˜¯æ‰€æœ‰è¿™äº›ç³»ç»Ÿç†è§£äººç±»è¯­è¨€çš„ç¬¬ä¸€æ­¥ï¼Œä¹Ÿæ˜¯æœ€å…³é”®çš„ä¸€æ­¥ã€‚ä½œä¸ºç°ä»£NLPç³»ç»Ÿçš„åŸºçŸ³ï¼Œå®ƒä¸ºäººæœºäº¤äº’æ­å»ºäº†ä¸€åº§æ¡¥æ¢ã€‚

## ğŸ¯ ä»€ä¹ˆæ˜¯åˆ†è¯ï¼ˆTokenizationï¼‰ï¼Ÿ
åˆ†è¯æ˜¯å°†ç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬åˆ‡åˆ†æˆLLMèƒ½ç†è§£çš„å°å•å…ƒï¼ˆtokensï¼‰çš„è¿‡ç¨‹ã€‚è¿™æ˜¯LLMå¤„ç†è‡ªç„¶è¯­è¨€çš„ç¬¬ä¸€æ­¥ï¼Œä¹Ÿæ˜¯æœ€åŸºç¡€ä½†æœ€é‡è¦çš„ä¸€æ­¥ã€‚

æƒ³è±¡ä¸€ä¸‹ï¼š
- ä½ è¯´ï¼š"ä»Šå¤©åŒ—äº¬çš„å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ"
- LLMå¬åˆ°ï¼š`["ä»Šå¤©", "åŒ—äº¬", "çš„", "å¤©æ°”", "æ€ä¹ˆæ ·", "ï¼Ÿ"]`
- LLMç†è§£ï¼šæ¯ä¸ªtokenéƒ½æ‰¿è½½ç€ç‰¹å®šçš„è¯­ä¹‰ä¿¡æ¯

ğŸ¤” ä¸ºä»€ä¹ˆæ˜¯è¿™æ ·åˆ†è¯ï¼Œè€Œä¸æ˜¯"ä»Š"ã€"å¤©åŒ—"ã€"äº¬çš„"ï¼Ÿ

è¿™æ¶‰åŠåˆ†è¯çš„æ ¸å¿ƒåŸç†â€”â€”è¯­ä¹‰å®Œæ•´æ€§ï¼š

```python
# âŒ é”™è¯¯çš„åˆ†è¯æ–¹å¼ï¼ˆç ´åè¯­ä¹‰ï¼‰
["ä»Š", "å¤©åŒ—", "äº¬çš„", "å¤©æ°”", "æ€ä¹ˆ", "æ ·ï¼Ÿ"]
# "å¤©åŒ—"ã€"äº¬çš„"ã€"æ€ä¹ˆ" éƒ½ä¸æ˜¯å®Œæ•´çš„è¯­ä¹‰å•å…ƒ

# âœ… æ­£ç¡®çš„åˆ†è¯æ–¹å¼ï¼ˆä¿æŒè¯­ä¹‰å®Œæ•´ï¼‰
["ä»Šå¤©", "åŒ—äº¬", "çš„", "å¤©æ°”", "æ€ä¹ˆæ ·", "ï¼Ÿ"]
# æ¯ä¸ªtokenéƒ½æ˜¯æœ‰æ„ä¹‰çš„è¯­ä¹‰å•å…ƒ
```

åˆ†è¯ç®—æ³•çš„æ™ºèƒ½ä¹‹å¤„ï¼š
1. è¯æ±‡è¡¨åŒ¹é…ï¼šLLMè®­ç»ƒæ—¶å­¦ä¹ äº†å¤§é‡è¯æ±‡ï¼ŒçŸ¥é“"ä»Šå¤©"æ˜¯ä¸€ä¸ªå®Œæ•´æ¦‚å¿µ
2. è¯­ä¹‰è¾¹ç•Œè¯†åˆ«ï¼šç®—æ³•èƒ½è¯†åˆ«å“ªäº›å­—ç¬¦ç»„åˆæœ‰ç‹¬ç«‹å«ä¹‰
3. ä¸Šä¸‹æ–‡ä¼˜åŒ–ï¼šåœ¨ä¸åŒè¯­å¢ƒä¸‹é€‰æ‹©æœ€ä½³çš„åˆ†è¯æ–¹æ¡ˆ
4. é¢‘ç‡ç»Ÿè®¡ï¼šé«˜é¢‘è¯ç»„åˆæ›´å®¹æ˜“è¢«è¯†åˆ«ä¸ºå•ä¸ªtoken

è¿™å°±æ˜¯ä¸ºä»€ä¹ˆLLMèƒ½"ç†è§£"è¯­è¨€â€”â€”å®ƒä¸æ˜¯éšæ„åˆ‡åˆ†æ–‡å­—ï¼Œè€Œæ˜¯æŒ‰ç…§è¯­ä¹‰å•å…ƒè¿›è¡Œæ™ºèƒ½åˆ†è¯ï¼ä¸ºäº†å®ç°è¿™ä¸ªåŠŸèƒ½ï¼ŒLLMéœ€è¦ç»è¿‡å¤§é‡çš„è®­ç»ƒå’Œä¼˜åŒ–ã€‚

## ğŸ” åˆ†è¯çš„æ ¸å¿ƒä½œç”¨

ä¸ºä»€ä¹ˆéœ€è¦åˆ†è¯ï¼Ÿ
- è®¡ç®—æœºåªè®¤æ•°å­—ï¼šæ–‡å­—éœ€è¦è½¬æ¢æˆæ•°å­—æ‰èƒ½å¤„ç†
- æ ‡å‡†åŒ–å¤„ç†ï¼šç»Ÿä¸€çš„tokenæ ¼å¼ä¾¿äºåç»­è®¡ç®—
- è¯­ä¹‰å•å…ƒï¼šæ¯ä¸ªtokenæ‰¿è½½ç‰¹å®šçš„è¯­ä¹‰ä¿¡æ¯
- æ¨ç†é€Ÿåº¦ï¼štokenæ•°é‡ç›´æ¥å½±å“ç”Ÿæˆé€Ÿåº¦å’Œå“åº”æ—¶é—´
- å¤šè¯­è¨€èƒ½åŠ›ï¼šä¸“é—¨ä¼˜åŒ–çš„åˆ†è¯å™¨åœ¨ç‰¹å®šè¯­è¨€ä¸Šè¡¨ç°æ›´å¥½

## ğŸš€ çœŸå®æ¡ˆä¾‹å¯¹æ¯”ï¼šåŒä¸€å¥è¯ï¼Œä¸åŒæ¨¡å‹çš„å·¨å¤§å·®å¼‚

è®©æˆ‘ä»¬ç”¨Tiktokenizerå·¥å…·æµ‹è¯•åŒä¸€ä¸ªä¸­æ–‡å¥å­"ä»Šå¤©åŒ—äº¬çš„å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ"ï¼š

```python
# è¾“å…¥ï¼šä»Šå¤©åŒ—äº¬çš„å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿï¼ˆ9ä¸ªä¸­æ–‡å­—ç¬¦ï¼‰

# Google Gemma-7bï¼ˆæœ€ä¼˜è¡¨ç°ï¼‰
gemma_tokens = 7ä¸ª
token_ids = [2, 22180, 35354, 74491, 236147, 67805, 235544]
æ•ˆç‡ = 7/9 â‰ˆ 0.78 tokens/å­—ç¬¦

# CodeLlamaï¼ˆä¸­ç­‰æ•ˆç‡ï¼‰
codellama_tokens = 16ä¸ª
token_ids = [29871, 31482, 30408, 30662, 30675, 30210, 30408, 233, 179, 151, 233, 131, 145, 31882, 31819, 30882]
æ•ˆç‡ = 16/9 â‰ˆ 1.78 tokens/å­—ç¬¦

# GPT-3.5-turboï¼ˆæ•ˆç‡è¾ƒä½ï¼‰
gpt35_tokens = 25ä¸ª  # åŒ…å«å¯¹è¯æ ¼å¼çš„ç‰¹æ®Štoken
token_ids = [100264, 9125, 198, 37271, 36827, 70090, 9554, 36827, 30320, 242, 17486, 236, 82696, 91985, 11571, 100265, 198, 100264, 882, 198, 100265, 198, 100264, 78191, 198]
æ•ˆç‡ = 25/9 â‰ˆ 2.78 tokens/å­—ç¬¦

# GPT-4ï¼ˆæ•ˆç‡æœ€ä½ï¼‰
gpt4_tokens = 60ä¸ª  # å¯¹è¯æ ¼å¼ + æ›´å¤æ‚çš„åˆ†è¯
token_ids = [27, 91, 318, 5011, 91, 29, 9125, 27, 91, 318, 55875, 91, 29, 37271, 36827, 70090, 9554, 36827, 30320, 242, 17486, 236, 82696, 91985, 11571, 27, 91, 318, 6345, 91, 1822, 91, 318, 5011, 91, 29, 78191, 27, 91, 318, 55875, 91, 29]
æ•ˆç‡ = 60/9 â‰ˆ 6.67 tokens/å­—ç¬¦

# æ€§èƒ½å·®è·å¯¹æ¯”
æœ€ä¼˜vsæœ€å·® = 60/7 â‰ˆ 8.6å€æ•ˆç‡å·®è·ï¼
è®¡ç®—é‡å·®è· = (60Â²)/(7Â²) = 3600/49 â‰ˆ 73.5å€ï¼ï¼ï¼
```

ä¸ºä»€ä¹ˆå·®è·è¿™ä¹ˆå¤§ï¼Ÿ

ä¸åŒæ¨¡å‹ä½¿ç”¨ä¸åŒçš„åˆ†è¯ç®—æ³•ï¼š
| æ¨¡å‹ | Tokenæ•°é‡ | åˆ†è¯ç®—æ³• | ç‰¹ç‚¹ | ä¸­æ–‡å¤„ç†èƒ½åŠ› |
|------|----------|----------|------|-------------|
| Gemma-7b | 7ä¸ª | ä¼˜åŒ–çš„SentencePiece | Googleä¸“é—¨ä¼˜åŒ– | ä¼˜ç§€ |
| CodeLlama | 16ä¸ª | BPEå˜ç§ | ä¸“ä¸ºä»£ç ä¼˜åŒ– | ä¸­ç­‰ |
| GPT-3.5-turbo | 25ä¸ª | BPE + å¯¹è¯æ ¼å¼ | è‹±æ–‡+å¯¹è¯ä¼˜åŒ– | æ•ˆç‡ä½ |
| GPT-4 | 60ä¸ª | å¤æ‚BPE + å¯¹è¯æ ¼å¼ | åŠŸèƒ½å¼ºå¤§ä½†tokenæ¶ˆè€—å·¨å¤§ | æ•ˆç‡æœ€ä½ |

å®é™…å½±å“æœ‰å¤šå¤§ï¼Ÿ

```python
# è®¡ç®—å¤æ‚åº¦å¯¹æ¯”ï¼ˆè‡ªæ³¨æ„åŠ›æœºåˆ¶ æ­£æ¯”äº tokenÂ²ï¼‰
Gemmaå¤„ç†æˆæœ¬ = 7Â² = 49å•å…ƒ
CodeLlamaå¤„ç†æˆæœ¬ = 16Â² = 256å•å…ƒ
GPT-3.5å¤„ç†æˆæœ¬ = 25Â² = 625å•å…ƒ
GPT-4å¤„ç†æˆæœ¬ = 60Â² = 3600å•å…ƒ

# å¤§è§„æ¨¡åº”ç”¨çš„æˆæœ¬å·®å¼‚ï¼ˆæ—¥å¤„ç†100ä¸‡æ¬¡è¯·æ±‚ï¼‰
- Gemmaæˆæœ¬ï¼š49Må•å…ƒ
- CodeLlamaæˆæœ¬ï¼š256Må•å…ƒï¼ˆæ¯”Gemmaè´µ5.2å€ï¼‰
- GPT-3.5æˆæœ¬ï¼š625Må•å…ƒï¼ˆæ¯”Gemmaè´µ12.8å€ï¼‰
- GPT-4æˆæœ¬ï¼š3600Må•å…ƒï¼ˆæ¯”Gemmaè´µ73.5å€ï¼ï¼ï¼ï¼‰

# ç†è®ºè®¡ç®—å¤æ‚åº¦å·®å¼‚ï¼ˆç›¸åŒç¡¬ä»¶æ¡ä»¶ä¸‹ï¼‰
Gemmaè®¡ç®—å¤æ‚åº¦ï¼š1.0å€ï¼ˆåŸºå‡†ï¼‰
CodeLlamaè®¡ç®—å¤æ‚åº¦ï¼š5.2å€
GPT-3.5è®¡ç®—å¤æ‚åº¦ï¼š12.8å€
GPT-4è®¡ç®—å¤æ‚åº¦ï¼š73.5å€

æ³¨æ„ï¼šè¿™é‡ŒæŒ‡çš„æ˜¯ç†è®ºè®¡ç®—é‡çš„ç›¸å¯¹æ¯”ä¾‹ï¼Œä¸æ˜¯å®é™…APIå“åº”æ—¶é—´ã€‚å®é™…å“åº”æ—¶é—´å—å¹¶è¡Œè®¡ç®—ã€ç¡¬ä»¶ä¼˜åŒ–ã€ç¼“å­˜ç­‰å› ç´ å½±å“ï¼Œé€šå¸¸åœ¨1-3ç§’å†…ã€‚
è¿™é‡Œçš„å•å…ƒæŒ‡ç†è®ºè®¡ç®—å¤æ‚åº¦ï¼Œå®é™…æˆæœ¬è¿˜å—ç¡¬ä»¶ã€ä¼˜åŒ–ç­‰å› ç´ å½±å“ã€‚
ç›¸åŒçš„ä¸€å¥æç¤ºè¯ï¼Œæ ¹æ®é€‰æ‹©çš„æ¨¡å‹ä¸åŒï¼Œäº§ç”Ÿçš„è´¹ç”¨å·®åˆ«ä¹Ÿè¾ƒå¤§ï¼Œç”±æ­¤å¯è§ï¼šé€‰æ‹©åˆé€‚çš„æ¨¡å‹ï¼Œä¸€å¹´èƒ½èŠ‚çœå‡ åå€çš„è®¡ç®—èµ„æºå’Œæˆæœ¬ï¼

## ä¸ºä»€ä¹ˆGPTç³»åˆ—tokenæ¶ˆè€—è¿™ä¹ˆå¤§ï¼Ÿ

### GPT-3.5-turboï¼ˆ25ä¸ªtokenï¼‰
ä»æˆªå›¾å¯ä»¥çœ‹åˆ°ï¼ŒGPT-3.5çš„25ä¸ªtokenåŒ…å«ï¼š
```
<|im_start|>system
ä»Šå¤©åŒ—äº¬çš„å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ <|im_end|>
<|im_start|>user
<|im_end|>
<|im_start|>assistant
```

### GPT-4ï¼ˆ60ä¸ªtokenï¼ï¼‰
GPT-4æ›´åŠ å¤¸å¼ ï¼ŒåŒæ ·çš„å¥å­éœ€è¦60ä¸ªtokenï¼ä»æˆªå›¾å¯ä»¥çœ‹åˆ°ï¼š
```
<|im_start|>system<|im_sep|>ä»Šå¤©åŒ—äº¬çš„å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ<|im_end|>
<|im_start|>user<|im_sep|><|im_end|><|im_start|>assistant<|im_sep|>
```

å…³é”®å·®å¼‚åˆ†æï¼š
1. æ›´å¤æ‚çš„åˆ†éš”ç¬¦ï¼šGPT-4ä½¿ç”¨äº†`<|im_sep|>`ç­‰é¢å¤–çš„åˆ†éš”ç¬¦
2. æ›´ç»†ç²’åº¦çš„åˆ†è¯ï¼šæ¯ä¸ªæ ‡ç‚¹ã€ç©ºæ ¼éƒ½å¯èƒ½æ˜¯ç‹¬ç«‹token
3. å¯¹è¯æ ¼å¼å¼€é”€ï¼šå¤§éƒ¨åˆ†tokenç”¨äºæ ¼å¼æ§åˆ¶ï¼Œå®é™…å†…å®¹å æ¯”æå°
4. ç‰ˆæœ¬è¿­ä»£æˆæœ¬ï¼šåŠŸèƒ½è¶Šå¼ºå¤§ï¼Œtokenæ¶ˆè€—è¶Šå¤§

## ğŸ¯ åˆ†è¯ç®—æ³•ä¸€è§ˆ
ä¸åŒçš„LLMä½¿ç”¨ä¸åŒçš„åˆ†è¯ç®—æ³•ï¼Œè¿™ç›´æ¥å½±å“äº†å¤„ç†æ•ˆç‡å’Œæˆæœ¬ï¼š
#### 1. SentencePieceï¼ˆGoogleç³»ï¼‰
- ä»£è¡¨æ¨¡å‹ï¼šGemmaã€T5ã€PaLM
- ç‰¹ç‚¹ï¼šä¸“é—¨ä¼˜åŒ–ï¼Œå¯¹ä¸­æ–‡å‹å¥½
- ä¼˜åŠ¿ï¼štokenæ•°é‡å°‘ï¼Œè®¡ç®—æ•ˆç‡é«˜
- é€‚ç”¨åœºæ™¯ï¼šå¤šè¯­è¨€ä»»åŠ¡ï¼Œç‰¹åˆ«æ˜¯ä¸­æ–‡å¤„ç†

#### 2. BPE (Byte Pair Encoding)ï¼ˆOpenAIç³»ï¼‰
- ä»£è¡¨æ¨¡å‹ï¼šGPTç³»åˆ—ã€CodeLlama
- ç‰¹ç‚¹ï¼šåŸºäºå­—èŠ‚å¯¹ç¼–ç ï¼Œè‹±æ–‡ä¼˜åŒ–
- ä¼˜åŠ¿ï¼šå¤„ç†æœªçŸ¥è¯èƒ½åŠ›å¼º
- é€‚ç”¨åœºæ™¯ï¼šè‹±æ–‡ä»»åŠ¡ï¼Œä»£ç ç”Ÿæˆ

#### 3. WordPieceï¼ˆGoogle BERTç³»ï¼‰
- ä»£è¡¨æ¨¡å‹ï¼šBERTã€RoBERTa
- ç‰¹ç‚¹ï¼šå¹³è¡¡è®¾è®¡ï¼Œå…¼é¡¾å¤šè¯­è¨€
- ä¼˜åŠ¿ï¼šåœ¨ç†è§£ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜ç§€
- é€‚ç”¨åœºæ™¯ï¼šæ–‡æœ¬ç†è§£ï¼Œæƒ…æ„Ÿåˆ†æ

## ğŸ”§ ç‰¹æ®ŠTokençš„ä½œç”¨

åœ¨åˆ†è¯è¿‡ç¨‹ä¸­ï¼Œé™¤äº†æ™®é€šçš„æ–‡æœ¬tokenï¼Œè¿˜ä¼šæ·»åŠ ä¸€äº›ç‰¹æ®Šçš„tokenï¼š

### å¸¸è§ç‰¹æ®ŠTokenç±»å‹

#### BERTç³»åˆ—ç‰¹æ®ŠToken
```python
tokens = ["[CLS]", "ä»Šå¤©", "åŒ—äº¬", "çš„", "å¤©æ°”", "æ€ä¹ˆæ ·", "ï¼Ÿ", "[SEP]"]
```
- [CLS]ï¼šClassification tokenï¼Œå¥å­å¼€å§‹æ ‡è®°ï¼Œç”¨äºåˆ†ç±»ä»»åŠ¡
- [SEP]ï¼šSeparator tokenï¼Œå¥å­åˆ†éš”ç¬¦ï¼Œç”¨äºåˆ†éš”ä¸åŒå¥å­
- [PAD]ï¼šPadding tokenï¼Œå¡«å……æ ‡è®°ï¼Œç”¨äºå¯¹é½åºåˆ—é•¿åº¦
- [UNK]ï¼šUnknown tokenï¼ŒæœªçŸ¥è¯æ ‡è®°ï¼Œå¤„ç†è¯æ±‡è¡¨å¤–çš„è¯

#### GPTç³»åˆ—ç‰¹æ®ŠToken
```python
tokens = ["<|im_start|>", "system", "<|im_sep|>", "ä»Šå¤©", "åŒ—äº¬", "çš„", "å¤©æ°”", "æ€ä¹ˆæ ·", "ï¼Ÿ", "<|im_end|>"]
```
- <|im_start|>ï¼šå¯¹è¯å¼€å§‹æ ‡è®°
- <|im_end|>ï¼šå¯¹è¯ç»“æŸæ ‡è®°
- <|im_sep|>ï¼šå¯¹è¯åˆ†éš”ç¬¦
- system/user/assistantï¼šè§’è‰²æ ‡è¯†ç¬¦

ğŸ¤” ä¸ºä»€ä¹ˆGPTè¦ç”¨è¿™äº›"æ˜‚è´µ"çš„ç‰¹æ®ŠTokenï¼Ÿ

ä»æˆæœ¬è§’åº¦çœ‹ï¼Œè¿™ç¡®å®æ˜¯ä¸€ä¸ªçŸ›ç›¾çš„è®¾è®¡ï¼š
- æŠ€æœ¯å†å²ï¼šGPTæœ€åˆä¸ºæ–‡æœ¬ç”Ÿæˆè®¾è®¡ï¼Œåæ¥ç¡¬åŠ äº†å¯¹è¯æ ¼å¼
- åŠŸèƒ½æ¢æˆæœ¬ï¼šç”¨tokenå¼€é”€æ¢å–ç²¾ç¡®çš„å¯¹è¯æ§åˆ¶å’Œå¤šä»»åŠ¡å…¼å®¹
- å•†ä¸šç­–ç•¥ï¼šOpenAIé€‰æ‹©åŠŸèƒ½ä¼˜å…ˆï¼Œå°†é«˜æˆæœ¬è½¬å«ç»™ç”¨æˆ·
- ç«äº‰æœºä¼šï¼šè¿™ç»™äº†Gemmaç­‰"æ•ˆç‡æ´¾"æ¨¡å‹å·¨å¤§çš„ä¼˜åŒ–ç©ºé—´

è¿™å°±æ˜¯ä¸ºä»€ä¹ˆåŒæ ·ä¸€å¥è¯ï¼ŒGPT-4éœ€è¦60ä¸ªtokenè€ŒGemmaåªéœ€è¦7ä¸ªï¼

## ğŸ’¡ åˆ†è¯çš„å®é™…åº”ç”¨åœºæ™¯
### 1. å¯¹è¯ç³»ç»Ÿ
```python
ç”¨æˆ·è¾“å…¥ï¼š"å¸®æˆ‘æŸ¥ä¸€ä¸‹æ˜å¤©çš„å¤©æ°”"
åˆ†è¯ç»“æœï¼š["å¸®", "æˆ‘", "æŸ¥", "ä¸€ä¸‹", "æ˜å¤©", "çš„", "å¤©æ°”"]
LLMç†è§£ï¼šç”¨æˆ·æƒ³è¦æŸ¥è¯¢å¤©æ°”ä¿¡æ¯
LLMæ“ä½œï¼šè°ƒç”¨MCPç­‰æœåŠ¡è·å–å®æ—¶ä¿¡æ¯ï¼Œç”Ÿæˆè¾“å‡º
```
MCP (Model Context Protocol)ï¼šè®©LLMèƒ½å¤Ÿå®‰å…¨åœ°è®¿é—®å¤–éƒ¨æ•°æ®å’ŒæœåŠ¡çš„åè®®ï¼Œå¦‚å®æ—¶å¤©æ°”ã€æ•°æ®åº“æŸ¥è¯¢ç­‰ã€‚

### 2. ä»£ç ç”Ÿæˆ
```python
ç”¨æˆ·è¾“å…¥ï¼š"å†™ä¸€ä¸ªPythonå‡½æ•°è®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—"
åˆ†è¯ç»“æœï¼š["å†™", "ä¸€ä¸ª", "Python", "å‡½æ•°", "è®¡ç®—", "æ–æ³¢é‚£å¥‘", "æ•°åˆ—"]
LLMç†è§£ï¼šéœ€è¦ç”ŸæˆPythonä»£ç ï¼Œå¹¶è¾“å‡ºä»£ç 
```

### 3. å¤šè¯­è¨€ç¿»è¯‘
```python
ç”¨æˆ·è¾“å…¥ï¼š"Hello, how are you?"
åˆ†è¯ç»“æœï¼š["Hello", ",", "how", "are", "you", "?"]
LLMç†è§£ï¼šè‹±æ–‡é—®å€™è¯­ï¼Œéœ€è¦ç¿»è¯‘æˆç›®æ ‡è¯­è¨€ï¼Œå¹¶è¾“å‡ºç¿»è¯‘ç»“æœ
```
## ğŸ’° LLMæ”¶è´¹æ¨¡å¼ï¼šä¸ºä»€ä¹ˆæŒ‰Tokenè®¡è´¹ï¼Ÿ

ç†è§£äº†åˆ†è¯æœºåˆ¶åï¼Œæˆ‘ä»¬å°±èƒ½æ˜ç™½ä¸ºä»€ä¹ˆå‡ ä¹æ‰€æœ‰LLMæœåŠ¡éƒ½é‡‡ç”¨æŒ‰Tokenè®¡è´¹çš„æ¨¡å¼ã€‚

#### 1. è®¡ç®—æˆæœ¬ç›´æ¥ç›¸å…³
```python
# è®¡ç®—å¤æ‚åº¦ âˆ tokenÂ²
7ä¸ªtokençš„è®¡ç®—æˆæœ¬ = 49å•å…ƒ
60ä¸ªtokençš„è®¡ç®—æˆæœ¬ = 3600å•å…ƒï¼ˆ73å€å·®å¼‚ï¼ï¼‰

# æŒ‰tokenè®¡è´¹æœ€å…¬å¹³
ç”¨æˆ·Aï¼š7ä¸ªtoken â†’ ä»˜è´¹49å•å…ƒçš„æˆæœ¬
ç”¨æˆ·Bï¼š60ä¸ªtoken â†’ ä»˜è´¹3600å•å…ƒçš„æˆæœ¬
```

#### 2. èµ„æºæ¶ˆè€—å¯é¢„æµ‹
- å†…å­˜å ç”¨ï¼šæ¯ä¸ªtokenéœ€è¦å›ºå®šçš„å‘é‡å­˜å‚¨ç©ºé—´
- GPUè®¡ç®—ï¼štokenæ•°é‡å†³å®šçŸ©é˜µè¿ç®—è§„æ¨¡
- ç½‘ç»œä¼ è¾“ï¼štokenç›´æ¥å½±å“æ•°æ®ä¼ è¾“é‡

#### 3. ç”¨æˆ·è¡Œä¸ºå·®å¼‚å·¨å¤§
```python
# ä¸åŒç”¨æˆ·çš„tokenæ¶ˆè€—å·®å¼‚ï¼ˆè¾“å…¥+è¾“å‡ºï¼‰
ç®€å•é—®ç­”ï¼š
è¾“å…¥ï¼š"ä½ å¥½" â†’ 2-3ä¸ªtoken
è¾“å‡ºï¼š"ä½ å¥½ï¼æœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ" â†’ 10-15ä¸ªtoken

å¤æ‚å¯¹è¯ï¼š
è¾“å…¥ï¼š"è¯·è¯¦ç»†åˆ†æè¿™ä¸ªæŠ€æœ¯æ–¹æ¡ˆçš„ä¼˜ç¼ºç‚¹..." â†’ 50-100ä¸ªtoken
è¾“å‡ºï¼šè¯¦ç»†åˆ†æå›å¤ â†’ 500-1000ä¸ªtoken

ä»£ç ç”Ÿæˆï¼š
è¾“å…¥ï¼š"å†™ä¸€ä¸ªå®Œæ•´çš„ç”¨æˆ·ç®¡ç†ç³»ç»Ÿ..." â†’ 20-50ä¸ªtoken
è¾“å‡ºï¼šå®Œæ•´ä»£ç  + è§£é‡Š â†’ 2000-5000ä¸ªtokenï¼
```

å…³é”®æ´å¯Ÿï¼šè¾“å‡ºtokenå¾€å¾€æ¯”è¾“å…¥tokenå¤šå¾—å¤šï¼Œè¿™æ˜¯æˆæœ¬çš„ä¸»è¦æ¥æºï¼

#### ğŸ’¡ å®é™…æˆæœ¬è®¡ç®—ç¤ºä¾‹

ä»¥æˆ‘ä»¬çš„æµ‹è¯•å¥å­ä¸ºä¾‹ï¼š
```python
å¥å­ï¼š"ä»Šå¤©åŒ—äº¬çš„å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ"

ä½¿ç”¨GPT-4 Turboï¼š
è¾“å…¥Tokenï¼š60ä¸ª â†’ 60 Ã— $0.01/1000 = $0.0006
è¾“å‡ºTokenï¼šå‡è®¾å›å¤50ä¸ªtoken â†’ 50 Ã— $0.03/1000 = $0.0015
æ€»æˆæœ¬ï¼š$0.0006 + $0.0015 = $0.0021ï¼ˆçº¦0.015å…ƒï¼‰

ä½¿ç”¨Gemini Proï¼š
è¾“å…¥Tokenï¼šçº¦15ä¸ª â†’ 15 Ã— $0.00025/1000 = $0.00000375
è¾“å‡ºTokenï¼šå‡è®¾å›å¤30ä¸ªtoken â†’ 30 Ã— $0.0005/1000 = $0.000015
æ€»æˆæœ¬ï¼š$0.00000375 + $0.000015 = $0.00001875ï¼ˆçº¦0.00013å…ƒï¼‰

çœŸå®æˆæœ¬å·®å¼‚ï¼šGPT-4æ¯”Geminiè´µä¸å°‘ï¼
```

### ğŸ¯ å¯¹ç”¨æˆ·çš„å¯ç¤º

#### 1. é€‰æ‹©åˆé€‚çš„æ¨¡å‹ï¼š
- ç®€å•ä»»åŠ¡ç”¨é«˜æ•ˆæ¨¡å‹ï¼ˆå¦‚Geminiï¼‰
- å¤æ‚ä»»åŠ¡æ‰ç”¨å¼ºå¤§æ¨¡å‹ï¼ˆå¦‚GPT-4ï¼‰

#### 2. ä¼˜åŒ–æç¤ºè¯å’Œè¾“å‡ºï¼š
- ç®€æ´æ˜ç¡®ï¼Œé¿å…å†—ä½™çš„è¾“å…¥
- æ§åˆ¶è¾“å‡ºé•¿åº¦ï¼šè¦æ±‚"ç®€è¦å›ç­”"è€Œä¸æ˜¯"è¯¦ç»†åˆ†æ"
- å‡å°‘ä¸å¿…è¦çš„å¯¹è¯è½®æ¬¡

#### 3. ç†è§£å®Œæ•´è®¡è´¹é€»è¾‘ï¼š
- è¾“å…¥å’Œè¾“å‡ºéƒ½è®¡è´¹ï¼šå¾ˆå¤šäººå¿½ç•¥äº†è¾“å‡ºtokençš„è´¹ç”¨
- è¾“å‡ºæ›´è´µï¼šè¾“å‡ºtokenä»·æ ¼é€šå¸¸æ˜¯è¾“å…¥tokençš„2-3å€
- é•¿å›å¤æˆæœ¬é«˜ï¼šLLMå›å¤è¶Šè¯¦ç»†ï¼Œè¾“å‡ºtokenè¶Šå¤šï¼Œæˆæœ¬è¶Šé«˜
- å¯¹è¯å†å²ç´¯ç§¯ï¼šæ¯è½®å¯¹è¯éƒ½åŒ…å«ä¹‹å‰çš„å†å²
- ç‰¹æ®Šæ ¼å¼å¼€é”€ï¼šGPTç³»åˆ—çš„å¯¹è¯æ ¼å¼å ç”¨å¤§é‡token

## ğŸ¯ æ€»ç»“ï¼šåˆ†è¯çš„é‡è¦æ€§

åˆ†è¯çœ‹ä¼¼ç®€å•ï¼Œä½†å®ƒæ˜¯LLMç†è§£çš„åŸºçŸ³ã€‚ç†è§£åˆ†è¯æœºåˆ¶ï¼Œèƒ½å¸®åŠ©æˆ‘ä»¬æ›´å¥½åœ°é€‰æ‹©å’Œä½¿ç”¨LLMï¼
### âœ… å…³é”®è¦ç‚¹
1. æ•ˆç‡å·®å¼‚å·¨å¤§ï¼šä¸åŒæ¨¡å‹çš„tokenæ¶ˆè€—å¯èƒ½ç›¸å·®73å€ï¼
2. æˆæœ¬ç›´æ¥ç›¸å…³ï¼štokenæ•°é‡ç›´æ¥å½±å“è®¡ç®—æˆæœ¬å’Œå“åº”é€Ÿåº¦
3. æ¨¡å‹é€‰æ‹©é‡è¦ï¼šæ ¹æ®ä»»åŠ¡ç±»å‹é€‰æ‹©åˆé€‚çš„åˆ†è¯ç®—æ³•
4. ç‰¹æ®Štokenä½œç”¨ï¼šç†è§£å¯¹è¯æ ¼å¼å’Œæ§åˆ¶ç¬¦çš„å¼€é”€

### ğŸš€ å®è·µå»ºè®®
- ä¸­æ–‡ä»»åŠ¡ï¼šä¼˜å…ˆé€‰æ‹©Gemmaç­‰ä¸­æ–‡ä¼˜åŒ–æ¨¡å‹
- ä»£ç ä»»åŠ¡ï¼šé€‰æ‹©CodeLlamaç­‰ä»£ç ä¸“ç”¨æ¨¡å‹
- è‹±æ–‡ä»»åŠ¡ï¼šGPTç³»åˆ—åœ¨è‹±æ–‡ä¸Šè¡¨ç°ä¼˜ç§€
- æˆæœ¬æ•æ„Ÿï¼šåœ¨åŠŸèƒ½æ»¡è¶³çš„å‰æä¸‹ï¼Œä¼˜å…ˆé€‰æ‹©tokenæ•ˆç‡é«˜çš„æ¨¡å‹

ç†è§£Tokenæœºåˆ¶ä¸ä»…å¸®åŠ©æˆ‘ä»¬ç†è§£LLMå·¥ä½œåŸç†ï¼Œæ›´èƒ½å¸®åŠ©æˆ‘ä»¬æ˜æ™ºåœ°ä½¿ç”¨å’Œé€‰æ‹©LLMæœåŠ¡ï¼

----- English

# Tokenization: How LLM Actually Reads Your Text

Ever wonder why ChatGPT costs more than other LLM models for the same question? The answer lies in tokenization - how Large Language Models (LLMs) break down your text.

## ğŸ¯ What You Need to Know First

Three key concepts that matter:

- AI (Artificial Intelligence): The broad field of making machines smart
- NLP (Natural Language Processing): AI that handles human language
- LLM (Large Language Model): Powerful LLM systems like GPT, Claude, and Gemini

The hierarchy: AI â†’ NLP â†’ LLM. Tokenization is how all these systems first process your text.

## ğŸ¯ Tokenization Explained Simply

Tokenization = Breaking your text into chunks that LLM can process.

Here's what happens:
- You type: "You are a helpful assistant"
- LLM sees: `["You", "are", "a", "helpful", "assistant"]`
- Result: 5 separate tokens, each with meaning

But what about more complex text?

Here's where tokenization gets interesting with real English:

```python
# Example: "Don't worry, it's working!"

# âŒ Poor tokenization (breaks contractions)
["Don", "'", "t", "worry", ",", "it", "'", "s", "working", "!"]
# Splits contractions awkwardly, harder for LLM to understand

# âœ… Smart tokenization (preserves meaning)
["Don't", "worry", ",", "it's", "working", "!"]
# Keeps contractions as complete units
```

How smart tokenization works:
1. Vocabulary matching: LLM recognizes "Don't" as one unit, not "Don" + "'" + "t"
2. Context awareness: Contractions stay together because they're common patterns
3. Frequency analysis: "it's" appears millions of times in training, so it's one token
4. Semantic boundaries: Punctuation gets separate tokens when it changes meaning

Bottom line: Good tokenization = better LLM understanding = lower costs.

## ğŸ” Why Tokenization Matters

Simple reason: Computers can't read English - they need numbers.

What tokenization does:
- Converts text â†’ numbers LLM can process
- Creates consistent format for calculations
- Preserves meaning in each chunk
- Directly impacts speed and cost (fewer tokens = faster + cheaper)
- Enables multilingual support

## ğŸš€ The Shocking Truth: Same Text, Wildly Different Costs

We tested "Dont worry, it's working!" across different LLM models. The results will surprise you:

```python
# Input: Dont worry, it's working! (5 English words)

# Google Gemma-7b (Best performance)
gemma_tokens = 9
token_ids = [2, 52094, 12067, 235269, 665, 235303, 235256, 3434, 235341]
efficiency = 9/5 = 1.8 tokens/word

# CodeLlama (Medium efficiency)
codellama_tokens = 9
token_ids = [360, 609, 15982, 29892, 372, 29915, 29879, 1985, 29991]
efficiency = 9/5 = 1.8 tokens/word

# GPT-3.5-turbo (Lower efficiency)
gpt35_tokens = 21  # Including special tokens for dialogue format
token_ids = [100264, 9125, 198, 35, 546, 11196, 11, 433, 596, 3318, 0, 100265, 198, 100264, 882, 198, 100265, 198, 100264, 78191, 198]
efficiency = 21/5 = 4.2 tokens/word

# GPT-4 (Lowest efficiency)
gpt4_tokens = 55  # Dialogue format + more complex tokenization
token_ids = [27, 91, 318, 5011, 91, 29, 9125, 27, 91, 318, 55875, 91, 29, 35, 546, 11196, 11, 433, 596, 3318, 88032, 91, 318, 6345, 91, 1822, 91, 318, 5011, 91, 29, 882, 27, 91, 318, 55875, 91, 1822, 91, 318, 6345, 91, 1822, 91, 318, 5011, 91, 29, 78191, 27, 91, 318, 55875, 91, 29]
efficiency = 55/5 = 11.0 tokens/word

# Performance gap comparison
Best vs Worst = 55/9 â‰ˆ 6.1x efficiency gap!
Computational complexity gap = (55Â²)/(9Â²) = 3025/81 â‰ˆ 37.3x!!!
```

Why the huge difference?

Each Model uses different tokenization methods:
| Model | Token Count | Tokenization Algorithm | Features | English Processing |
|-------|-------------|------------------------|----------|-------------------|
| Gemma-7b | 9 | Optimized SentencePiece | Google specially optimized | Excellent |
| CodeLlama | 9 | BPE variant | Optimized for code | Good |
| GPT-3.5-turbo | 21 | BPE + dialogue format | English + dialogue optimized | Moderate |
| GPT-4 | 55 | Complex BPE + dialogue format | Powerful but huge token consumption | Variable |

The real-world impact:

```python
# Computational complexity comparison (self-attention mechanism âˆ tokenÂ²)
Gemma processing cost = 9Â² = 81 units
CodeLlama processing cost = 9Â² = 81 units
GPT-3.5 processing cost = 21Â² = 441 units
GPT-4 processing cost = 55Â² = 3,025 units

# Large-scale application cost differences (1M requests per day)
- Gemma cost: 81M units
- CodeLlama cost: 81M units (same as Gemma)
- GPT-3.5 cost: 441M units (5.4x more expensive than Gemma)
- GPT-4 cost: 3,025M units (37.3x more expensive than Gemma!!!)

# Theoretical computational complexity differences (same hardware conditions)
Gemma computational complexity: 1.0x (baseline)
CodeLlama computational complexity: 1.0x (same as Gemma)
GPT-3.5 computational complexity: 5.4x
GPT-4 computational complexity: 37.3x

Important: These are computational complexity ratios, not actual response times. Real API calls usually take 1-3 seconds regardless of model.

Key takeaway: Choose the right model and save up to 37x on computational costs annually.
```

## Why GPT Models Are Token Hogs

### GPT-3.5-turbo (21 tokens)
From the data, we can see that GPT-3.5's 21 tokens include:
```
<|im_start|>system
Dont worry, it's working!<|im_end|>
<|im_start|>user
<|im_end|>
<|im_start|>assistant
```

### GPT-4 (55 tokens!)
GPT-4 is even more extreme, requiring 55 tokens for the same sentence! From the data:
```
<|im_start|>system<|im_sep|>Dont worry, it's working!<|im_end|><|im_start|>user<|im_sep|><|im_end|><|im_start|>assistant<|im_sep|>
```

Why GPT uses so many tokens:
1. Complex separators: Extra control tokens like `<|im_sep|>`
2. Fine-grained splitting: Every punctuation mark = separate token
3. Format overhead: 80%+ of tokens are just formatting, not your actual content
4. Feature creep: More capabilities = more token waste

## ğŸ¯ The Three Main Tokenization Methods

Choose your fighter - each method has trade-offs:

### 1. SentencePiece (Google's approach)
- Models: Gemma, T5, PaLM
- Strength: Efficient, multilingual-friendly
- Best for: Non-English languages, cost optimization

### 2. BPE (Byte Pair Encoding) (OpenAI's choice)
- Models: GPT series, CodeLlama
- Strength: Handles rare words well
- Best for: English text, code generation

### 3. WordPiece (Google BERT)
- Models: BERT, RoBERTa
- Strength: Balanced performance
- Best for: Text analysis, understanding tasks

## ğŸ”§ Special Tokens: The Hidden Cost

Beyond your actual text, LLM models add control tokens that you pay for:

### Common Special Token Types

#### BERT Series Special Tokens
```python
tokens = ["[CLS]", "Dont worry, it's working!", "[SEP]"]
```
- [CLS]: Classification token, sentence start marker, used for classification tasks
- [SEP]: Separator token, sentence separator, used to separate different sentences
- [PAD]: Padding token, padding marker, used to align sequence lengths
- [UNK]: Unknown token, unknown word marker, handles words outside vocabulary

#### GPT Series Special Tokens
```python
tokens = ["<|im_start|>system
Dont worry, it's working!<|im_end|>
<|im_start|>user
<|im_end|>
<|im_start|>assistant"]
```
- <|im_start|>: Dialogue start marker
- <|im_end|>: Dialogue end marker
- <|im_sep|>: Dialogue separator
- system/user/assistant: Role identifiers

Why does GPT waste tokens on formatting?

The brutal truth:
- Legacy design: GPT started as a text generator, chat was bolted on later
- Feature bloat: More capabilities = more control tokens = higher costs
- Business model: OpenAI passes the inefficiency cost to you
- Market opportunity: Efficient models like Gemma capitalize on this waste

Result: Same text, GPT-4 uses 6x more tokens than efficient models like Gemma.

## ğŸ’¡ Real-World Examples

### 1. Dialogue Systems
```python
User input: "Help me check tomorrow's weather"
Tokenization result: ["Help", "me", "check", "tomorrow's", "weather"]
LLM understanding: User wants to query weather information
LLM operation: Call MCP and other services to get real-time information, generate output
```

> MCP (Model Context Protocol): A protocol that allows LLMs to safely access external data and services, such as real-time weather, database queries, etc.

### 2. Code Generation
```python
User input: "Write a Python function to calculate Fibonacci sequence"
Tokenization result: ["Write", "a", "Python", "function", "to", "calculate", "Fibonacci", "sequence"]
LLM understanding: Need to generate Python code
```

### 3. Multilingual Translation
```python
User input: "Hello, how are you?"
Tokenization result: ["Hello", ",", "how", "are", "you", "?"]
LLM understanding: English greeting, needs translation to target language
```

## ğŸ’° Why LLM Companies Charge Per Token

Simple answer: Tokens = computational cost. More tokens = more processing power needed.

### ğŸ¤” The Economics Behind Token Pricing
```python
1. Directly Related to Computational Cost
# Computational complexity âˆ tokenÂ²
[GEMMA_COUNT] tokens computational cost = [RESULT] units
[GPT4_COUNT] tokens computational cost = [RESULT] units ([RATIO]x difference!)

# Token-based pricing is fairest
User A: [GEMMA_COUNT] tokens â†’ pays for [RESULT] units of cost
User B: [GPT4_COUNT] tokens â†’ pays for [RESULT] units of cost
```

2. Predictable Resource Consumption
- Memory usage: Each token requires fixed vector storage space
- GPU computation: Token count determines matrix operation scale
- Network transmission: Tokens directly affect data transmission volume

3. Huge User Behavior Differences
# Different users' token consumption differences (input + output)
Simple Q&A:
Input: "Hello" â†’ 1-2 tokens
Output: "Hello! How can I help you?" â†’ 8-10 tokens

Complex dialogue:
Input: "Please analyze the pros and cons of this technical solution..." â†’ 15-25 tokens
Output: Detailed analysis response â†’ 200-500 tokens

Code generation:
Input: "Write a complete user management system..." â†’ 8-15 tokens
Output: Complete code + explanation â†’ 1000-3000 tokens!
```

Critical insight: LLM responses are usually longer than your questions - that's where costs explode!

### ğŸ’¡ Actual Cost Calculation Example

Using our test sentence as an example:
```python
Sentence: "Dont worry, it's working!"

Using GPT-4 Turbo:
Input Tokens: 55 â†’ 55 Ã— $0.01/1000 = $0.00055
Output Tokens: Assume 20 token reply â†’ 20 Ã— $0.03/1000 = $0.0006
Total cost: $0.00115

Using Gemma (via API):
Input Tokens: 9 â†’ 9 Ã— $0.00025/1000 = $0.00000225
Output Tokens: Assume 15 token reply â†’ 15 Ã— $0.0005/1000 = $0.0000075
Total cost: $0.00000975

Real cost difference: GPT-4 is 118x more expensive than Gemma!
```

### Understanding Complete Billing Logic:
- Input and output both charged: Many people ignore output token costs
- Output more expensive: Output token prices are usually 2-3x input tokens
- Long replies cost more: The more detailed the LLM reply, the more output tokens, the higher the cost
- Dialogue history accumulation: Each dialogue round includes previous history
- Special format overhead: GPT series dialogue format consumes many tokens

## ğŸ¯ Bottom Line: Why This Matters

Tokenization isn't just technical trivia - it directly impacts your LLM costs and performance.

### âœ… What You Need to Remember
1. Token count = your bill: Different models can cost 37-118x more for identical tasks
2. Output costs more: LLM responses are usually 2-3x more expensive than your input
3. Model choice matters: Pick the right tool for the job
4. Hidden costs exist: Special formatting tokens add up fast

### ğŸš€ Action Items
- For English: GPT models work great (but cost more)
- For code: Use CodeLlama or similar specialized models
- For other languages: Try Gemma or multilingual-optimized models
- For budgets: Always test cheaper alternatives first

The takeaway: Understanding tokenization saves money and improves performance. Choose wisely.
