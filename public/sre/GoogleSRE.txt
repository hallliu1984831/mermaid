章节1:SRE谷歌运维解密事后总结之避免指责
大家好，今天来看看 Google 运维解密的新篇章分享：避免指责，对事不对人。

1️⃣ 原文翻译：
指责
"我们需要重写整个复杂的后端系统！过去三个季度它每周都在出问题，我们厌倦了一个一个地修复这些问题。说真的，如果我再被告警吵醒一次，我就自己重写它..."

对事不对人
"重写整个后端系统的行动项目实际上可能会防止这些烦人的告警信息继续发生，目前版本的维护手册非常冗长，学习成本很高，我相信我们未来的值班人员会感谢我们的！"

最佳实践：避免指责，提供建设性意见
对事不对人的事后复盘可能很难写，因为复盘格式会清楚地识别出导致事故的行为。从复盘中消除指责能让人们有信心上报问题而不用担心被责备。同样重要的是，不要因为某个人或团队频繁产生复盘而对其污名化。指责的氛围可能会创造一种文化，在这种文化中事故和问题被掩盖起来，从而给组织带来更大的风险。

2️⃣ 理论 VS 现实
理想很丰满，现实很骨感。Google SRE的无责备文化在国内还是少数，大部分公司仍然是"出了事先找人，找到人先处罚"的传统思维。可能的处理步骤大致如下
👉 杀鸡儆猴：处罚典型，震慑他人
👉 息事宁人：找个替罪羊，快速平息
👉 推卸责任：向上级汇报时淡化自己责任
👉 短期思维：只关心这次故障，不关心长期改进

作为技术人，我们既要学习先进理念，也要在现实环境中保护好自己！
👉 重要操作必须有邮件确认
👉 风险提前预警并留下记录
👉 建立技术文档保护自己
👉 选择技术文化好的公司

各位小红薯有遇到过啥不平事，欢迎来分享下 💗

章节2:SRE谷歌运维解密事后总结之协作分享
大家好，今天来看看 Google 运维解密的新篇章分享：协作与知识分享，从这篇开始，我试着避开直接翻译，用更轻松的语言来介绍。

我们重视协作，事后复盘流程当然也不例外。毕竟，一个人闷头写复盘就像一个人吃火锅——技术上可行，但总感觉少了点什么。
我们的事后复盘文档使用Google Docs，配合内部模板。不管你用什么工具，都要关注以下关键特性：
实时协作
便于快速收集数据和想法。想象一下，十几个工程师同时在线编辑同一个文档，就像一群程序员在同一个键盘上疯狂敲代码——听起来很混乱，但实际上效率惊人。
开放的评论系统
让众包解决方案变得简单。这就像给文档开了个弹幕功能，大家可以随时吐槽："这里的根因分析有点浅啊"或者"我觉得这个操作应该优先级更高"。
邮件通知
可以定向发送给协作者，或者拉其他人进来提供意见。拉个群，聊起来！

正式审查与发布：不是写完就完事
写事后复盘还涉及正式审查和发布。团队会在内部分享第一版草稿，并邀请资深工程师评估完整性。审查标准包括：
关键事件数据是否已收集？
影响评估是否完整？（别只说"有点影响"，要给数据、图标）
根因分析是否足够深入？（不要停留在"代码有bug"这种表面现象）
行动计划是否合适？
是否与相关方分享了结果？（别让其他团队踩同样的坑）
初步审查完成后，事后复盘会更广泛地分享给工程团队或内部邮件列表。我们的目标是分享给尽可能广泛的受众，让他们从中获得知识或经验教训。

需要注意的是，Google对用户信息有严格规定，即使内部文档也绝不包含可识别用户的信息。毕竟，我们要保护用户隐私，不能让事后复盘变成"社死现场复盘"。

最佳实践：没有审查的事后复盘等于没写
一个未经审查的事后复盘就像一个没人看的朋友圈——存在的意义令人怀疑。为确保每个草稿都得到审查，我们鼓励定期举行审查会议，结束讨论、捕捉想法、最终确定状态。

相关人员满意后，事后复盘会被添加到团队的过往事件存储库中。透明分享让其他人更容易找到并学习。
想象一下，这就像建立了一个事故的慢镜头回放，每一帧都详细记录了当时的惨状和改进措施。再有类似问题发生时，直接复盘："这个坑之前有人踩过了，还好有详细的避坑指南"

这样的协作文化不仅让事后复盘写得更好，还让整个团队的技术水平螺旋上升。一个人的踩坑经验，经过精心包装后，就变成了全团队的避坑宝典。


章节3:SRE谷歌运维解密：处理连锁故障之CPU耗尽
大家好，今天来看看 Google 运维解密的新篇章分享
资源耗尽
某一种资源的耗尽可以导致高延迟、高错误率，或者导致低质量响应。这些确实是资源耗尽时的预期表现:在负载不断上升到过载时，服务器无法始终保持完全正常的状态。
取决于究竟哪种资源最终耗尽，和软件服务器的构建方法，该情况可能会导致系统以低效率运行，甚至崩溃。负载均衡系统进而将请求转发给其他服务器，有可能导致整个集群请求处理成功率的下降，甚至使整个集群或者整个服务进入连锁故障模式。
不同种类的资源耗尽会对软件服务器产生不同的影响。
CPU
如果 CPU 资源不足以应对请求负载，一般来说所有的请求都会变慢。这个场景会造成一系列的副作用，包括如下几项。
正在处理的(in-flight)请求数量上升
因为处理请求需要较长的时间，同一时间服务器必须同时处理更多的请求(达到一定数量后可能开始进入队列排队)。这会影响其他所有的资源，包括内存、活跃线程数(在每个请求一个线程的编程模型下)、文件描述符，和后端服务器的资源(该资源的耗尽可能会带来其他连锁问题)。
队列过长
如果没有足够的资源以稳定的速度处理所有请求，服务器会逐渐将请求队列填满。这意味着延迟上升(因为所有请求都要排队一段时间),同时队列会使用更多的内存。
线程卡住
如果一个线程由于等待某个锁而无法处理请求，可能服务器无法在合理的时间内处理健康检查请求(Borg 系统会将这种情况视为服务器已经失败，从而杀掉它)。
CPU死锁或者请求卡住
服务器内置的看门狗机制(watchdog)可能会检测到服务器无法进行工作,导致软件服务器最终由于CPU资源不够而崩溃。如果看门狗机制是远端触发的，但是由于请求队列排队，这些请求无法被及时处理，而触发看门狗机制杀掉进程
RPC超时
服务器过载时，对客户端 RPC的回复会变慢，最终会超过客户端所设置的超时时间。这会导致服务器对请求实际进行的处理都被浪费了，而客户端可能会重试RPC，造成更严重的过载。
CPU 缓存效率下降
CPU 使用得越多，任务被分配到多个 CPU 核心上的几率越大，从而导致 CPU核心本地缓存的失效，进而降低 CPU 处理的效率。

章节4:SRE谷歌运维解密：处理连锁故障之其他资源耗尽
大家好，之前章节的谷歌运维解密介绍了CPU耗尽的场景，今天来聊聊其他资源耗尽的情况。
内存耗尽 💾：
就算没有其他副作用，同时处理的请求数量升高也会消耗更多的内存用于存放请求、回复以及RPC对象。内存耗尽可能导致如下情况的发生。
任务崩溃
例如，某任务可能会因为超过资源限制而被容器管理器驱逐（VM或者其他），或者程序自身逻辑会触发崩溃。
Java垃圾回收(GC)速率加快，从而导致CPU使用率上升
一个糟糕透顶的场景：由于CPU资源减少，请求处理速度变慢、内存使用率上升导致GC触发次数增多，导致CPU资源的进一步减少。我们将此称为“GC死亡螺旋”。
缓存命中率下降
可用内存的减少可能会导致应用层缓存的命中率降低，导致向后端发送更多的RPC，可能会导致后端任务过载。
线程不足 🧵
线程不足（如超过系统默认的1024个线程限制）可能会导致错误或者导致健康检查失败、如果服务器为此增加更多线程、这些线程可能会占用更多内存。在极端情况下，线程不足可能会导致进程ID不足(Linux的PID数量有上限)。
文件描述符耗尽 📁
文件描述符(file descriptor)不足可能会导致无法建立网络连接,进而导致健康检查失败。

资源之间的相互依赖
注意，很多资源的耗尽都会导致其他资源出现问题--某个服务过载经常会出现一系列次级现象看起来很像根本问题，这会使定位问题更困难。
建议：
**连锁故障场景分析：**
1️⃣ Java前端服务器GC参数未调优
2️⃣ 高负载下GC问题导致CPU不足
3️⃣ CPU不足 → 请求处理变慢
4️⃣ 并发请求增多 → 内存使用上升
5️⃣ 内存压力 → 缓存内存减少
6️⃣ 缓存减少 → 命中率下降
7️⃣ 命中率下降 → 后端请求增多
8️⃣ 后端CPU/线程不足
9️⃣ 健康检查失败 → 连锁故障触发
在上述这个复杂情景下，发生故障时可能没有时间仔细分析因果关系。尤其是在前端和后端有不同团队运维时，判断后端崩溃是由于前端缓存命中率下降可能非常困难。