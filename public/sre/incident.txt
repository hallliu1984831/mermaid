ç« èŠ‚1:
æ¢ç´¢SREï¼šè¶…æ—¶ timeout è¿™ä¸ªâ€œå®šæ—¶ç‚¸å¼¹â€
å¤§å®¶å¥½ï¼Œä»Šå¤©æ¥èŠèŠ SRE æ—¥å¸¸å·¥ä½œä¸­ç»å¸¸ç¢°åˆ°çš„åº”ç”¨/æœåŠ¡è¶…æ—¶ï¼štimeout ğŸ˜‚
 æ‰€æœ‰çš„åº”ç”¨è½¯ä»¶ï¼Œä¸ç®¡æ˜¯å®¢æˆ·ç«¯è½¯ä»¶è¿˜æ˜¯æœåŠ¡ç«¯è½¯ä»¶ï¼Œéƒ½æœ‰è¶…æ—¶è®¾ç½®ã€‚è¿™ä¹ˆåšçš„æ ¸å¿ƒç›®çš„æ˜¯ä¸ºäº†é˜²æ­¢ç­‰ï¼Œç­‰ç¯ç­‰ç¯ ğŸ‘‰ğŸ‘‰ "ç­‰åˆ°å¤©è’åœ°è€"ã€‚å¦‚å›¾2ï¼Œæ¯ä¸€æ¬¡å®¢æˆ·ç«¯å‘èµ·çš„è¯·æ±‚éƒ½åº”è¯¥åˆç†èŒƒå›´å†…å¾—åˆ°å›å¤ï¼Œå¦‚æœæ²¡æœ‰å¾—åˆ°æ­£å¸¸çš„å›å¤ï¼Œé‚£å°±æœ‰å¯èƒ½æ˜¯è¶…æ—¶è¿™ä¸ªå®¶ä¼™å‡ºç°äº†ã€‚
	
1ï¸âƒ£ ä¸ºä»€ä¹ˆéœ€è¦ timeout é…ç½®ï¼Ÿ
 èµ„æºä¿æŠ¤ï¼šå°±åƒç»™æ¯ä¸ªè¯·æ±‚è®¾ä¸ª"åˆ°æœŸæ—¶é—´"ï¼Œé˜²æ­¢çº¿ç¨‹/è¿æ¥è¢«æ— é™å ç”¨ï¼Œé¿å…å†…å­˜æ³„æ¼å’Œèµ„æºè€—å°½ã€‚
 ç”¨æˆ·ä½“éªŒï¼š ç”¨æˆ·ä¸ä¼šæ— é™ç­‰å¾…ï¼Œä½ æˆ‘éƒ½æ²¡é‚£ä¹ˆä¹…è€å¿ƒï¼›å¿«é€Ÿå¤±è´¥æ€»å¥½è¿‡æ…¢æ…¢ç­‰æ­»ï¼›åŒæ—¶ç»™ç”¨æˆ·æ˜ç¡®çš„åé¦ˆï¼š"å‡ºé”™äº†ï¼Œä¸æ˜¯å¡ä½äº†"
 ç³»ç»Ÿç¨³å®šæ€§ï¼š é˜²æ­¢é›ªå´©æ•ˆåº”ï¼šä¸€ä¸ªæ…¢è¯·æ±‚ä¹Ÿå¯èƒ½æ‹–å®æ•´ä¸ªç³»ç»Ÿï¼›éš”ç¦»æ•…éšœï¼šåæ‰çš„æœåŠ¡ä¸å½±å“å…¶ä»–æœåŠ¡ï¼›å¯é¢„æµ‹çš„è¡Œä¸ºï¼šçŸ¥é“æœ€åæƒ…å†µä¸‹å¤šä¹…ä¼šç»“æŸã€‚
 è¿ç»´å¯æ§ï¼šä¾¿äºç›‘æ§å’Œå‘Šè­¦ï¼›å¸®åŠ©å®šä½æ€§èƒ½ç“¶é¢ˆï¼›æ”¯æŒè‡ªåŠ¨æ¢å¤æœºåˆ¶ã€‚
	
2ï¸âƒ£ å®¢æˆ·ç«¯è¶…æ—¶ï¼šæˆ‘ç­‰ä¸ä¸‹å»äº†ï¼ğŸ˜¡
å½“å®¢æˆ·ç«¯ç­‰å¾…æœåŠ¡ç«¯å“åº”è¶…è¿‡ timeout æ—¶é—´ï¼Œä¼šå‘ç”Ÿä»€ä¹ˆï¼š
 ç›´æ¥æ”¾å¼ƒï¼š"ç®—äº†ç®—äº†ï¼ŒåŠ³èµ„ä¸ç­‰äº†ï¼"
 æŠ›å¼‚å¸¸ï¼šTimeoutExceptionã€SocketTimeoutException ç­‰å„ç§"æˆ‘ç”Ÿæ°”äº†"çš„æ¶ˆæ¯
 é‡è¯•æœºåˆ¶ï¼šæœ‰äº›å®¢æˆ·ç«¯ä¼šè¯´ï¼šå†ç»™ä½ ä¸€æ¬¡æœºä¼š
 ç”¨æˆ·ä½“éªŒç‚¸è£‚ï¼šç”¨æˆ·çœ‹åˆ°è½¬åœˆåœˆæˆ–è€…æŠ¥é”™é¡µé¢
	
 å®¢æˆ·ç«¯timeout = ç”¨æˆ·çš„è€å¿ƒå€¼
è®¾ç½®å¤ªçŸ­ â†’ ç”¨æˆ·ï¼š"è¿™ç ´è½¯ä»¶ï¼"
è®¾ç½®å¤ªé•¿ â†’ ç”¨æˆ·ï¼š"è¿™ç ´ç½‘ï¼"
	
3ï¸âƒ£ æœåŠ¡ç«¯è¶…æ—¶ï¼šæˆ‘å¿™ä¸è¿‡æ¥äº†ï¼ğŸ¤”
æœåŠ¡ç«¯å¤„ç†è¯·æ±‚è¶…è¿‡timeoutæ—¶é—´ä¼šå‘ç”Ÿä»€ä¹ˆï¼š
 å¼ºåˆ¶ä¸­æ–­ï¼šæ­£åœ¨æ‰§è¡Œçš„æ“ä½œè¢«"å’”åš“"æ‰
 èµ„æºå›æ”¶ï¼šè¿æ¥ã€çº¿ç¨‹ã€å†…å­˜ç­‰èµ„æºè¢«é‡Šæ”¾
 è¿”å›é”™è¯¯ï¼šé€šå¸¸æ˜¯504 Gateway Timeout æˆ– 500 Internal Server Error
 çº§è”æ•…éšœï¼šä¸€ä¸ªæ…¢è¯·æ±‚å¯èƒ½æ‹–å®æ•´ä¸ªæœåŠ¡
	
 æœåŠ¡ç«¯timeout = æœåŠ¡çš„è‡ªæˆ‘ä¿æŠ¤æœºåˆ¶
æ²¡æœ‰timeout â†’ æœåŠ¡è¢«æ…¢è¯·æ±‚æ‹–æ­»
timeoutå¤ªçŸ­ â†’ æ­£å¸¸è¯·æ±‚ä¹Ÿè¢«è¯¯æ€
timeoutå¤ªé•¿ â†’ èµ„æºè¢«é•¿æœŸå ç”¨

å¦‚å›¾3ï¼ŒSREçš„å·¥ä½œä¹‹ä¸€ï¼Œå°±æ˜¯ç¡®ä¿è¿™äº›è¶…æ—¶è®¾ç½®å¾—â€œæ°åˆ°å¥½å¤„â€å¹¶æœ‰æ•ˆç›‘æ§ï¼Œæ—¢ä¸å¤ªæ•æ„Ÿï¼ˆåŠ¨ä¸åŠ¨å°±è¶…æ—¶ï¼‰ï¼Œä¹Ÿä¸å¤ªè¿Ÿé’ï¼ˆç­‰é»„èŠ±èœéƒ½å‡‰äº†æ‰ååº”è¿‡æ¥ï¼‰ã€‚

----- English
Exploring SRE: Timeout - The "Time Bomb" â°

Hello everyone! Today let's talk about application/service timeouts that SRE folks deal with every day ğŸ˜‚

All applications, whether client-side or server-side, has timeout settings. The core purpose is to prevent endless waiting -- "waiting until your coffee gets cold." As shown in Figure 2, every client request should receive a response within a reasonable timeframe. If no normal response comes back, timeout is likely the culprit.

1ï¸âƒ£ Why do we need timeout configuration?

Resource Protection: Like setting an "expiration date" for each request, preventing threads/connections from being tied up indefinitely, avoiding memory leaks and resource exhaustion.

User Experience: Users won't wait forever - nobody has that kind of patience; failing fast beats a slow death; gives users clear feedback: "Something's broken, but we're not frozen."

System Stability: Prevents cascade failures: one sluggish request shouldn't bring down the entire system; isolates problems: broken services don't drag down healthy ones; predictable behavior: you know the worst-case scenario timing.

Operational Control: Enables effective monitoring and alerting; helps pinpoint performance bottlenecks; supports automated recovery mechanisms.

2ï¸âƒ£ Client-side Timeout: I'm done waiting! ğŸ˜¡

When the client waits beyond the timeout period for a server response:

Gives up: "Screw this, I'm out!"
Throws exceptions: TimeoutException, SocketTimeoutException, and various "I'm fed up" errors
Retry logic: Some clients think: "Let me try that one more time"
User experience disaster: Users see endless spinners or error pages
Client timeout = User's patience threshold

Too short â†’ User: "This app sucks!"
Too long â†’ User: "This connection sucks!"
3ï¸âƒ£ Server-side Timeout: I'm swamped! ğŸ¤”

When server processing exceeds the timeout:

Hard stop: In-flight operations get terminated abruptly
Resource cleanup: Connections, threads, memory get freed up
Error responses: Typically 504 Gateway Timeout or 500 Internal Server Error
Domino effect: One slow request can tank the entire service
Server timeout = Service's survival instinct

No timeout â†’ Service dies from slow request overload
Too aggressive â†’ Healthy requests get wrongly terminated
Too lenient â†’ Resources stay tied up too long
As shown in Figure 3, part of an SRE's job is ensuring these timeout settings hit the sweet spot and are properly monitored - not hair-trigger sensitive (timing out at every hiccup) nor completely oblivious (only reacting when it's way too late).

ç« èŠ‚2:
----- Chinese
ç”Ÿäº§äº‹æ•…å¤ç›˜ï¼šCLOSE_WAITå¤§å†›çš„é€†è¢­ ğŸ’€

å¤§å®¶å¥½ï¼Œä¹‹å‰æˆ‘ä»‹ç»äº†timeoutè¿™ä¸ªå®šæ—¶ç‚¸å¼¹ï¼Œä»Šå¤©æ¥åˆ†äº«ä¸€ä¸ªç›¸å…³çš„ç”Ÿäº§äº‹æ•…ã€‚

2025å¹´æŸä¸ªé£å’Œæ—¥ä¸½çš„å‘¨äº”ä¸‹åˆï¼ˆåˆæ˜¯å‘¨äº”ï¼ï¼‰ï¼Œæ­£å½“SREå‡†å¤‡æ‘¸é±¼ä¸‹ç­æ—¶ï¼Œçªç„¶æ”¶åˆ°å®¢æˆ·çš„å¤ºå‘½è¿ç¯callï¼šä½ ä»¬çš„APIå’‹æ²¡ç›¸åº”äº†ï¼Ÿå‘ç”Ÿäº†ä»€ä¹ˆäº‹æƒ…ï¼Ÿå½±å“åˆ°æˆ‘ä»¬çš„æ­£å¸¸ä½¿ç”¨äº†ï¼Œè¯·é©¬ä¸Šç€æ‰‹è§£å†³ï¼ï¼ï¼

å¥½å®¶ä¼™ï¼Œåˆæ¥æ´»äº†ï¼å€¼ç­SREå¿ƒé‡Œä¸€ä¸‡åªè‰æ³¥é©¬å¥”è¿‡ï¼Œä½†è¿˜æ˜¯é»˜é»˜æ‰“å¼€äº†Grafanaï¼ˆæ¯•ç«Ÿå·¥èµ„è¿˜è¦è¦çš„ï¼‰ï¼Œæ£€æŸ¥æ—¥å¿—ç³»ç»Ÿï¼Œç¬¬ä¸€æ—¶é—´ç¡®è®¤é—®é¢˜ï¼Œç»è¿‡æ£€æŸ¥å¾—å‡ºå¦‚ä¸‹ç»“è®º
APIå“åº”æ—¶é—´ä»ä¸æ»‘çš„5msç›´æ¥èµ·é£åˆ°2s+ï¼Œç”¨æˆ·ç­‰å¾—éƒ½èƒ½æ³¡æ¯å’–å•¡äº†
CPUç”¨é‡æ— å¼‚å¸¸
APIæ˜¯java basedåº”ç”¨ï¼Œæ˜¾ç¤º garbage collection GCå¼€å§‹ç–¯ç‹‚"æ‰“æ‰«å«ç”Ÿ"ï¼Œåƒåœ¾å›æ”¶æ—¶é—´å˜é•¿ï¼Œæ¬¡æ•°ä»æ¯åˆ†é’Ÿæ‚ é—²çš„3æ¬¡å˜æˆç„¦è™‘çš„15æ¬¡ï¼Œè€å¹´ä»£ä»60%çš„ä½›ç³»çŠ¶æ€é£™å‡åˆ°95%çš„æ¿’æ­»è¾¹ç¼˜ã€‚

åˆæ­¥åˆ†æï¼š
è‚¯å®šæ˜¯APIçš„è¯·æ±‚çº¿ç¨‹åœ¨å¤„ç†çš„æ—¶å€™å¡åœ¨ä»€ä¹ˆåœ°æ–¹äº†ï¼Œä½†æ˜¯åˆ°åº•å¡åœ¨å“ªé‡Œå‘¢ï¼Ÿå·²çŸ¥APIæœ‰å¦‚ä¸‹å¤„ç†æ­¥éª¤ï¼š
1. APIè¯·æ±‚è¦é€šè¿‡è´Ÿè½½å‡è¡¡æœåŠ¡å™¨ï¼Œå†åˆ†é…åˆ°ç‰¹å®šçš„å®ä¾‹æ¥è¿›è¡Œå¤„ç†
2. APIè¦å’Œç¬¬ä¸‰æ–¹æœåŠ¡äº¤äº’ï¼Œæ ¹æ®è¯·æ±‚å‚æ•°è·å–å…·ä½“å¤„ç†æ•°æ®
3. APIå†…éƒ¨éœ€è¦åšæ ¡éªŒï¼Œå’Œæ•°æ®åº“çš„è¡¨äº¤äº’è·å–å®æ—¶æ•°æ®æ¥åšå„ç§å¤„ç†
4. APIè¦å’Œæ•°æ®åº“å†æ¬¡äº¤äº’ä¿å­˜å¤„ç†ç»“æœæ•°æ®

å¦‚ä½•ç¡®è®¤APIæ˜¯å¦æœ‰å…¶ä»–çš„å¤„ç†é€»è¾‘/åˆ†æ”¯ï¼Ÿé™¤äº†é˜…è¯»ä»£ç è¿™ä¸ªæœ€ç›´è§‚çš„åŠæ³•å¤–ï¼Œè¿˜æœ‰ä»€ä¹ˆå…¶ä»–æ›´å¿«é€Ÿçš„æ–¹æ³•å—ï¼Ÿ

å¦‚ä½•å¤„ç†ï¼š
å…µåˆ†ä¸¤è·¯ï¼šä¸€é˜Ÿå»ä»£ç åº“å•ƒä»£ç ï¼ˆç¨‹åºå‘˜çš„æ—¥å¸¸ç—›è‹¦ï¼‰ï¼Œå¦ä¸€é˜Ÿå¼€å§‹"å¤§æµ·æé’ˆ"å¼çš„å…¨é¢æ’æŸ¥
å›´ç»•APIåŠå…¶è™šæ‹Ÿæœºæ·±æŒ–å„ä¸ª grafana çš„å›¾ï¼›ç»§ç»­æŸ¥æ‰¾æ—¥å¿—ç³»ç»Ÿï¼›ç™»é™†åˆ°è™šæ‹ŸæœºæŸ¥çœ‹æœºå™¨çš„çŠ¶æ€ã€‚

å„è‡ªåŸ‹å¤´è‹¦å¹²ï¼Œä¸€æ®µæ—¶é—´ä»¥åã€‚ã€‚ã€‚ã€‚

æŸ³æš—èŠ±æ˜ï¼šç»ˆäºæŠ“åˆ°è¿™ä¸ªå¹•åé»‘æ‰‹
åŸæ¥æ˜¯8000+ä¸ªCLOSE_WAITåƒµå°¸è¿æ¥åœ¨ä½œå¦–ï¼Œå®ƒä»¬åƒä¸èµ°çš„å®¢äººä¸€æ ·éœ¸å ç€socketèµ„æº
é€šè¿‡ç™»é™†è™šæ‹Ÿæœºæ‰§è¡Œnetstatå‘½ä»¤ï¼Œå‘ç°å¤§é‡CLOSE_WAIT (8000+) çš„é“¾æ¥ï¼Œå®ƒä»¬ä½œä¸ºæ— ç”¨çš„é“¾æ¥å ç”¨äº†å¤§é‡ä¸»æœºçš„socketèµ„æºï¼Œå¯¼è‡´ç³»ç»Ÿæ— æ³•ä¸ºæ–°çš„APIè¯·æ±‚åˆ†é…é“¾æ¥ï¼Œæœ€ç»ˆå¯¼è‡´äº†ç³»ç»Ÿæ€§èƒ½ä¸‹é™ã€‚

netstat ğŸ‘‰an | grep CLOSE_WAIT | wc ğŸ‘‰l
âœ… netstatï¼šæ˜¾ç¤ºç½‘ç»œè¿æ¥ã€è·¯ç”±è¡¨ã€æ¥å£ç»Ÿè®¡ç­‰ç½‘ç»œä¿¡æ¯
    ğŸ‘‰aï¼šæ˜¾ç¤ºæ‰€æœ‰è¿æ¥å’Œç›‘å¬ç«¯å£ï¼ˆAllï¼‰
    ğŸ‘‰nï¼šä»¥æ•°å­—å½¢å¼æ˜¾ç¤ºåœ°å€å’Œç«¯å£å·ï¼Œä¸è¿›è¡ŒDNSè§£æï¼ˆNumericï¼‰
âœ… grep CLOSE_WAITï¼šè¿‡æ»¤å‡ºçŠ¶æ€ä¸º CLOSE_WAIT çš„è¿æ¥
âœ… wc ğŸ‘‰lï¼šç»Ÿè®¡è¡Œæ•°ï¼Œå³è¿æ¥çš„æ•°é‡
âœ… ç®¡é“ç¬¦ | æ˜¯Unix/Linuxç³»ç»Ÿä¸­çš„æ ¸å¿ƒæ¦‚å¿µï¼Œç”¨äºå°†ä¸€ä¸ªå‘½ä»¤çš„è¾“å‡ºä½œä¸ºå¦ä¸€ä¸ªå‘½ä»¤çš„è¾“å…¥ã€‚

å…³äº CLOSE_WAIT çš„æµç¨‹ï¼Œè¯·å‚è€ƒå›¾2å’Œ3

ä¿®å¤ï¼š
ç»è¿‡æ¿€çƒˆçš„"å¤´è„‘é£æš´"ï¼ˆå…¶å®å°±æ˜¯å‡ ä¸ªäººå›´ç€ç”µè„‘æŒ‡æŒ‡ç‚¹ç‚¹ï¼‰ï¼Œå†³å®šç»™timeout"ç»­å‘½"ï¼šä»5åˆ†é’Ÿå»¶é•¿åˆ°30åˆ†é’Ÿï¼ŒAPIçš„å“åº”æ—¶é—´æ¢å¤æ­£å¸¸ã€‚ä½œä¸ºä¸´æ—¶è§£å†³æ–¹æ¡ˆï¼Œè¿‡æ¸¡åˆ°æœ€ç»ˆé—®é¢˜å¾—åˆ°è§£å†³ã€‚
è‡³äºä»£ç çš„ä¼˜åŒ–ï¼ŒSREä¹Ÿç»™ç ”å‘å›¢é˜Ÿæäº†é—®é¢˜å•ï¼Œè¦æ±‚ç¨‹åºèƒ½å¤Ÿå¤„ç†æœåŠ¡ç«¯çš„timeoutè¿™ä¸ªå¼‚å¸¸æƒ…å†µï¼Œå‡å°‘æ— ç”¨çš„é“¾æ¥å»ºç«‹ã€‚



æ ‡å‡†çš„TCPè¿æ¥å…³é—­è¿‡ç¨‹ï¼š
å®¢æˆ·ç«¯                    æœåŠ¡ç«¯
  |                        |
  |                        | 1. æœåŠ¡ç«¯ä¸»åŠ¨å…³é—­
  |    <ğŸ‘‰ğŸ‘‰ FINåŒ… ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰|    (å‘é€FINåŒ…)
  |                        |
  | 2. å®¢æˆ·ç«¯æ”¶åˆ°FINåŒ…      |
  |    ğŸ‘‰ğŸ‘‰ğŸ‘‰ ACKåŒ… ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰> |    (å‘é€ACKç¡®è®¤)
  |                        |
  | [è¿›å…¥CLOSE_WAITçŠ¶æ€]    | [è¿›å…¥FIN_WAIT_2çŠ¶æ€]
  |                        |
  | 3. å®¢æˆ·ç«¯å¤„ç†å®Œæ•°æ®     |
  |    ğŸ‘‰ğŸ‘‰ğŸ‘‰ FINåŒ… ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰> |    (å®¢æˆ·ç«¯ä¹Ÿå‘é€FINåŒ…)
  |                        |
  |    <ğŸ‘‰ğŸ‘‰ ACKåŒ… ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰| 4. æœåŠ¡ç«¯ç¡®è®¤
  |                        |
  | [è¿æ¥å®Œå…¨å…³é—­]          | [è¿æ¥å®Œå…¨å…³é—­]

å¼‚å¸¸çš„CLOSE_WAITæµç¨‹ï¼ˆå¡ä½ä¸åŠ¨ï¼‰
å®¢æˆ·ç«¯                    æœåŠ¡ç«¯
  |                        |
  |                        | 1. æœåŠ¡ç«¯ä¸»åŠ¨å…³é—­
  |    <ğŸ‘‰ğŸ‘‰ FINåŒ… ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰|    (å‘é€FINåŒ…)
  |                        |
  | 2. å®¢æˆ·ç«¯æ”¶åˆ°FINåŒ…      |
  |    ğŸ‘‰ğŸ‘‰ğŸ‘‰ ACKåŒ… ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰ğŸ‘‰> |    (å‘é€ACKç¡®è®¤)
  |                        |
  | [è¿›å…¥CLOSE_WAITçŠ¶æ€]    | [è¿›å…¥FIN_WAIT_2çŠ¶æ€]
  |                        |
  | 3. å®¢æˆ·ç«¯ä»£ç æœ‰BUGï¼    |
  |    âŒ æ²¡æœ‰å‘é€FINåŒ…     |    (åº”è¯¥å‘é€ä½†æ²¡å‘é€)
  |                        |
  | [æ°¸è¿œå¡åœ¨CLOSE_WAIT]    | [æ°¸è¿œç­‰å¾…å®¢æˆ·ç«¯çš„FIN]
  |                        |
  | ğŸ’€ åƒµå°¸è¿æ¥è¯ç”Ÿï¼       | ğŸ’€ èµ„æºæ— æ³•é‡Šæ”¾ï¼

----- English
Production Env Incident Review: The Attack of the CLOSE_WAIT Army

Hello everyone, I previously introduced timeout as a ticking time bomb. Today I'm sharing a related production incident.

On a sunny Friday afternoon in 2025, just as the SRE was about to wrap up for the day, we suddenly received urgent calls from customers: "Your API isn't responding! This is impacting our operations - when will it be fixed?"

The on-call SRE's heart sank, but they dutifully opened Grafana, checked the logging system, and began investigating. After some digging, we reached the following conclusions:

API response time skyrocketed from a smooth 5 ms to 2 s+, users could brew a full cup of coffee while waiting
CPU usage showed no anomalies
The Java-based API showed garbage collection (GC) issues: GC time increased significantly, frequency jumped from 3 times per minute to 15 times per minute, and the old generation heap usage spiked from 60% to a critical 95%.
Initial Analysis:
The API request threads must be stuck somewhere during processing, but where exactly? We know the API has the following processing steps:

API requests go through a load balancer, then get distributed to specific instances for processing
API interacts with third-party services to get specific processing data based on request parameters
API needs internal validation and interacts with database tables to get real-time data for various processing
API interacts with the database again to save processing result data
How to confirm if the API has other processing logic/branches? Besides reading code (the most straightforward but painful method for programmers), are there other faster methods?

How to Handle:
We split into two teams: one team dove into the code repository (the classic "read the source" approach), while another team began systematic troubleshooting across all system components.

The troubleshooting team conducted comprehensive diagnostics:
- Analyzed Grafana charts for API and VM metrics
- Searched through log systems for error patterns
- SSH'd into virtual machines to check system status

After thorough investigation, we identified the root cause...

Root Cause Identified: The CLOSE_WAIT Army
We discovered 8000+ CLOSE_WAIT zombie connections consuming socket resources like unwelcome guests who refuse to leave the party.

Using the netstat command, we confirmed the massive connection leak. These zombie connections exhausted the host's socket resources, preventing new API requests from establishing connections and causing severe performance degradation.

```bash
netstat -an | grep CLOSE_WAIT | wc -l
# Result: 8000+
```

Command Breakdown:
- `netstat -an`: Shows all network connections in numeric format
  - `-a`: Display all connections and listening ports
  - `-n`: Show addresses/ports as numbers (no DNS lookup)
- `grep CLOSE_WAIT`: Filter for connections stuck in CLOSE_WAIT state
- `wc -l`: Count the number of matching lines
- `|`: Pipe operator chains commands together

For the CLOSE_WAIT process flow, please refer to Figures 2 and 3

Immediate Fix:
The team implemented a quick workaround by extending the connection timeout from 5 minutes to 30 minutes. This immediately restored API response times to normal levels.

Long-term Resolution:
We filed a critical issue for the development team to:
1. Implement proper connection cleanup in exception handling
2. Add connection pooling with appropriate limits
3. Ensure graceful connection closure in all code paths

Standard TCP Connection Closing Process:
Client                    Server
  |                        |
  |                        | 1. Server initiates close
  |    <-- FIN packet ------|    (sends FIN packet)
  |                        |
  | 2. Client receives FIN  |
  |    --- ACK packet ----> |    (sends ACK confirmation)
  |                        |
  | [Enters CLOSE_WAIT]     | [Enters FIN_WAIT_2]
  |                        |
  | 3. Client finishes data |
  |    --- FIN packet ----> |    (Client also sends FIN)
  |                        |
  |    <-- ACK packet ------| 4. Server confirms
  |                        |
  | [Connection closed]     | [Connection closed]

Abnormal CLOSE_WAIT Process (Stuck):
Client                    Server
  |                        |
  |                        | 1. Server initiates close
  |    <-- FIN packet ------|    (sends FIN packet)
  |                        |
  | 2. Client receives FIN  |
  |    --- ACK packet ----> |    (sends ACK confirmation)
  |                        |
  | [Enters CLOSE_WAIT]     | [Enters FIN_WAIT_2]
  |                        |
  | 3. Client code has BUG! |
  |    âŒ No FIN sent       |    (should send but doesn't)
  |                        |
  | [Forever stuck in       | [Forever waiting for
  |  CLOSE_WAIT]           |  client's FIN]
  |                        |
  | ğŸ’€ Zombie connection!   | ğŸ’€ Resources can't be freed!

ç« èŠ‚3:
----- Chinese
äº‹åæ€»ç»“ï¼šä»å¤±è´¥ä¸­å­¦ä¹ 
å¤§å®¶å¥½ï¼Œä¹‹å‰æˆ‘åˆ†äº«äº† CLOSE_WAIT é€ æˆæ··ä¹±çš„äº‹æ•…ï¼Œç»“åˆæˆ‘ä»¬å­¦ä¹ çš„è°·æ­Œè¿ç»´è§£å¯†ï¼Œä»Šå¤©æˆ‘ä»¬æ¥èŠèŠäº‹æ•…çš„åç»­ï¼Œä¸»è¦åŒ…æ‹¬å¦‚ä¸‹å‡ ä¸ªæ–¹é¢ï¼š

ğŸ“ å®¢æˆ·æ²Ÿé€šå¤„ç†
åŠæ—¶é€šçŸ¥ï¼š äº‹æ•…å‘ç”Ÿ15åˆ†é’Ÿå†…é€šçŸ¥å…³é”®å®¢æˆ·ï¼Œæä¾›å½±å“è¯„ä¼°å’Œé¢„è®¡æ¢å¤æ—¶é—´ã€‚
ğŸ‘‰ è¿›å±•æ±‡æŠ¥ï¼šæ¢å¤æœŸé—´æ¯å°æ—¶æ›´æ–°è¿›å±•
ğŸ‘‰ é€æ˜åº¦ï¼šè¯„ä¼°äº‹æ•…ç­‰çº§ï¼Œæä¾›è¯¦ç»†äº‹æ•…åŸå› å’Œå½±å“èŒƒå›´ï¼Œå‡†å¤‡äº‹æ•…è¯´æ˜ä¹¦
ğŸ‘‰ é¢„é˜²æªæ–½ï¼šæä¾›å…·ä½“æ”¹è¿›è®¡åˆ’å’Œæ—¶é—´è¡¨
ğŸ‘‰ è¡¥å¿æ–¹æ¡ˆï¼šæ ¹æ®SLAåè®®æä¾›æœåŠ¡è¡¥å¿
å¦‚æœäº‹æ•…å¾ˆä¸¥é‡ï¼ˆæŸå¤±äº†å¾ˆå¤šğŸ’°ï¼‰ï¼Œåˆ™éœ€è¦å•†åŠ¡ä»‹å…¥ï¼Œå®‰æ’é«˜å±‚æ²Ÿé€šã€‚

ğŸ” äº‹æ•…å¤ç›˜ 
äº‹æ•…å¤ç›˜æ˜¯ä»å¤±è´¥ä¸­å¸å–æ•™è®­çš„å…³é”®æ­¥éª¤ï¼š
1ï¸âƒ£ é—®é¢˜ç¡®è®¤åŠå½±å“è¯„ä¼°ï¼šç½—åˆ—äº‹æ•…çš„æ—¶é—´çº¿ï¼Œè¯„ä¼°ä¸šåŠ¡å½±å“ç¨‹åº¦ã€èŒƒå›´å’ŒæŒç»­æ—¶é—´ï¼Œç¡®ä¿æ— é—æ¼å…³é”®ä¿¡æ¯
2ï¸âƒ£ åŸå› åˆ†æï¼šæ‰¾å‡ºæ•…éšœæ ¹æœ¬åŸå› ï¼Œåˆ†ææ¸…æ¥šå‰å› åæœ
3ï¸âƒ£ æ”¹è¿›æªæ–½åŠæ€»ç»“ï¼šåˆ¶å®šå…·ä½“æ”¹è¿›æ–¹æ¡ˆï¼Œé™ä½ / é˜²æ­¢é—®é¢˜å†æ¬¡å‘ç”Ÿï¼›åˆ†äº«ç»éªŒæ•™è®­ï¼Œè®©å›¢é˜Ÿå…±åŒæˆé•¿

ğŸš¨ ç›‘æ§å‘Šè­¦ä¼˜åŒ–ï¼šæ–°å¢TCPè¿æ¥çŠ¶æ€åˆ†å¸ƒå’ŒCLOSE_WAITæ•°é‡é˜ˆå€¼å‘Šè­¦:
warning å‘Šè­¦ï¼šé˜ˆå€¼ > 1000 å¹¶ä¸”æŒç»­15åˆ†é’Ÿ
critical å‘Šè­¦ï¼šé˜ˆå€¼ > 2000 å¹¶ä¸”æŒç»­5åˆ†é’Ÿ

ğŸ”§ æŠ€æœ¯æ”¹è¿›æªæ–½ 
ğŸ‘‰ ä»£ç å±‚é¢ï¼š ä¼˜é›…å…³é—­æœºåˆ¶ã€è¿æ¥çŠ¶æ€ç›‘æ§ï¼›
ğŸ‘‰ è¿ç»´å±‚é¢ï¼š ä½¿ç”¨ä¸´æ—¶è§£å†³æ–¹æ¡ˆï¼Œç­‰ä»£ç ä¿®å¤åç¡®è®¤é—®é¢˜æ˜¯å¦å¤ç°ï¼›å®šæœŸå·¡æ£€ã€‚

ğŸ“š çŸ¥è¯†æ²‰æ·€
ğŸ‘‰ æ–‡æ¡£å»ºè®¾ï¼š æ›´æ–°æ•…éšœå¤„ç†æ‰‹å†Œã€å®Œå–„çŸ¥è¯†åº“ã€‚
ğŸ‘‰ å›¢é˜ŸåŸ¹è®­ï¼š ç»„ç»‡å¤ç›˜åˆ†äº«ä¼šã€åº”æ€¥æ¼”ç»ƒã€æŠ€æœ¯åŸ¹è®­ã€‚
æŒç»­æ”¹è¿›ï¼š å®šæœŸå›é¡¾æ”¹è¿›æ•ˆæœã€æ”¶é›†åé¦ˆã€ä¼˜åŒ–æµç¨‹å·¥å…·ã€‚

äº‹æ•…ä¸å¯æ€•ï¼Œå¯æ€•çš„æ˜¯ä¸ä»äº‹æ•…ä¸­å­¦ä¹ ï¼é€šè¿‡ç³»ç»ŸåŒ–çš„äº‹åæ€»ç»“ï¼Œè®©æ¯ä¸€æ¬¡äº‹æ•…éƒ½æˆä¸ºå›¢é˜Ÿæˆé•¿çš„å‚¬åŒ–å‰‚ï¼ç»“åˆè¿™æ¬¡çš„äº‹æ•…å¤ç›˜ï¼Œä¹Ÿèƒ½è®©æˆ‘ä»¬æ›´å¥½ç†è§£ä¹‹å‰åˆ†äº«çš„è°·æ­Œè¿ç»´è§£å¯†ä¹‹äº‹æ•…ç®¡ç†ï¼Œç†è®ºç»“åˆå®è·µï¼Œèµä¸€ä¸ªğŸ‘

----- English

Post-Incident Review: Learning from Failures

Hello everyone, I previously shared the incident caused by CLOSE_WAIT chaos. Building on our study of Google's SRE practices, today let's discuss the post-incident follow-up, covering the following key aspects:

ğŸ“ Customer Communication
- Immediate notification: Notify key customers within 15 minutes of incident detection, providing impact assessment and estimated recovery time
- Progress updates: Provide hourly status updates during recovery
- Transparency: Assess incident severity, provide detailed root cause and impact scope, prepare comprehensive incident report
- Prevention measures: Share specific improvement plan and implementation timeline
- Compensation: Provide service credits according to SLA agreements

For severe incidents (significant financial impact ğŸ’°), business teams must get involved and arrange executive-level communication.

ğŸ” Post-Incident Review Process
Post-incident review is the critical step for learning from failures:
1ï¸âƒ£ Problem confirmation and impact assessment: Document detailed incident timeline, assess business impact severity, scope and duration, ensure no critical information is overlooked
2ï¸âƒ£ Root cause analysis: Identify the fundamental cause of the failure, analyze the complete cause-and-effect chain
3ï¸âƒ£ Improvement measures and lessons learned: Develop specific improvement plans to reduce/prevent problem recurrence; document lessons learned for team growth

ğŸš¨ Monitoring and Alerting Optimization: Add TCP connection state distribution and CLOSE_WAIT count threshold alerts:
- Warning alert: threshold > 1000 connections sustained for 15 minutes
- Critical alert: threshold > 2000 connections sustained for 5 minutes

ğŸ”§ Technical Improvement Measures
- Code level: Implement graceful shutdown mechanisms, add connection state monitoring
- Operations level: Deploy temporary solutions, verify if issues recur after code fixes; establish regular health checks

ğŸ“š Knowledge Management
- Documentation: Update incident response playbooks, enhance knowledge base
- Team training: Organize post-incident sharing sessions, conduct emergency drills, provide technical training
- Continuous improvement: Regularly review improvement effectiveness, collect team feedback, optimize processes and tools

Incidents aren't scary - what's scary is not learning from them! Through systematic post-incident reviews, every incident becomes a catalyst for team growth. This incident review perfectly demonstrates the Google SRE incident management practices we studied earlier - theory meets practice! ğŸ‘

ç« èŠ‚4:
----- Chinese
NOCï¼šSREç•Œçš„"å¤œçŒ«å­ä¿å®‰"ğŸ¦‰
å¤§å®¶å¥½ï¼å‰é¢èŠäº†äº‹æ•…å¤„ç†å’Œå¤ç›˜ï¼Œä»Šå¤©æ¥èŠèŠNOCï¼ˆNetwork Operations Centerï¼‰ã€‚åœ¨SREçš„ä¸–ç•Œé‡Œï¼ŒNOCä¸“é—¨è´Ÿè´£ç›¯æ¢¢ï¼æ­£æ˜¯å› ä¸ºæœ‰è¿™ç¾¤æ•¬ä¸šçš„å°ä¼™ä¼´æ˜¼å¤œä¸åœåœ°ç›¯ç€å„ç§ä»ªè¡¨ç›˜ï¼Œæ‰èƒ½åœ¨ç³»ç»Ÿ"é—¹è„¾æ°”"çš„ç¬¬ä¸€æ—¶é—´ï¼ˆä¸ç®¡æ˜¯å¤§å¹´ä¸‰åè¿˜æ˜¯æƒ…äººèŠ‚ï¼‰å¤ºå‘½è¿ç¯callï¼šå…„å¼Ÿï¼Œæœ‰æ€ªè¦æ‰“ï¼ğŸ˜‚

1ï¸âƒ£ NOCæ˜¯ä»€ä¹ˆï¼Ÿ ğŸ”
ç½‘ç»œè¿è¥ä¸­å¿ƒï¼Œ24 * 7 ä¸“é—¨è´Ÿè´£ç›¯æ¢¢ç”Ÿäº§ç³»ç»Ÿçš„é›†ä¸­åŒ–è¿è¥ä¸­å¿ƒã€‚ä¸€èˆ¬æ¥è¯´ï¼ŒNOCå°±æ˜¯ä¸€ç¾¤"å€¼ç­æˆ˜å£«"ï¼Œæ¯å‘¨éƒ½æœ‰å®šåˆ¶çš„æ’ç­è¡¨ï¼Œç‰¹å®šæ—¶é—´æ®µæœ‰ç‰¹å®šçš„äººç›¯ç€å„ç§èŠ±èŠ±ç»¿ç»¿çš„ç›‘æ§å›¾è¡¨ï¼Œæ¯”å¦‚å‘Šè­¦ç³»ç»Ÿã€Grafanaä»ªè¡¨ç›˜ç­‰ï¼Œç®€ç›´æ¯”çœ‹ç”µè§†å‰§è¿˜ä¸“æ³¨ï¼
æ ¸å¿ƒèŒè´£ï¼š
ğŸ–¥ï¸ å®æ—¶ç›‘æ§ï¼šåƒæ¸¸æˆä¸»æ’­ä¸€æ ·ç›¯ç€å„ç§ä»ªè¡¨ç›˜
ğŸš¨ å‘Šè­¦å¤„ç†ï¼šæ”¶åˆ°å‘Šè­¦å°±åƒæ”¶åˆ°"ç´§æ€¥ä»»åŠ¡"ï¼Œç«‹é©¬å¼€å§‹"æ‰“æ€ªå‡çº§"
ğŸ“ æ²Ÿé€šåè°ƒï¼šå……å½“"ä¼ è¯ç­’"ï¼Œç¬¬ä¸€æ—¶é—´æ‰¾äººæ¥â€œç­ç«â€
ğŸ“ è®°å½•ç»´æŠ¤ï¼šæŠŠæ¯ä¸ª"æˆ˜æ–—"è¿‡ç¨‹éƒ½è®°å½•å¾—æ˜æ˜ç™½ç™½

2ï¸âƒ£ NOC è¿è¡Œæ¨¡å¼
ğŸ‘‰ è¢«åŠ¨ç›‘æ§ï¼šåƒé—¨å«å¤§çˆ·ï¼Œå¹³æ—¶åç€å–èŒ¶ï¼Œæœ‰äº‹æ‰èµ·èº«
ğŸ‘‰ äººå·¥å€¼å®ˆï¼š24å°æ—¶"è¹²å®ˆ"ï¼Œæ¯”ç½‘ç®¡è¿˜æ•¬ä¸š
ğŸ‘‰ æŒ‰æµç¨‹æ“ä½œï¼šä¸¥æ ¼æŒ‰"æ”»ç•¥"èµ°
ğŸ‘‰ é‡ç‚¹å…³æ³¨"å‘ç°é—®é¢˜"

3ï¸âƒ£ ä¸åŒå…¬å¸çš„NOCç­–ç•¥
å¤§å‹ä¼ä¸šï¼ˆé€šå¸¸ä¿ç•™NOCï¼‰ï¼š
ğŸ‘‰ ä¼ ç»Ÿé‡‘èã€ç”µä¿¡ã€å¤§å‹äº’è”ç½‘å…¬å¸
ğŸ‘‰ NOCåš"çœ¼ç›"ï¼ŒSREåš"å¤§è„‘"
ğŸ‘‰ åˆ†å·¥æ˜ç¡®ï¼šç›‘æ§ vs ä¼˜åŒ–

ä¸­å°å‹å…¬å¸ï¼ˆé€šå¸¸æ•´åˆï¼‰ï¼š
ğŸ‘‰ åˆ›ä¸šå…¬å¸ã€æ•æ·å›¢é˜Ÿ
ğŸ‘‰ SREç›´æ¥æ‰¿æ‹…NOCèŒè´£
ğŸ‘‰ ä¸€ä¸ªå›¢é˜Ÿå¤šé‡è§’è‰²

ç°ä»£è¶‹åŠ¿ï¼ˆæ™ºèƒ½åŒ–æ›¿ä»£ï¼‰ï¼š
ğŸ¤– AIè¾…åŠ©ç›‘æ§å’Œè¯Šæ–­
ğŸ”„ è‡ªåŠ¨æ•…éšœæ¢å¤ç³»ç»Ÿ
ğŸ“Š æ™ºèƒ½å‘Šè­¦è¿‡æ»¤å’Œåˆ†æ

4ï¸âƒ£ NOCçš„ä»·å€¼ä¸æŒ‘æˆ˜ 
ä»·å€¼ï¼š
âœ… ä¸“ä¸šç›‘æ§ï¼šä¸“ä¸š"ç›¯æ¢¢"å›¢é˜Ÿï¼Œç«çœ¼é‡‘ç›ï¼Œç»éªŒè€é“
âœ… 24/7è¦†ç›–ï¼šæ¯”ä¾¿åˆ©åº—è¿˜å…¨å¤©å€™ï¼Œæ°¸ä¸æ‰“çƒŠï¼Œå ªç§°"ä¸çœ æˆ˜å£«"
âœ… æ ‡å‡†åŒ–æµç¨‹ï¼šç»Ÿä¸€çš„å¤„ç†æµç¨‹å’Œå“åº”æ ‡å‡†
âœ… æˆæœ¬æ•ˆç›Šï¼šç›¸æ¯”SREï¼ŒNOCäººå‘˜æˆæœ¬è¾ƒä½

æŒ‘æˆ˜ï¼š
âŒ ååº”å¼æ€ç»´ï¼šç­‰é—®é¢˜å‡ºç°æ‰å¤„ç†
âŒ æŠ€èƒ½å±€é™ï¼šç¼ºä¹æ·±åº¦æŠ€æœ¯åˆ†æèƒ½åŠ›
âŒ æ²Ÿé€šæˆæœ¬ï¼šNOCå’ŒSREä¹‹é—´çš„ä¿¡æ¯ä¼ é€’æŸè€—
âŒ åˆ›æ–°é˜»ç¢ï¼šè¿‡åº¦ä¾èµ–æµç¨‹ï¼Œç¼ºä¹çµæ´»æ€§

5ï¸âƒ£ NOC çš„å…¸å‹å·¥ä½œæµç¨‹
å‘Šè­¦æ¥æ”¶ â†’ åˆæ­¥åˆ†æ â†’ åˆ†ç±»å¤„ç† â†’ å‡çº§/è§£å†³ â†’ è®°å½•å½’æ¡£
è¯´åˆ°è¿™ï¼Œå¤§å®¶åº”è¯¥éƒ½èƒ½æ˜ç™½SREçš„å‘Šè­¦æ˜¯ä»å“ªé‡Œæ¥çš„äº†ï¼é™¤äº†è‡ªå·±å®šæ—¶å·¡æ£€å¤–ï¼ŒNOCå°±æ˜¯å¦ä¸€ä¸ªé‡è¦çš„"æƒ…æŠ¥æ¥æº"ã€‚

----- English

NOC: The "Watchdogs" of the SRE World ğŸ¦‰

Hello everyone! After discussing incident handling and post-incident reviews, today let's talk about NOC (Network Operations Center). In the SRE world, NOC specializes in monitoring! It's precisely because of these dedicated folks who watch various dashboards day and night that they can make those urgent calls at the first sign of system issues (whether it's New Year's Eve or Valentine's Day): "Hey buddy, we've got problems to solve!" ğŸ˜‚

1ï¸âƒ£ What is NOC? ğŸ”

Network Operations Center - a centralized operations center that monitors production systems 24/7. Generally speaking, NOC consists of operators with scheduled shifts, where specific people monitor various colorful charts during designated time periods, including alerting systems and Grafana dashboards - they're more focused than people watching TV shows!

Core Responsibilities:
ğŸ–¥ï¸ Real-time monitoring: Monitor various dashboards like game players
ğŸš¨ Alert handling: Receiving alerts triggers immediate response - like getting urgent missions to resolve
ğŸ“ Communication coordination: Act as bridge, quickly connecting the right people to resolve issues
ğŸ“ Record maintenance: Document every incident and response process thoroughly

2ï¸âƒ£ NOC Operating Modes
ğŸ‘‰ Reactive monitoring: Like security guards - typically passive until incidents occur
ğŸ‘‰ 24/7 staffing: Round-the-clock coverage with dedicated personnel
ğŸ‘‰ Process-driven operations: Strictly follow established procedures and playbooks
ğŸ‘‰ Primary focus: Early problem detection and escalation

3ï¸âƒ£ Different Company NOC Strategies

Large enterprises (typically maintain NOC):
ğŸ‘‰ Traditional finance, telecom, and large internet companies
ğŸ‘‰ NOC serves as "eyes," SRE serves as "brain"
ğŸ‘‰ Clear separation of duties: monitoring vs optimization

Small-medium companies (typically integrate):
ğŸ‘‰ Startups and agile teams
ğŸ‘‰ SRE teams directly handle NOC responsibilities
ğŸ‘‰ One team wearing multiple hats

Modern trends (intelligent automation):
ğŸ¤– AI-assisted monitoring and diagnostics
ğŸ”„ Automated incident recovery systems
ğŸ“Š Intelligent alert filtering and analysis

4ï¸âƒ£ NOC Value and Challenges

Value:
âœ… Specialized monitoring: Dedicated monitoring team with expertise and experience
âœ… 24/7 coverage: Continuous coverage like convenience stores - true "always-on" operations
âœ… Standardized processes: Consistent handling procedures and response standards
âœ… Cost efficiency: NOC personnel costs are typically lower than SRE engineers

Challenges:
âŒ Reactive approach: Only responds to problems after they occur
âŒ Limited technical depth: May lack advanced troubleshooting capabilities
âŒ Communication overhead: Potential information loss between NOC and SRE teams
âŒ Process rigidity: Over-dependence on procedures can limit adaptability

5ï¸âƒ£ Typical NOC Workflow
Alert reception â†’ Initial analysis â†’ Issue classification â†’ Escalation/Resolution â†’ Documentation

Now you should understand where SRE alerts come from! Besides proactive monitoring, NOC serves as another crucial "intelligence source."


ç« èŠ‚5:
æ¢ç´¢SREäº§çº¿äº‹æ•…:å­˜å‚¨å¼•å‘çš„â€œåˆ åº“è·‘è·¯â€

å¤§å®¶å¥½ï¼å‰é¢èŠäº†NOCå’Œäº‹æ•…å¤ç›˜ï¼Œä»Šå¤©ä½œä¸ºä¸€åSREï¼Œæ¥åˆ†äº«ä¸€ä¸ªè®©æˆ‘è‡³ä»Šæƒ³èµ·æ¥è¿˜"å¿ƒæœ‰ä½™æ‚¸"çš„çœŸå®æ•…éšœï¼šStorageClassé»˜è®¤å›æ”¶ç­–ç•¥å¯¼è‡´çš„æ•°æ®ä¸¢å¤±äº‹ä»¶ã€‚è¿™ä¸ªæ•…éšœè®©æˆ‘æ·±åˆ»ä½“ä¼šåˆ°äº†ä»€ä¹ˆå«"é»˜è®¤é…ç½®æ˜¯é­”é¬¼"ã€‚

1ï¸âƒ£ æ•…éšœèƒŒæ™¯ï¼šçœ‹ä¼¼æ— å®³çš„"æ¸…ç†"æ“ä½œ
ä¸€ä¸ªæ­£å¸¸çš„å·¥ä½œæ—¥ä¸‹åˆï¼Œæˆ‘åœ¨æ¸…ç†K8sé›†ç¾¤ä¸­ä¸€äº›ä¸ç”¨çš„èµ„æºã€‚çœ‹åˆ°ä¸€ä¸ªå·²ç»ä¸ä½¿ç”¨çš„PVCï¼Œäºæ˜¯æ‰“ç®—å°†å…¶åˆ é™¤ã€‚å¿«é€Ÿå¤åˆ¶PVCåå­—ï¼Œåˆ é™¤å‘½ä»¤ä¸€æ°”å‘µæˆï¼Œå›è½¦èµ°èµ·âš¡ï¸

2ï¸âƒ£ æ•…éšœçˆ†å‘ï¼šSREçš„"è¿ç¯æ€"
åˆ é™¤å‘½ä»¤æ‰§è¡ŒæˆåŠŸï¼Œæ¥ç€æ£€æŸ¥æ‰€æœ‰PVCçš„çŠ¶æ€ï¼Œå‘ç°è¢«åˆ é™¤çš„pvcä¸€ç›´å¤„äº Terminating çš„çŠ¶æ€ï¼Œæ›´è¦å‘½çš„æ˜¯åˆ é™¤å‘½ä»¤é‡Œçš„pvcå±…ç„¶å¤åˆ¶é”™äº†ï¼Œå¤åˆ¶æˆäº†è¢«åˆ é™¤å¯¹è±¡çš„ä¸Šä¸€ä¸ªï¼šgrafanaæ­£åœ¨ä½¿ç”¨çš„PVC ğŸ˜­

3ï¸âƒ£ å‘ç°é—®é¢˜ï¼šSREçš„"å¿ƒè„éª¤åœ"æ—¶åˆ»
çœ‹åˆ°è¯¯åˆ çš„PVCçŠ¶æ€æ˜¯ä¸­æ­¢çš„å†…å¿ƒç‹¬ç™½ï¼š
ğŸ‘‰ å‡ºäº‹äº†ï¼ æ•²å®Œå‘½ä»¤å’‹ä¸æ£€æŸ¥ä¸‹ï¼Ÿï¼å’‹æ¢å¤å‘¢ï¼Ÿï¼Ÿï¼Ÿ

4ï¸âƒ£ ç»è¿‡æœ€åˆçš„æ…Œä¹±ï¼Œæˆ‘å¾ˆå¿«å†·é™ä¸‹æ¥ï¼Œæ¥ç€æ€è€ƒå¯¹ç­–ï¼Œå¾ˆå¿«æ¢³ç†å‡ºäº†å¦‚ä¸‹ä¿¡æ¯ï¼š
ğŸ‘‰ PVCå¤„äºä¸­æ­¢çŠ¶æ€ï¼Œå› ä¸ºGrafana Podè¿˜åœ¨ä½¿ç”¨å®ƒ
ğŸ‘‰ å¦‚æœgrafanaçš„podé‡å¯ï¼Œé‚£ä¹ˆè¿™ä¸ªPVCå°±ä¼šè¢«çœŸæ­£åˆ é™¤ï¼Œç”±äºå®ƒæ˜¯ä½¿ç”¨StorageClassåˆ›å»ºçš„ï¼Œå¹¶ä¸”SCçš„å›æ”¶ç­–ç•¥æ˜¯Deleteï¼ŒPVCè¢«åˆ é™¤ä»¥åè¿å¸¦PVä¹Ÿä¼šè¢«åˆ é™¤ï¼Œä¹Ÿå°±æ˜¯è¯´grafanaçš„æ•°æ®å°±æ²¡æœ‰äº†ã€‚
ğŸ‘‰ grafanaå†æ¬¡å¯åŠ¨ä¼šå¤±è´¥ï¼ŒåŸå› æ˜¯ä¾èµ–çš„PVCä¸å­˜åœ¨äº†
æ­¤æ—¶çš„Grafanaå„é¡¹æŒ‡æ ‡æ­£å¸¸ï¼Œä½†è¿™åªæ˜¯"å›å…‰è¿”ç…§"ï¼Œä¸é è°±å•Šâ˜ ï¸

5ï¸âƒ£ ç´§æ€¥æ•‘æ´ï¼šè‡ªå·±ä»å‘é‡Œçˆ¬å‡ºæ¥
ğŸ‘‰ é€šçŸ¥å›¢é˜Ÿï¼Œè¯´æ˜è¯¯æ“ä½œï¼Œå¹¶æ±‡æŠ¥æ¥ä¸‹æ¥çš„æ¢å¤æ­¥éª¤ï¼ˆåŒ…æ‹¬grafanaé‡å¯ï¼‰ï¼Œè¿™ä¸‹KPIè¦éš¾çœ‹äº†ğŸ˜­
ğŸ‘‰ è¶PVCè¿˜èƒ½ç”¨ï¼Œèµ¶ç´§å¤‡ä»½æ•°æ®ï¼Œè¿™æ˜¯æ•‘å‘½ç¨»è‰ï¼
ğŸ‘‰ é‡å¯grafana podï¼Œæ–°pod å˜ä¸ºpendingçŠ¶æ€ï¼Œæç¤ºä¾èµ–çš„pvcä¸å­˜åœ¨
ğŸ‘‰ åˆ›å»ºæ–° PVCï¼Œå’Œä¹‹å‰è¯¯åˆ çš„åŒå
ğŸ‘‰ æ–°pod é¡ºåˆ©å¯åŠ¨ï¼Œç„¶åå€’å…¥å¤‡ä»½çš„grafanaæ•°æ®
ğŸ‘‰ æ£€æŸ¥grafanaçš„å„é¡¹æŒ‡æ ‡ï¼Œç¡®ä¿æ¢å¤

6ï¸âƒ£ æ€»ç»“æ•™è®­
ğŸ‘‰ æ¸…ç†å¯¹è±¡è¦ä»”ç»†æ£€æŸ¥ï¼Œæ‰§è¡Œå‘½ä»¤å‰ä¸€å®šè¦äºŒæ¬¡ç¡®è®¤
ğŸ‘‰ SCé»˜è®¤Deleteç­–ç•¥æ˜¯éšå½¢æ€æ‰‹ï¼Œéœ€è¦ä¿®æ”¹ä¸ºRetainæ¨¡å¼
ğŸ‘‰ å¤‡ä»½æ˜¯SREçš„ç”Ÿå‘½çº¿ï¼Œæ°¸è¿œä¸è¦ç›¸ä¿¡"ä¸ä¼šå‡ºé—®é¢˜"ï¼


```bash
å¤‡ä»½å‘½ä»¤
kubectl exec -it grafana-pod -- tar -czf /tmp/grafana-backup.tar.gz /var/lib/grafana
kubectl cp grafana-pod:/tmp/grafana-backup.tar.gz ./grafana-backup.tar.gz

æ¢å¤å‘½ä»¤
kubectl cp ./grafana-backup.tar.gz grafana-pod:/tmp/grafana-backup.tar.gz
kubectl exec -it grafana-pod -- tar -xzf /tmp/grafana-backup.tar.gz -C /var/lib/grafana
```

ç« èŠ‚6:
æ¢ç´¢SREäº§çº¿äº‹æ•…: API Serveræ‹’ç»è®¿é—®
å¤§å®¶å¥½ï¼å‰é¢èŠäº†PV/PVCçš„åˆ åº“è·‘è·¯æ•…éšœï¼Œä»Šå¤©æ¥åˆ†äº«å¦ä¸€ä¸ªçœŸå®å‘ç”Ÿçš„ç”Ÿäº§äº‹æ•…ï¼šAPI Serverè¯ä¹¦è¿‡æœŸå¯¼è‡´çš„é›†ç¾¤ç®¡ç†å¤±æ§äº‹ä»¶ã€‚è¿™ä¸ªæ•…éšœè®©æˆ‘æ·±åˆ»ä½“ä¼šåˆ°äº†ä»€ä¹ˆå«"ä¸€å¼ è¯ä¹¦è®©ä½ å¤±å»æ•´ä¸ªé›†ç¾¤çš„æ§åˆ¶æƒ"ã€‚
1ï¸âƒ£ äº‹æ•…å‘ç”Ÿï¼š
çœ‹ä¼¼å¹³é™çš„å·¥ä½œæ—¥æ—©ä¸Šï¼Œçªç„¶å·¥ä½œç¾¤é‡Œä¼ æ¥äº†ä¸å¥½çš„æ¶ˆæ¯ï¼š"CI/CDæ€ä¹ˆå…¨æŒ‚äº†ï¼Ÿ"ã€"kubectlè¿ä¸ä¸Šé›†ç¾¤ï¼"ã€â€œå‘ç”Ÿäº†å•¥ï¼Ÿâ€
æˆ‘çš„ç¬¬ä¸€ååº”ï¼š"æ˜¯ç½‘ç»œé—®é¢˜ï¼Ÿ"
æ®‹é…·çš„ç°å®ï¼šæ¯”ç½‘ç»œé—®é¢˜æ›´è‡´å‘½...

2ï¸âƒ£ æ•…éšœæ£€æŸ¥ï¼š
âš¡ï¸ CI/CDæµæ°´çº¿å…¨éƒ¨å¤±è´¥ï¼Œæ— æ³•è¿æ¥K8sé›†ç¾¤
âš¡ï¸ kubectlå‘½ä»¤æŠ¥é”™"æ— æ³•è¿æ¥æœåŠ¡å™¨"
âš¡ï¸ API Serveræ— æ³•è®¿é—®
âš¡ï¸ çœ‹åˆ°è¯ä¹¦è¿‡æœŸçš„é”™è¯¯æ—¥å¿—(x509: certificate has expired or is not valid yet)ï¼Œè¡€å‹ç¬é—´é£™å‡
å†…å¿ƒç‹¬ç™½ï¼šå®Œäº†ï¼Œæˆ‘å¯æ€œçš„KPI ğŸ˜­

3ï¸âƒ£ åŸå› åˆ†æï¼š
API Serverçš„å®¢æˆ·ç«¯è¯ä¹¦è¿‡æœŸäº†ï¼
äº‹æ•…é“¾æ¡ï¼šè¯ä¹¦åˆ°æœŸ â†’ API Serveræ‹’ç»è¿æ¥ â†’ kubectlå¤±æ•ˆ â†’ CI/CDä¸­æ–­ â†’ å¤±å»é›†ç¾¤æ§åˆ¶æƒ
å°±åƒæ€»åŠ¡å¤„å·¥ä½œäººå‘˜çš„å·¥ä½œè¯è¿‡æœŸäº†ï¼Œé—¨å«ä¸è®©è¿›åŠå…¬å®¤ï¼š
ğŸ‘‰ å­¦ç”Ÿæ— æ³•ç”³è¯·å®¿èˆï¼ˆkubectlå¤±æ•ˆï¼‰
ğŸ‘‰ æ¥¼ç®¡å¤§å¦ˆè”ç³»ä¸ä¸Šæ€»åŠ¡å¤„ï¼ˆkubeletå¤±è”ï¼‰
ğŸ‘‰ æ•´ä¸ªå®¿èˆç®¡ç†ç³»ç»Ÿé™·å…¥æ··ä¹±

4ï¸âƒ£ ä¸šåŠ¡å½±å“ï¼š
ğŸ‘‰ é›†ç¾¤å¤±å»å¯è§‚æµ‹æ€§
ğŸ‘‰ æ— æ³•è¿›è¡Œä»»ä½•ç®¡ç†æ“ä½œ
ğŸ‘‰ å·¥å…·çš„CI/CDä¸­æ–­ï¼ŒæœåŠ¡æ— æ³•æ›´æ–°
ä¼´éšä¸šåŠ¡å½±å“çš„ç¡®è®¤ï¼Œä¸€ä¸ªæ— æ³•é¿å…çš„é—®é¢˜æ‘†åœ¨çœ¼å‰ï¼šä¸ºä»€ä¹ˆè¯ä¹¦è¿‡æœŸå‰æ²¡æœ‰é¢„è­¦ï¼Ÿ

5ï¸âƒ£ é—®é¢˜ä¿®å¤ï¼š
ğŸ‘‰ ç¡®è®¤è¯ä¹¦è¿‡æœŸé—®é¢˜
ğŸ‘‰ ç”Ÿæˆæ–°çš„å®¢æˆ·ç«¯è¯ä¹¦
ğŸ‘‰ æ›´æ–°API Serveré…ç½®ï¼Œé‡å¯æœåŠ¡
ğŸ‘‰ 30åˆ†é’Ÿå†…æ¢å¤kubectlè¿æ¥
ğŸ‘‰ é€šçŸ¥å„å›¢é˜Ÿç³»ç»Ÿå·²æ¢å¤

6ï¸âƒ£ æ”¹è¿›æªæ–½ï¼š
ğŸ‘‰ è®¾ç½®è¯ä¹¦è¿‡æœŸå‰30å¤©ã€7å¤©ã€1å¤©çš„è‡ªåŠ¨å‘Šè­¦
ğŸ‘‰ å»ºç«‹è¯ä¹¦ç®¡ç†çš„æ ‡å‡†æ“ä½œç¨‹åº(SOP)
ğŸ‘‰ å°†è¯ä¹¦æ£€æŸ¥çº³å…¥æ—¥å¸¸å·¡æ£€æ¸…å•
ğŸ‘‰ å®æ–½è¯ä¹¦è‡ªåŠ¨ç»­æœŸæœºåˆ¶(å¯é€‰é¡¹ï¼Œæ ¹æ®å®é™…æƒ…å†µåˆ¤æ–­)

ğŸ“– è¿™æ¬¡æ•…éšœè®©æˆ‘æ·±åˆ»è®¤è¯†åˆ°ï¼šè¯ä¹¦è¿‡æœŸå°±åƒä¸€é¢—"å®šæ—¶ç‚¸å¼¹"ï¼Œå¹³æ—¶çœ‹ä¸å‡ºé—®é¢˜ï¼Œä¸€æ—¦çˆ†ç‚¸å°±è®©ä½ å¤±å»é›†ç¾¤æ§åˆ¶æƒï¼è™½ç„¶ Pod åŠå…¶æœåŠ¡è¿˜åœ¨æ­£å¸¸è¿è¡Œï¼Œç”¨æˆ·æœåŠ¡ä¸å—å½±å“ï¼Œä½†ä½œä¸ºSREå´æ— æ³•ç®¡ç†é›†ç¾¤ï¼
åœ¨K8sçš„ä¸–ç•Œé‡Œï¼Œè¯ä¹¦ä¸æ˜¯è£…é¥°å“ï¼Œè€Œæ˜¯æ§åˆ¶æƒçš„"é’¥åŒ™"ï¼ğŸ”‘

ç« èŠ‚7:
æ¢ç´¢SREäº§çº¿äº‹æ•…: VIMå‘½ä»¤çš„å¨åŠ›

å¤§å®¶å¥½ï¼å‰é¢èŠäº†å„ç§ç”Ÿäº§ç¯å¢ƒçš„æ•…éšœï¼Œä»Šå¤©æ¥åˆ†äº«ä¸€ä¸ªè®©æˆ‘"ç¤¾æ­»"çš„ä½çº§é”™è¯¯ï¼šä¸€ä¸ªçœ‹ä¼¼æ— å®³çš„vimå‘½ä»¤ï¼Œæ˜¯å¦‚ä½•æŠŠæ•´ä¸ªVMé€ä¸Šè¥¿å¤©çš„ã€‚

1ï¸âƒ£ æ•…éšœèƒŒæ™¯ï¼šç£ç›˜å‘Šè­¦
é‚£æ˜¯ä¸€ä¸ªé£å’Œæ—¥ä¸½çš„ä¸‹åˆï¼Œæˆ‘æ”¶åˆ°äº†æ¥è‡ªNOCçš„å‘Šè­¦ï¼šæŸå°VMçš„ç£ç›˜å ç”¨ç‡åˆ°äº†85%ï¼è¿™ä¸ªæ•…äº‹å°±è¿™æ ·æ³¢æ¾œä¸æƒŠçš„å¼€å§‹äº†ğŸ˜‚

2ï¸âƒ£ ç£ç›˜æ£€æŸ¥ï¼š
æŒ‰éƒ¨å°±ç­æ£€æŸ¥Grafanaå›¾è¡¨å¹¶ç™»å½•VMï¼Œå„é¡¹æ£€æŸ¥æ­£å¸¸ï¼š60Gç£ç›˜ï¼Œè·‘ç€Tomcatåº”ç”¨ï¼Œä»…ä»…ç£ç›˜ä½¿ç”¨è¿‡å¤§

3ï¸âƒ£ æ•…éšœå‘ç°ï¼šæ—¥å¿—æ–‡ä»¶çš„"è†¨èƒ€ä¹‹è·¯" ğŸ“ˆ

```bash
df -h
# /dev/sda1    60G   51G  9G  85%   /

du -sh /var/log/* | sort -hr
# 40G    /var/log/tomcat/catalina.out
```

çœ‹åˆ°è¿™ä¸ªç»“æœï¼š"å¥½å®¶ä¼™ï¼Œ40Gçš„æ—¥å¿—æ–‡ä»¶ï¼"ï¼Œé—®é¢˜çœ‹èµ·æ¥å¾ˆæ¸…æ™°ï¼š
ğŸ‘‰ æ—¥å¿—æ–‡ä»¶éœ€è¦æ¸…ç†
ğŸ‘‰ åº”ç”¨æ—¥å¿—è¾“å‡ºæ²¡æœ‰æ§åˆ¶
ğŸ‘‰ æ—¥å¿—è½®è½¬é…ç½®å¤±æ•ˆ
æ­¤æ—¶çš„æˆ‘ï¼Œä¿¡å¿ƒæ»¡æ»¡ï¼š"å°caseï¼Œçœ‹çœ‹æ—¥å¿—å†…å®¹ï¼Œæ‰¾å‡ºé—®é¢˜æ ¹æºï¼"

4ï¸âƒ£ è‡´å‘½æ“ä½œï¼švimçš„"æ­»äº¡ç¿»æ»š" ğŸ’€
ä¸‹æ„è¯†åœ°è¾“å…¥VIMã€æŒ‡å®šæ—¥å¿—æ–‡ä»¶ï¼Œå›è½¦èµ°èµ·ï¼Œç†Ÿæ‚‰çš„æ—¥å¿—å†…å®¹çª—å£å¹¶æ²¡æœ‰å‡ºç°ï¼Œæ„Ÿè§‰ä¸å¯¹åŠ²äº†ï¼š
ğŸ‘‰ vimå¼€å§‹è¯»å–40Gçš„æ–‡ä»¶
ğŸ‘‰ ç³»ç»Ÿç–¯ç‹‚ä½¿ç”¨å‰©ä½™çš„9Gç£ç›˜ç©ºé—´
ğŸ‘‰ åˆ›å»ºä¸´æ—¶æ–‡ä»¶ã€äº¤æ¢æ–‡ä»¶ã€ç¼“å­˜æ–‡ä»¶...
ğŸ‘‰ å‡ åˆ†é’Ÿåï¼Œç£ç›˜ç©ºé—´å½»åº•è€—å°½

5ï¸âƒ£ æ•…éšœçˆ†å‘ï¼šç£ç›˜ç©ºé—´è€—å°½åçš„VMçš„"å…¨é¢å´©æºƒ" ğŸ’¥
ğŸ‘‰ æ— æ³•åˆ›å»ºæ–°æ–‡ä»¶ï¼Œç³»ç»ŸæœåŠ¡å¼‚å¸¸
ğŸ‘‰ Tomcatæ— æ³•å†™å…¥æ—¥å¿—ï¼Œå¼€å§‹æŠ¥é”™
ğŸ‘‰ åº”ç”¨æ— æ³•åˆ›å»ºä¸´æ—¶æ–‡ä»¶
ğŸ‘‰ SSHè¿æ¥å˜å¾—æå…¶ç¼“æ…¢
ğŸ‘‰ VMçš„ç›‘æ§å„ç§å‘Šè­¦å˜ç€æ³•å­æ¥è¿å‡ºç°

æ­¤æ—¶çš„æˆ‘ï¼š"å®Œäº†ï¼Œæˆ‘å¯æ€œçš„KPI...ä¸ºä»€ä¹ˆè¦ç”¨vimæ‰“å¼€40Gçš„æ–‡ä»¶ï¼Ÿä¸ºä»€ä¹ˆä¸å…ˆç”¨tailçœ‹ä¸€ä¸‹ï¼Ÿ"

6ï¸âƒ£ åº”æ€¥å¤„ç†ï¼šå†æ¬¡æŠŠè‡ªå·±æèµ·æ¥ ğŸ†˜
ğŸ‘‰ é€šçŸ¥å›¢é˜Ÿå’ŒNOCï¼Œæˆ‘æƒ¹ç¥¸äº† ğŸ˜…ğŸ˜­
ğŸ‘‰ å¼ºåˆ¶é€€å‡ºvim
ğŸ‘‰ åˆ é™¤vimä¸´æ—¶æ–‡ä»¶
ğŸ‘‰ å¤‡ä»½å¹¶æˆªæ–­æ—¥å¿—æ–‡ä»¶
ğŸ‘‰ é‡å¯æœåŠ¡

ç´§æ€¥å¤„ç†æ­¥éª¤ï¼š
```bash
# 1. å¼ºåˆ¶é€€å‡ºvim
ps aux | grep vim
kill -9 [vimè¿›ç¨‹ID]

# 2. åˆ é™¤vimä¸´æ—¶æ–‡ä»¶
rm -f /tmp/.catalina.out.swp
rm -f /var/tmp/vi.recover.*

# 3. å¤‡ä»½å¹¶æˆªæ–­æ—¥å¿—æ–‡ä»¶
tail -10000 /var/log/tomcat/catalina.out > /tmp/catalina_backup.log
truncate -s 0 /var/log/tomcat/catalina.out

# 4. é‡å¯æœåŠ¡
systemctl restart tomcat
```


7ï¸âƒ£ æ•…éšœå¤ç›˜ï¼šä»å¤±è´¥ä¸­å¸å–æ•™è®­
äº‹æ•…é“¾æ¡ï¼šæ—¥å¿—é…ç½®ç¼ºå¤± â†’ æ–‡ä»¶æ— é™å¢é•¿ â†’ æ“ä½œä¹ æƒ¯ä¸å½“ â†’ viméœ€è¦å¤§é‡ä¸´æ—¶ç©ºé—´ â†’ ç£ç›˜è€—å°½ â†’ ç³»ç»Ÿå¼‚å¸¸
vimçš„â€œè´ªåƒè›‡â€æœºåˆ¶ï¼š
ğŸ‘‰ åˆ›å»ºäº¤æ¢æ–‡ä»¶ï¼ˆ.swpï¼‰
ğŸ‘‰ åˆ›å»ºå¤‡ä»½æ–‡ä»¶å’Œæ’¤é”€å†å²
ğŸ‘‰ å¯¹40Gæ–‡ä»¶éœ€è¦å¤§é‡ç£ç›˜ç©ºé—´

8ï¸âƒ£ æ”¹è¿›æªæ–½ï¼šä»"è¸©å‘"åˆ°"é¿å‘" ğŸ›¡ï¸
ğŸ‘‰ è®¾ç½®logrotateï¼Œé™åˆ¶å•ä¸ªæ–‡ä»¶å¤§å°
ğŸ‘‰ é…ç½®åˆç†çš„æ—¥å¿—çº§åˆ«å’Œè¾“å‡ºæ ¼å¼
ğŸ‘‰ å¤§æ–‡ä»¶æŸ¥çœ‹ä½¿ç”¨lessã€tailã€headç­‰å‘½ä»¤
ğŸ‘‰ æ“ä½œå‰å…ˆç¡®è®¤å‰©ä½™ç£ç›˜ç©ºé—´
ğŸ‘‰ å¯¹å¤§æ–‡ä»¶æ“ä½œè¦æ ¼å¤–å°å¿ƒ
è¿™æ¬¡æ•…éšœè®©æˆ‘æ·±åˆ»è®¤è¯†åˆ°ï¼šæœ‰æ—¶å€™æœ€è‡´å‘½çš„ä¸æ˜¯å¤æ‚çš„æ¶æ„é—®é¢˜ï¼Œè€Œæ˜¯çœ‹ä¼¼ç†Ÿæ‚‰çš„æ—¥å¸¸å‘½ä»¤ï¼
è¯·è®°ä½ï¼šå·¥å…·æœ‰è¾¹ç•Œï¼Œæ“ä½œè¦è°¨æ…ï¼

æ­£ç¡®çš„å¤§æ–‡ä»¶æŸ¥çœ‹å§¿åŠ¿
```bash
æŸ¥çœ‹æ–‡ä»¶ç»“å°¾
tail -100 /var/log/tomcat/catalina.out

åˆ†é¡µæŸ¥çœ‹ï¼ˆæ¨èï¼‰
less /var/log/tomcat/catalina.out

æœç´¢ç‰¹å®šå†…å®¹
grep "ERROR" /var/log/tomcat/catalina.out | tail -20
```

ç« èŠ‚8:
----- Chinese
æ¢ç´¢SREäº§çº¿äº‹æ•…: Promeæ•°æ®ç¼ºå¤±ï¼ˆä¸Šç¯‡ï¼šæ•…éšœå‘ç°ï¼‰

å¤§å®¶å¥½ï¼å‰é¢èŠäº†vimçš„"æ­»äº¡ç¿»æ»š"ï¼Œä»Šå¤©æ¥åˆ†äº«å¦ä¸€ä¸ªè®©æˆ‘"å¤´ç§ƒ"çš„çœŸå®æ•…éšœï¼šPrometheusè”é‚¦æ¨¡å¼ä¸‹çš„æ•°æ®ç¼ºå¤±é—®é¢˜ã€‚è¿™ä¸ªæ•…éšœè®©æˆ‘æ·±åˆ»ä½“ä¼šåˆ°äº†ä»€ä¹ˆå«"çœ‹èµ·æ¥å¾ˆç®€å•çš„æ¶æ„ï¼Œå®é™…ä¸Šå‘å¾ˆæ·±"ï¼
1ï¸âƒ£ æ•…éšœèƒŒæ™¯ï¼šè”é‚¦æ¨¡å¼çš„"ç¾å¥½æ„¿æ™¯" ğŸŒ
é‚£æ˜¯ä¸€ä¸ªé˜³å…‰æ˜åªšçš„å‘¨äºŒï¼Œæˆ‘ä»¬åˆšåˆšéƒ¨ç½²äº†Prometheusçš„è”é‚¦ï¼ˆFederationï¼‰æ¶æ„æ²¡å¤šä¹…ï¼Œç»„ä»¶å¦‚ä¸‹ï¼š
ğŸ‘‰ Source Instanceï¼šè´Ÿè´£æ”¶é›†å„ç§metricsï¼Œæ•°æ®å¾ˆå…¨é¢ï¼Œä¸»è¦ç”¨äºæ•°æ®çš„å†™æ“ä½œ
ğŸ‘‰ Federate Instanceï¼šé€šè¿‡`/federate`ç«¯ç‚¹ä»Sourceæ‹‰å–æ•°æ®ï¼Œç”¨äºå„ç§æ•°æ®æŸ¥è¯¢å’Œé•¿æœŸå­˜å‚¨
æ¶æ„è®¾è®¡åˆè¡·ï¼š"è¿™æ ·æ—¢èƒ½ä¿è¯æ•°æ®æ”¶é›†çš„å®Œæ•´æ€§ï¼Œåˆèƒ½å®ç°æ•°æ®çš„åˆ†å±‚å­˜å‚¨ï¼Œè€Œä¸”è¿˜å®ç°äº†è¯»å†™åˆ†ç¦»ï¼Œè¿™ä¸‹prometheusçš„æ€§èƒ½ä¸å¾—æ æ çš„ï¼Œç®€ç›´å®Œç¾ï¼"
2ï¸âƒ£ æ•…éšœå‘ç°ï¼šç›‘æ§æ•°æ®çš„"ç¥ç§˜å¤±è¸ª" ğŸ“‰
ä¸å‡ºæ„å¤–çš„è¯æ„å¤–å°±æ¥äº†ï¼Œå¾ˆå¿«åŒäº‹å°±æ‰¾åˆ°æˆ‘ï¼ŒæŠ›å‡ºä¸€ä¸ªé—®é¢˜ï¼š"ä¸ºä»€ä¹ˆGrafanaä¸Šçš„æŸäº›æŒ‡æ ‡å›¾è¡¨æœ‰æ–­ç‚¹ï¼Ÿæ˜æ˜åº”ç”¨åœ¨æ­£å¸¸è¿è¡Œå•Šï¼"
3ï¸âƒ£ åˆæ­¥æ£€æŸ¥ï¼š
âœ… Source prometheus æ•°æ®å®Œæ•´ï¼Œæ‰€æœ‰targetéƒ½æ­£å¸¸
âŒ Federate prometheus éƒ¨åˆ†æ•°æ®ç¼ºå¤±ï¼Œçº¦30%çš„targetæ•°æ®ä¸å­˜åœ¨
3ï¸âƒ£ é—®é¢˜ç¡®è®¤ï¼š
Federate Instanceç¡®å®åœ¨ä¸¢å¤±æ•°æ®ï¼æŸäº›metricsåœ¨Sourceæœ‰ï¼Œä½†åœ¨Federateå°±æ˜¯æ²¡æœ‰ã€‚
4ï¸âƒ£ æ¥èŠèŠå§ï¼š
å„ä½äº²ï¼Œå¦‚æœä½ é‡åˆ°è¿™ç§è”é‚¦æ•°æ®ç¼ºå¤±çš„æƒ…å†µï¼Œä½ ä¼šä»å“ªå‡ ä¸ªæ–¹å‘å¼€å§‹æ’æŸ¥ï¼Ÿ
ğŸ‘‰ ç½‘ç»œè¿é€šæ€§ï¼Ÿ
ğŸ‘‰ é…ç½®æ–‡ä»¶ï¼Ÿ  
ğŸ‘‰ æ—¥å¿—åˆ†æï¼Ÿ
ğŸ‘‰ æ•°æ®é‡å’Œæ€§èƒ½ï¼Ÿ
ğŸ‘‰ è¿˜æ˜¯å…¶ä»–è§’åº¦ï¼Ÿ
æ¬¢è¿æ¥äº’åŠ¨è®¨è®º
5ï¸âƒ£ ä¸‹ç¯‡é¢„å‘Šï¼š
åœ¨åç»­ç¯‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†æ·±å…¥åˆ†æï¼š
- å¦‚ä½•ç³»ç»Ÿæ€§åœ°æ’æŸ¥è”é‚¦æ•°æ®ç¼ºå¤±é—®é¢˜
- å‘ç°çš„æƒŠäººæ ¹å› ï¼šæ•°æ®é‡çš„"ç”œèœœè´Ÿæ‹…"
- å®ç”¨çš„è§£å†³æ–¹æ¡ˆå’Œä¼˜åŒ–ç­–ç•¥
æ•¬è¯·æœŸå¾…ï¼ğŸ”

----- English
Exploring SRE Production Incidents: Prometheus Data Loss (Part 1: Incident Discovery)

Hello everyone! I want to share a real incident that drove me crazy: a data loss issue in Prometheus federation mode. This incident made me deeply realize that "A seemingly simple architecture can be full of hidden traps."

1ï¸âƒ£ Incident Background: The "Beautiful Vision" of Prometheus Federation Mode ğŸŒ
It was a sunny Tuesday, and we had just deployed Prometheus federation architecture. The components were as follows:
ğŸ‘‰ Source Instance: Responsible for collecting various metrics, with comprehensive data, mainly used for data writing operations.
ğŸ‘‰ Federate Instance: Pulls data from the Source via the `/federate` endpoint, used for various data queries and long-term storage.
Design intention: "This way, we can ensure the integrity of data collection, achieve layered storage, and implement read-write separation. Prometheus performance should be excellentâ€”simply perfect!"

2ï¸âƒ£ Incident Discovery: The "Mysterious Disappearance" of Monitoring Data ğŸ“‰
Sure enough, an issue arose. Soon, a colleague approached me with a question: "Why are there gaps in some Grafana dashboards? The application is running normally!"
3ï¸âƒ£ Troubleshooting Steps:
âœ… Source Prometheus data is complete, all targets are normal.
âŒ Federate Prometheus is missing some data; about 30% of target data is absent.
Some metrics exist in the Source but are missing from the Federate instance.
4ï¸âƒ£ Let's Discuss:
When facing federation data loss, which troubleshooting directions would you consider?
- Network connectivity
- Configuration files
- Log analysis
- Data volume and performance
- Other perspectives
What would you check first? Share your thoughts below!

5ï¸âƒ£ Preview of the Next Part:
In the following chapters, we will analyze:
- How to systematically troubleshoot federation data loss issues
- The surprising root cause: the "sweet burden" of data volume
- Practical solutions and optimization strategies
Stay tuned! ğŸ”

ç« èŠ‚9
----- Chinese
Prometheusæ•°æ®ç¼ºå¤±æ•…éšœ: æ—¶åºæ•°æ®ç®€ä»‹
å¤§å®¶å¥½ï¼ä¸Šç¯‡ç»™Prometheus æ•°æ®ç¼ºå¤±æ•…éšœå¼€äº†ä¸ªå¤´ï¼Œä»Šå¤©æˆ‘ä»¬ç»§ç»­æ¥èŠèŠè¿™ä¸ªæ•…éšœçš„ä¸»è§’ï¼šæ—¶åºæ•°æ®ã€‚
1ï¸âƒ£ æ—¶åºæ•°æ®
é¡¾åæ€ä¹‰ï¼Œæ—¶åºæ•°æ®å°±æ˜¯éšç€æ—¶é—´å˜åŒ–çš„æ•°æ®ã€‚ä¸¾ä¸ªè¿‡å±±è½¦çš„ğŸŒ°ï¼Œå½“ä½ åä¸Šè¿‡å±±è½¦ä»å‡ºå‘å¼€å§‹ï¼Œä½ çš„é«˜åº¦å°±æ˜¯æ—¶åºæ•°æ®ï¼Œéšç€æ—¶é—´çš„æ¨ç§»ï¼Œä½ çš„é«˜åº¦åœ¨ä¸æ–­å˜åŒ–ï¼Œä½ çš„å£°éŸ³ä¹Ÿåœ¨å‰§çƒˆæ³¢åŠ¨ï¼Œæ¯ä¸€ä¸ªæ—¶é—´ç‚¹çš„é«˜åº¦å’Œå£°éŸ³éƒ½æ˜¯æ—¶åºæ•°æ®ã€‚
æ—¶åºæ•°æ®çš„ç‰¹ç‚¹ï¼š
ğŸ‘‰ æµ·é‡æ•°æ®ï¼šæ ¹æ®ä½ çš„é‡‡é›†éœ€æ±‚ï¼Œæ¯åˆ†é’Ÿå¯èƒ½äº§ç”Ÿå‡ ç™¾ï½æˆåƒä¸Šä¸‡ä¸ªæ•°æ®ç‚¹ï¼Œç”šè‡³æ›´å¤šğŸ”
ğŸ‘‰ æ—¶é—´æ•æ„Ÿï¼šæ•°æ®æŒ‰æ—¶é—´æˆ³ä¸¥æ ¼æ’åºï¼Œå¿ å®è®°å½•ç›‘æ§å¯¹è±¡çš„äº‹å®çŠ¶æ€
ğŸ‘‰ é«˜å†™å…¥é¢‘ç‡ï¼šç›‘æ§æŒ‡æ ‡éœ€è¦æŒç»­ä¸æ–­åœ°å†™å…¥
ğŸ‘‰ æŸ¥è¯¢æ¨¡å¼å›ºå®šï¼šä¸»è¦æ˜¯èŒƒå›´æŸ¥è¯¢å’Œèšåˆè®¡ç®—
2ï¸âƒ£ ä¸ºä»€ä¹ˆéœ€è¦æ—¶åºæ•°æ®ï¼Ÿ
å› ä¸ºæ•…éšœå’Œæ€§èƒ½é—®é¢˜éƒ½æœ‰æ—¶é—´ç‰¹å¾ï¼
ç›‘æ§çš„æœ¬è´¨éœ€æ±‚ï¼š
ğŸ‘‰ è¶‹åŠ¿åˆ†æï¼šCPUä½¿ç”¨ç‡æ˜¯åœ¨ä¸Šå‡è¿˜æ˜¯ä¸‹é™ï¼Ÿ
ğŸ‘‰ å¼‚å¸¸æ£€æµ‹ï¼šå“åº”æ—¶é—´çªç„¶é£™å‡äº†å—ï¼Ÿ
ğŸ‘‰ å®¹é‡è§„åˆ’ï¼šå†…å­˜ä½¿ç”¨é‡çš„å¢é•¿è¶‹åŠ¿å¦‚ä½•ï¼Ÿ
ğŸ‘‰ æ•…éšœå›æº¯ï¼šé—®é¢˜æ˜¯ä»€ä¹ˆæ—¶å€™å¼€å§‹çš„ï¼Ÿ
3ï¸âƒ£ ä¸¾ä¸ªä¾‹å­
å¦‚æœæœ‰äººé—®ä½ "è¿‡å±±è½¦åˆºæ¿€å—ï¼Ÿ"ï¼Œä½ åªå‘Šè¯‰ä»–"æˆ‘ç°åœ¨åœ¨50ç±³é«˜åº¦"ï¼Œä»–æ ¹æœ¬æ„Ÿå—ä¸åˆ°åˆºæ¿€ã€‚ä½†å¦‚æœä½ ç»™ä»–çœ‹å®Œæ•´çš„é«˜åº¦å˜åŒ–æ›²çº¿ï¼š
00:00 - 2ç±³ (èµ·ç‚¹å¹³å°)ï¼Œ æœ‰è¯´æœ‰ç¬‘ï¼Œè¿˜åœ¨æ‹ç…§
00:30 - 25ç±³ (ç¼“æ…¢çˆ¬å‡) ï¼Œå¼€å§‹ç´§å¼ ï¼Œæ¡ç´§æ‰¶æ‰‹
01:00 - 80ç±³ (åˆ°è¾¾æœ€é«˜ç‚¹)ï¼Œå¿ƒè·³åŠ é€Ÿï¼Œå¾€ä¸‹çœ‹è…¿è½¯
01:05 - 5ç±³ (æ€¥é€Ÿä¿¯å†²ï¼)ï¼Œ å°–å«å£°éœ‡å¤©ï¼Œé­‚é£é­„æ•£
01:10 - 45ç±³ (ç¬¬äºŒä¸ªé«˜å³°) ï¼Œ è¿˜æ²¡ç¼“è¿‡ç¥ï¼Œåˆæ¥ä¸€æ³¢
01:15 - 2ç±³ (å›åˆ°ç»ˆç‚¹)ï¼Œ è…¿è½¯ä¸‹è½¦ğŸ˜…
è¿™æ ·ä»–å°±èƒ½æ¸…æ¥šåœ°æ„Ÿå—åˆ°è¿‡å±±è½¦çš„åˆºæ¿€è½¨è¿¹ï¼Œç†è§£ä»€ä¹ˆå«"å¿ƒè·³åŠ é€Ÿ"ï¼
åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œç›‘æ§å¯¹è±¡åœ¨å‘ç”Ÿæ•…éšœæ—¶ä¹Ÿå­˜åœ¨ç±»ä¼¼çš„å˜åŒ–ï¼šè¿è¡Œæ­£å¸¸ -> é—®é¢˜å‡ºç° -> æ¶åŒ– -> å®•æœºï¼Œæ—¶åºæ•°æ®å¿ å®åœ°è®°å½•ç€è¿™äº›å˜åŒ–è¿‡ç¨‹ã€‚
ä¾‹å¦‚APIçš„å“åº”æ—¶é—´åœ¨æ•…éšœæœŸé—´çš„å˜åŒ–è¿‡ç¨‹ï¼š
00:00 - 5ms (æ­£å¸¸æ°´å¹³)
00:30 - 10ms (è½»å¾®ä¸Šå‡)
01:00 - 200ms (å‰§çƒˆä¸Šå‡)
01:30 - 1s (æŒç»­æ¶åŒ–) 
02:00 - 2s (æ— æ³•è®¿é—®)
å¦‚æœç›‘æ§ç³»ç»Ÿæ²¡æœ‰å‘ç°è¿™ä¸ªæ•°æ®çš„å¼‚å¸¸å¹¶è§¦å‘å‘Šè­¦ï¼Œé‚£ä¹ˆæ­å–œä½ ï¼šå–œæå®¢æˆ·æŠ•è¯‰å’Œè€æ¿çš„"äº²åˆ‡é—®å€™"âš¡ï¸
4ï¸âƒ£ æ—¶åºæ•°æ®çš„å­˜å‚¨
æ—¶åºæ•°æ®çš„å­˜å‚¨éœ€è¦è€ƒè™‘ä»¥ä¸‹å› ç´ ï¼š
- æ•°æ®é‡ï¼šæ¯ç§’äº§ç”Ÿå¤šå°‘æ•°æ®ç‚¹
- æŸ¥è¯¢æ¨¡å¼ï¼šä¸»è¦çš„æŸ¥è¯¢ç±»å‹å’Œé¢‘ç‡
- æ•°æ®ä¿ç•™ï¼šéœ€è¦ä¿ç•™å¤šé•¿æ—¶é—´çš„æ•°æ®
- å¯æ‰©å±•æ€§ï¼šå­˜å‚¨ç³»ç»Ÿæ˜¯å¦å®¹æ˜“æ‰©å±•
- æˆæœ¬ï¼šå­˜å‚¨å’ŒæŸ¥è¯¢çš„æˆæœ¬
è¯´åˆ°è¿™ï¼Œå°±è‡ªç„¶è€Œç„¶çš„è¿‡æ¸¡åˆ°é€‰æ‹©Prometheusä½œä¸ºæ—¶åºæ•°æ®åº“ï¼åœ¨å¤å¸Œè…Šç¥è¯ä¸­ï¼ŒPrometheusæ˜¯ä¸ºäººç±»ç›—å–ç«ç§çš„ç¥ï¼Œè€Œåœ¨ç›‘æ§é¢†åŸŸï¼Œå®ƒä¸ºæˆ‘ä»¬å¸¦æ¥äº†å¼ºå¤§çš„æ—¶åºæ•°æ®å¤„ç†èƒ½åŠ›ã€‚

åœ¨æˆ‘ä»¬çš„Prometheusåœºæ™¯ä¸­ï¼š
Source Instanceéœ€è¦æ”¶é›†å¤§é‡çš„æ—¶åºæ•°æ®ï¼š
- ç³»ç»ŸæŒ‡æ ‡ï¼šCPUã€å†…å­˜ã€ç£ç›˜ã€ç½‘ç»œçš„å®æ—¶å˜åŒ–
- åº”ç”¨æŒ‡æ ‡ï¼šHTTPè¯·æ±‚æ•°ã€å“åº”æ—¶é—´ã€é”™è¯¯ç‡çš„æ³¢åŠ¨
- ä¸šåŠ¡æŒ‡æ ‡ï¼šè®¢å•æ•°é‡ã€ç”¨æˆ·æ´»è·ƒåº¦ã€æ”¶å…¥çš„æ—¶é—´åˆ†å¸ƒ
- KubernetesæŒ‡æ ‡ï¼šPodçŠ¶æ€ã€Serviceå¥åº·åº¦ã€èµ„æºä½¿ç”¨çš„åŠ¨æ€å˜åŒ–

æ•°æ®é‡çš„ç°å®ï¼š
å‡è®¾æˆ‘ä»¬æœ‰100ä¸ªæœåŠ¡ï¼Œæ¯ä¸ªæœåŠ¡æš´éœ²200ä¸ªæŒ‡æ ‡ï¼Œæ¯15ç§’é‡‡é›†ä¸€æ¬¡ï¼š
```
100æœåŠ¡ Ã— 200æŒ‡æ ‡ Ã— 4æ¬¡/åˆ†é’Ÿ = 80,000ä¸ªæ•°æ®ç‚¹/åˆ†é’Ÿ
```
ä¸€å¤©å°±æ˜¯ï¼š80,000 Ã— 60 Ã— 24 = 1.15äº¿ä¸ªæ•°æ®ç‚¹ï¼

åŠ ä¸ŠPrometheusçš„æ ‡ç­¾ï¼ˆlabelsï¼‰ç³»ç»Ÿï¼Œå®é™…æ•°æ®é‡è¿˜ä¼šæˆå€å¢é•¿ã€‚æ¯”å¦‚ä¸€ä¸ªHTTPè¯·æ±‚æŒ‡æ ‡å¯èƒ½æœ‰è¿™äº›æ ‡ç­¾ï¼š
```
http_requests_total{method="GET", status="200", endpoint="/api/users"}
http_requests_total{method="POST", status="404", endpoint="/api/orders"}
http_requests_total{method="PUT", status="500", endpoint="/api/products"}
```

æ¯ä¸ªæ ‡ç­¾ç»„åˆéƒ½æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„æ—¶é—´åºåˆ—ï¼è¿™å°±æ˜¯æˆ‘ä»¬åé¢è¦æ·±å…¥åˆ†æçš„"ç”œèœœè´Ÿæ‹…"â€”â€”æ•°æ®è¶Šè¯¦ç»†è¶Šæœ‰ç”¨ï¼Œä½†ä¼ è¾“å’Œå­˜å‚¨çš„å‹åŠ›ä¹Ÿè¶Šå¤§...
ä¸€å¥è¯ï¼Œæ—¶åºæ•°æ®å°±æ˜¯ç›‘æ§ç³»ç»Ÿçš„å“¨å…µï¼Œé‡è¦æ€§ä¸è¨€è€Œå–»ï¼Œå®ƒè¦æ˜¯ç¼ºå¤±äº†ä½ çš„ç›‘æ§å’ŒKPI ğŸ’° å°±ä¸ç¨³äº†ã€‚

----- English
Prometheus Data Loss Incident: Introduction to Time Series Data

Hello everyone! In the previous article, we introduced the Prometheus data loss incident. Today, let's continue and talk about the main character of this issue: time series data.

1ï¸âƒ£ Time Series Data
As the name suggests, time series data is data that changes over time. For example, think of a roller coaster ride: from the moment you set off, your altitude is time series data, constantly changing as time passes, and your screams fluctuate wildly. Every momentâ€™s altitude and sound are time series data.

Characteristics of time series data:
ğŸ‘‰ Massive volume: Depending on your collection needs, you may generate hundreds to thousands of data points per minute, or even more ğŸ”
ğŸ‘‰ Time sensitivity: Data is strictly ordered by timestamp, faithfully recording the actual state of the monitored object
ğŸ‘‰ High write frequency: Monitoring metrics need to be continuously written
ğŸ‘‰ Fixed query patterns: Mainly range queries and aggregation calculations

2ï¸âƒ£ Why do we need time series data?
Because faults and performance issues all have time characteristics!

Essential monitoring needs:
ğŸ‘‰ Trend analysis: Is CPU usage rising or falling?
ğŸ‘‰ Anomaly detection: Has response time suddenly spiked?
ğŸ‘‰ Capacity planning: What is the growth trend of memory usage?
ğŸ‘‰ Incident backtracking: When did the problem start?

3ï¸âƒ£ An example
If someone asks you, "Is the roller coaster exciting?" and you only tell them "I'm at 50 meters right now," they won't feel the thrill. But if you show them the full altitude curve:
00:00 - 2 meters (starting platform), chatting and taking photos
00:30 - 25 meters (slow climb), starting to get nervous, gripping the handrail
01:00 - 80 meters (reached the highest point), heart racing, legs weak when looking down
01:05 - 5 meters (rapid drop!), screams everywhere, scared out of your wits
01:10 - 45 meters (second peak), still recovering, another wave comes
01:15 - 2 meters (back to the end), legs weak getting off ğŸ˜…

Now they can clearly feel the roller coasterâ€™s thrilling trajectory and understand what "heart racing" means!

In production environments, monitored objects also experience similar changes during incidents: running normally -> problem appears -> worsens -> crashes. Time series data faithfully records these changes.

For example, API response time during an incident:
00:00 - 5ms (normal)
00:30 - 10ms (slight increase)
01:00 - 200ms (sharp rise)
01:30 - 1s (continues to worsen)
02:00 - 2s (unreachable)

If the monitoring system doesnâ€™t detect this anomaly and trigger an alert, congratulations: youâ€™ll get customer complaints and a "friendly greeting" from your boss âš¡ï¸

4ï¸âƒ£ Storing time series data
Storing time series data requires considering the following factors:
- Data volume: How many data points are generated per second
- Query patterns: Main query types and frequency
- Data retention: How long data needs to be kept
- Scalability: Is the storage system easy to scale
- Cost: Storage and query costs

This naturally leads to choosing Prometheus as the time series database! In Greek mythology, Prometheus is the god who stole fire for humanity. In monitoring, it brings us powerful time series data processing capabilities.

In our Prometheus scenario:
The Source Instance needs to collect massive amounts of time series data:
- System metrics: Real-time changes in CPU, memory, disk, and network
- Application metrics: Fluctuations in HTTP request count, response time, error rate
- Business metrics: Time distribution of order count, user activity, revenue
- Kubernetes metrics: Dynamic changes in Pod status, Service health, resource usage

The reality of data volume:
Suppose we have 100 services, each exposing 200 metrics, collected every 15 seconds:
100 services Ã— 200 metrics Ã— 4 times/minute = 80,000 data points/minute

One day is: 80,000 Ã— 60 Ã— 24 = 115 million data points!
With Prometheusâ€™s label system, the actual data volume multiplies. For example, an HTTP request metric may have these labels:
http_requests_total{method="GET", status="200", endpoint="/api/users"} 
http_requests_total{method="POST", status="404", endpoint="/api/orders"} 
http_requests_total{method="PUT", status="500", endpoint="/api/products"}

Each label combination is a separate time series! This is the "sweet burden" weâ€™ll analyze laterâ€”the more detailed the data, the more useful it is, but the greater the pressure on transmission and storage...

In short, time series data is the sentinel of the monitoring systemâ€”its importance is self-evident. If itâ€™s missing, your monitoring and KPIs ğŸ’° become unreliable.

ç« èŠ‚10: 
----- Chinese
Prometheusè”é‚¦æ•°æ®ç¼ºå¤±æ•…éšœè§£å†³æ–¹æ¡ˆ ğŸ”§

å¤§å®¶å¥½ï¼å‰é¢ä¸¤ç¯‡æˆ‘ä»¬åˆ†æäº†Prometheusè”é‚¦æ¨¡å¼çš„æ•°æ®ç¼ºå¤±é—®é¢˜å’Œæ—¶åºæ•°æ®çš„ç‰¹ç‚¹ï¼Œä»Šå¤©è®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•ç ´è§£è¿™ä¸ª"ç”œèœœè´Ÿæ‹…"ã€‚

1ï¸âƒ£ é—®é¢˜å®šä½ï¼šä»Prometheus Targeté¡µé¢å‘ç°ç«¯å€ª ğŸ¯

ç»è¿‡æ·±å…¥æ’æŸ¥ï¼Œæˆ‘ä»¬ç»ˆäºæ‰¾åˆ°äº†æ•°æ®ç¼ºå¤±çš„çœŸæ­£åŸå› ï¼
åœ¨Federate Instanceçš„Web UIä¸­æ£€æŸ¥Targeté¡µé¢ï¼Œå‘ç°federation targetçŠ¶æ€å¼‚å¸¸ï¼š
```
Target: http://source-prometheus:9090/federate?match[]={__name__=~".+"}
State: UP
Last Scrape: 118.234s ago  # ğŸš¨ æ¥è¿‘2åˆ†é’Ÿï¼
Scrape Duration: 118.234s  # ğŸš¨ è¶…æ—¶äº†ï¼
Error: context deadline exceeded
```

Targeté¡µé¢æ˜¾ç¤ºçš„å…³é”®ä¿¡æ¯ï¼š
âŒ Federate Instanceçš„federation targetæ˜¾ç¤ºé»„è‰²WARNING
ğŸš¨ Scrape Duration: 118.234sï¼ˆè¿œè¶…60ç§’è¶…æ—¶é™åˆ¶ï¼‰
ğŸ’¥ Last Scrape: æ˜¾ç¤ºä¸Šæ¬¡æˆåŠŸæŠ“å–æ˜¯2åˆ†é’Ÿå‰

2ï¸âƒ£ æ•°æ®é‡åˆ†æï¼šçœŸå®çš„æµ·é‡æ—¶åºæ•°æ®æŒ‘æˆ˜ ğŸ“Š

é€šè¿‡Prometheusçš„Statusé¡µé¢ç¡®è®¤äº†æƒŠäººçš„æ•°æ®é‡ï¼š

TSDB Head Statusæ˜¾ç¤ºï¼š
```
Number of Series: 6,535,837    # ğŸš¨ 650ä¸‡æ—¶é—´åºåˆ—ï¼
Number of Chunks: 16,211,223   # ğŸš¨ 1600ä¸‡æ•°æ®å—ï¼
Number of Label Pairs: 2,026,083  # ğŸš¨ 200ä¸‡æ ‡ç­¾å¯¹ï¼
```

æ•°æ®é‡çº§åˆ†æï¼š
- 6.5Mæ—¶é—´åºåˆ— = æ¯”é¢„æœŸé«˜å‡º2.3å€ï¼
- 16Mæ•°æ®å— = å¹³å‡æ¯ä¸ªåºåˆ—2.5ä¸ªæ´»è·ƒå—
- 2Mæ ‡ç­¾å¯¹ = å¹³å‡æ¯ä¸ªåºåˆ—3.2ä¸ªæ ‡ç­¾

Federationé…ç½®æ£€æŸ¥ï¼š
```yaml
scrape_configs:
  - job_name: 'federation'
    scrape_interval: 60s      # æ¯åˆ†é’ŸæŠ“å–ä¸€æ¬¡
    scrape_timeout: 60s       # 60ç§’è¶…æ—¶ âš ï¸
    params:
      'match[]':
        - '{__name__=~".+"}'   # æŠ“å–æ‰€æœ‰650ä¸‡åºåˆ—ï¼âš ï¸
```

3ï¸âƒ£ å®æ—¶æ€§è¦æ±‚ï¼šåˆ†é’Ÿçº§åŒæ­¥çš„æŒ‘æˆ˜ âš¡

ä¸šåŠ¡éœ€æ±‚åˆ†æï¼š
- ğŸ“ˆ Grafanaå®æ—¶ä»ªè¡¨æ¿ï¼šéœ€è¦1åˆ†é’Ÿå†…çš„æœ€æ–°æ•°æ®
- ğŸš¨ å‘Šè­¦ç³»ç»Ÿï¼šä¾èµ–åˆ†é’Ÿçº§æŒ‡æ ‡è§¦å‘
- ğŸ“Š SLAç›‘æ§ï¼šå®æ—¶è®¡ç®—å¯ç”¨æ€§æŒ‡æ ‡

çŸ›ç›¾ç‚¹ï¼š
```
é…ç½®è¦æ±‚ï¼šæ¯60ç§’åŒæ­¥ä¸€æ¬¡æ•°æ®
æ•°æ®ç°å®ï¼š6.5Mæ—¶é—´åºåˆ—éœ€è¦ä¼ è¾“
å®é™…è€—æ—¶ï¼š/federateä¼ è¾“éœ€è¦118ç§’
è¶…æ—¶è®¾ç½®ï¼š60ç§’åå¼ºåˆ¶æ–­å¼€è¿æ¥
ç»“æœï¼šåªä¼ è¾“äº†çº¦50%çš„æ•°æ®ï¼Œå…¶ä½™å…¨éƒ¨ä¸¢å¤±ï¼
```

Federation vs Remote Writeçš„æ¶æ„å·®å¼‚ï¼š
- Federationæ–¹å¼ï¼šéœ€è¦ä»TSDBè¯»å–â†’å†…å­˜åŠ è½½â†’APIå¤„ç†â†’åºåˆ—åŒ–â†’ç½‘ç»œä¼ è¾“ï¼ˆå¤šå±‚ä¸­è½¬ï¼‰
- Remote Writeæ–¹å¼ï¼šå†…å­˜é˜Ÿåˆ—â†’ç›´æ¥ç½‘ç»œä¼ è¾“â†’ç›´æ¥å†™å…¥ç›®æ ‡TSDBï¼ˆç›´æ¥æ•°æ®åº“æ“ä½œï¼‰

4ï¸âƒ£ è§£å†³æ–¹æ¡ˆï¼šRemote Writeæ‹¯æ•‘ä¸–ç•Œ ğŸš€

é¢å¯¹650ä¸‡æ—¶é—´åºåˆ—çš„æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ‰¾åˆ°äº†å®Œç¾çš„è§£å†³æ–¹æ¡ˆï¼šRemote Writeï¼

æ–¹æ¡ˆå¯¹æ¯”ï¼š
| ç‰¹æ€§ | /federate | Remote Write |
|------|-----------|--------------|
| æ•°æ®æµ | å¤šå±‚ä¸­è½¬ï¼Œéœ€è¯»å–TSDB | ç›´æ¥æ¨é€åˆ°ç›®æ ‡æ•°æ®åº“ |
| 6.5Måºåˆ—å¤„ç† | 118ç§’ï¼ˆè¶…æ—¶å¤±è´¥ï¼‰ | 3.2ç§’ï¼ˆæˆåŠŸï¼‰ |
| æ•°æ®æ ¼å¼ | æ–‡æœ¬æ ¼å¼ï¼Œæ— å‹ç¼© | Protocol Buffersï¼Œ70%å‹ç¼© |
| å¹¶å‘èƒ½åŠ› | å•çº¿ç¨‹APIå¤„ç† | å¤šé˜Ÿåˆ—å¹¶å‘å¤„ç† |

Remote Writeé…ç½®ä¼˜åŒ–ï¼š
```yaml
remote_write:
  - url: "http://federate-prometheus:9090/api/v1/write"
    queue_config:
      capacity: 50000             # å¤§å®¹é‡åº”å¯¹650ä¸‡åºåˆ—
      max_shards: 500            # é«˜å¹¶å‘åˆ†ç‰‡æ•°
      max_samples_per_send: 5000 # æ‰¹é‡å‘é€ä¼˜åŒ–
      batch_send_deadline: 1s    # å¿«é€Ÿå‘é€ä¿è¯å®æ—¶æ€§
    write_relabel_configs:
      # è¿‡æ»¤é«˜åŸºæ•°æŒ‡æ ‡ï¼Œå‡å°‘æ•°æ®é‡
      - source_labels: [__name__]
        regex: '^(prometheus_|go_|process_).*'
        action: drop
```

æ€§èƒ½æå‡æ•ˆæœï¼š
```bash
æ•°æ®åŒæ­¥å»¶è¿Ÿï¼š
Federation: 118ç§’ â†’ Remote Write: 3.2ç§’ (â¬‡ï¸ 97%æ”¹å–„)

æ•°æ®å®Œæ•´æ€§ï¼š
Federation: 50% â†’ Remote Write: 99.97% (â¬†ï¸ 99%æ”¹å–„)

ç³»ç»Ÿç¨³å®šæ€§ï¼š
Federation: æ¯æ—¥OOM 3-5æ¬¡ â†’ Remote Write: è¿ç»­è¿è¡Œ30å¤©æ— æ•…éšœ
```

5ï¸âƒ£ æ•…éšœå¤ç›˜ä¸æ€è€ƒ ğŸ¤”

ç»éªŒæ•™è®­ï¼š
- ğŸ“Š æ•°æ®é‡è¯„ä¼°è¦å‡†ç¡®ï¼š650ä¸‡åºåˆ—è¿œè¶…é¢„æœŸï¼Œéœ€è¦ä¸“é—¨çš„è§£å†³æ–¹æ¡ˆ
- ğŸ¯ Targeté¡µé¢æ˜¯è¯Šæ–­åˆ©å™¨ï¼šç›´è§‚æ˜¾ç¤º118ç§’çš„å¼‚å¸¸è€—æ—¶
- ğŸš€ æ¶æ„é€‰æ‹©è¦åŒ¹é…è§„æ¨¡ï¼šFederationé€‚åˆ<100ä¸‡åºåˆ—ï¼ŒRemote Writeé€‚åˆç™¾ä¸‡çº§ä»¥ä¸Š
- ğŸ”§ æ•°æ®æµè®¾è®¡å¾ˆå…³é”®ï¼šç›´æ¥æ•°æ®åº“æ“ä½œæ¯”APIä¸­è½¬æ•ˆç‡é«˜10å€ä»¥ä¸Š

æ¶æ„æ¼”è¿›æ€»ç»“ï¼š
```
é˜¶æ®µ1ï¼šå•æœºPrometheus â†’ å®¹é‡ç“¶é¢ˆ
é˜¶æ®µ2ï¼šFederationæ¶æ„ â†’ ä¼ è¾“ç“¶é¢ˆï¼ˆTargeté¡µé¢å‘ç°ï¼‰
é˜¶æ®µ3ï¼šRemote Write â†’ å®Œç¾è§£å†³ âœ…
```

å„ä½SREå°ä¼™ä¼´ä»¬ï¼Œé¢å¯¹å¤§è§„æ¨¡æ—¶åºæ•°æ®åŒæ­¥ï¼Œè®°ä½è¦é€‰æ‹©åˆé€‚çš„æ¶æ„æ–¹æ¡ˆã€‚Targeté¡µé¢æ˜¯ä½ æ’æŸ¥é—®é¢˜çš„ç¬¬ä¸€ç«™ï¼Œæ•°æ®æµçš„è®¾è®¡å†³å®šäº†ç³»ç»Ÿçš„æ€§èƒ½ä¸Šé™ï¼

è®°ä½ï¼šè§„æ¨¡å†³å®šæ¶æ„ï¼Œç›´æ¥æ•°æ®åº“æ“ä½œæ°¸è¿œæ¯”APIä¸­è½¬æ›´é«˜æ•ˆï¼ğŸ¯

----- English

Prometheus Federation Data Loss Troubleshooting Solution ğŸ”§

Hello everyone! In the previous two articles, we analyzed the data loss issues in Prometheus federation mode and the characteristics of time series data. Today, let's see how to solve this "sweet burden."

1ï¸âƒ£ Issue Identification: Discovering Clues from Prometheus Target Page ğŸ¯

After thorough investigation, we finally found the real cause of data loss!
Checking the Target page in the Federate Instance's Web UI revealed abnormal federation target status:
```
Target: http://source-prometheus:9090/federate?match[]={__name__=~".+"}
State: UP
Last Scrape: 118.234s ago  # ğŸš¨ Nearly 2 minutes!
Scrape Duration: 118.234s  # ğŸš¨ Timeout!
Error: context deadline exceeded
```

Key information displayed on Target page:
âŒ Federate Instance's federation target shows yellow WARNING
ğŸš¨ Scrape Duration: 118.234s (far exceeding 60-second timeout limit)
ğŸ’¥ Last Scrape: Shows last successful scrape was 2 minutes ago

2ï¸âƒ£ Data Volume Analysis: Real Massive Time Series Data Challenge ğŸ“Š

Confirmed the staggering data volume through Prometheus Status page:

TSDB Head Status shows:
```
Number of Series: 6,535,837    # ğŸš¨ 6.5 million time series!
Number of Chunks: 16,211,223   # ğŸš¨ 16 million data chunks!
Number of Label Pairs: 2,026,083  # ğŸš¨ 2 million label pairs!
```

Data scale analysis:
- 6.5M time series = 2.3x higher than expected!
- 16M data chunks = average 2.5 active chunks per series
- 2M label pairs = average 3.2 labels per series

Federation configuration check:
```yaml
scrape_configs:
  - job_name: 'federation'
    scrape_interval: 60s      # Scrape every minute
    scrape_timeout: 60s       # 60-second timeout âš ï¸
    params:
      'match[]':
        - '{__name__=~".+"}'   # Scraping all 6.5M series! âš ï¸
```

3ï¸âƒ£ Real-time Requirements: Minute-level Synchronization Challenge âš¡

Business requirements analysis:
- ğŸ“ˆ Grafana real-time dashboards: Need latest data within 1 minute
- ğŸš¨ Alert system: Depends on minute-level metrics triggering
- ğŸ“Š SLA monitoring: Real-time availability metric calculation

Contradiction:
```
Configuration requirement: Sync data every 60 seconds
Data reality: 6.5M time series need transmission
Actual time taken: /federate transmission takes 118 seconds
Timeout setting: Force disconnect after 60 seconds
Result: Only ~50% of data transmitted, rest completely lost!
```

Federation vs Remote Write architectural differences:
- Federation approach: TSDB read â†’ memory load â†’ API processing â†’ serialization â†’ network transmission (multi-layer relay)
- Remote Write approach: memory queue â†’ direct network transmission â†’ direct write to target TSDB (direct database operation)

4ï¸âƒ£ Solution: Remote Write Saves the World ğŸš€

Facing the challenge of 6.5 million time series, we found the perfect solution: Remote Write!

Solution comparison:
| Feature | /federate | Remote Write |
|---------|-----------|--------------|
| Data flow | Multi-layer relay, requires TSDB read | Direct push to target database |
| 6.5M series processing | 118s (timeout failure) | 3.2s (success) |
| Data format | Text format, no compression | Protocol Buffers, 70% compression |
| Concurrency | Single-threaded API processing | Multi-queue concurrent processing |

Remote Write configuration optimization:
```yaml
remote_write:
  - url: "http://federate-prometheus:9090/api/v1/write"
    queue_config:
      capacity: 50000             # Large capacity for 6.5M series
      max_shards: 500            # High concurrency shard count
      max_samples_per_send: 5000 # Batch send optimization
      batch_send_deadline: 1s    # Fast send ensuring real-time
    write_relabel_configs:
      # Filter high-cardinality metrics to reduce data volume
      - source_labels: [__name__]
        regex: '^(prometheus_|go_|process_).*'
        action: drop
```

Performance improvement results:
```bash
Data sync latency:
Federation: 118s â†’ Remote Write: 3.2s (â¬‡ï¸ 97% improvement)

Data integrity:
Federation: 50% â†’ Remote Write: 99.97% (â¬†ï¸ 99% improvement)

System stability:
Federation: 3-5 OOM daily â†’ Remote Write: 30 days continuous operation without failure
```

5ï¸âƒ£ Incident Review and Reflection ğŸ¤”

Lessons learned:
- ğŸ“Š Accurate data volume assessment: 6.5M series far exceeded expectations, requiring specialized solutions
- ğŸ¯ Target page is a diagnostic tool: Intuitively shows abnormal 118-second duration
- ğŸš€ Architecture choice must match scale: Federation suitable for <1M series, Remote Write for million+ scale
- ğŸ”§ Data flow design is crucial: Direct database operations are 10x more efficient than API relay

Architecture evolution summary:
```
Stage 1: Single Prometheus â†’ Capacity bottleneck
Stage 2: Federation architecture â†’ Transmission bottleneck (discovered via Target page)
Stage 3: Remote Write â†’ Perfect solution âœ…
```

Fellow SRE engineers, when facing large-scale time series data synchronization, remember to choose the appropriate architectural solution. The Target page is your first stop for troubleshooting, and data flow design determines your system's performance ceiling!

Remember: Scale determines architecture, direct database operations are always more efficient than API relay! ğŸ¯
