Sep 7, 2025
----- Chinese
æ¢ç´¢SREï¼šç«™åœ¨å¯é æ€§çš„æœ€å‰çº¿
å¤§å®¶å¥½ï¼Œæˆ‘æ˜¯ä¸€åSREï¼Œä¹Ÿå°±æ˜¯â€œç«™ç‚¹å¯é æ€§å·¥ç¨‹å¸ˆâ€ï¼ˆSite Reliability Engineerï¼‰ã€‚è¯´ç™½äº†ï¼Œæˆ‘çš„å·¥ä½œå°±æ˜¯è®©å„ç§ç½‘ç«™ã€Appå’ŒæœåŠ¡ä¸å‡ºé—®é¢˜ï¼Œèƒ½ç¨³å®šã€é«˜æ•ˆåœ°è¿è¡Œã€‚æˆ‘èº«åæ²¡æœ‰æŠ«é£ï¼Œä½†æˆ‘è§‰å¾—è‡ªå·±æ˜¯æŸç§æ„ä¹‰ä¸Šçš„â€œå¹•åè‹±é›„â€ï¼Œå› ä¸ºæˆ‘çš„ä»»åŠ¡å°±æ˜¯æ‹¯æ•‘ç³»ç»Ÿå´©æºƒï¼Œä¿éšœç”¨æˆ·ä½“éªŒã€‚

ä½ å¯èƒ½ä¼šå¥½å¥‡ï¼ŒSREåˆ°åº•æ˜¯ä¸ªä»€ä¹ˆæ ·çš„èŒä¸šï¼Ÿç®€å•æ¥è¯´ï¼Œæˆ‘ä»¬å°±åƒæ˜¯æ•°å­—ä¸–ç•Œçš„"æ€¥è¯ŠåŒ»ç”Ÿ"åŠ "å»ºç­‘å¸ˆ"çš„ç»“åˆä½“ã€‚å½“ç³»ç»Ÿå¥åº·çš„æ—¶å€™ï¼Œæˆ‘ä»¬åƒå»ºç­‘å¸ˆä¸€æ ·è®¾è®¡æ›´ç¨³å›ºçš„æ¶æ„ï¼›å½“ç³»ç»Ÿå‡ºé—®é¢˜çš„æ—¶å€™ï¼Œæˆ‘ä»¬åˆåƒæ€¥è¯ŠåŒ»ç”Ÿä¸€æ ·å¿«é€Ÿè¯Šæ–­ã€ç´§æ€¥æŠ¢æ•‘ã€‚æ¯å¤©çš„å·¥ä½œéƒ½å……æ»¡äº†æŒ‘æˆ˜ï¼Œä½†ä¹Ÿç‰¹åˆ«æœ‰æˆå°±æ„Ÿã€‚

é‚£ä¹ˆï¼Œæˆ‘æ¯å¤©åˆ°åº•åœ¨åšä»€ä¹ˆå‘¢ï¼Ÿè·Ÿæˆ‘ä¸€èµ·çœ‹çœ‹å§ï¼
	
å¯é æ€§ï¼šSREçš„æ ¸å¿ƒä½¿å‘½
ä½œä¸ºSREï¼Œæˆ‘æœ€é‡è¦çš„èŒè´£å°±æ˜¯è®©ç³»ç»Ÿâ€œé è°±â€ã€‚æ¯”å¦‚ï¼Œå½“ä½ æ·±å¤œç‚¹å¤–å–ã€ç”¨Appä¹°ç”µå½±ç¥¨æˆ–è€…åˆ·çŸ­è§†é¢‘æ—¶ï¼Œè¿™äº›æœåŠ¡éƒ½å¾—æ­£å¸¸è¿è¡Œï¼Œä¸ç„¶ä½ å¯èƒ½ä¼šæŠ“ç‹‚ï¼Œå¯¹å§ï¼Ÿä¸ºäº†åšåˆ°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä¼šè®¾å®šä¸€äº›ç›®æ ‡ï¼Œæ¯”å¦‚â€œç³»ç»Ÿ99.9%çš„æ—¶é—´éƒ½è¦åœ¨çº¿â€ï¼Œç„¶åé€šè¿‡ä¼˜åŒ–æ¶æ„ã€ä¿®å¤æ¼æ´ï¼Œæ¥è®©ç³»ç»Ÿå˜å¾—ç¨³å®šåˆåšå¼ºã€‚

ä½ çŸ¥é“å—ï¼Ÿ99.9%å¬èµ·æ¥å¾ˆé«˜ï¼Œä½†å…¶å®æ„å‘³ç€ä¸€å¹´ä¸­ç³»ç»Ÿå¯ä»¥å®•æœº8.76å°æ—¶ã€‚å¯¹äºä¸€äº›å…³é”®ä¸šåŠ¡æ¥è¯´ï¼Œè¿™å¯èƒ½è¿˜ä¸å¤Ÿï¼Œæ‰€ä»¥æˆ‘ä»¬ç»å¸¸è¿½æ±‚99.99%ç”šè‡³æ›´é«˜çš„å¯ç”¨æ€§ã€‚è¿™å°±åƒæ˜¯åœ¨èµ°é’¢ä¸ï¼Œæ—¢è¦ä¿è¯ç³»ç»Ÿç¨³å®šï¼Œåˆä¸èƒ½è¿‡åº¦å·¥ç¨‹åŒ–æµªè´¹èµ„æºã€‚
	
ç›‘æ§ï¼šSREçš„åƒé‡Œçœ¼
æˆ‘çš„æ—¥å¸¸ç¦»ä¸å¼€â€œç›‘æ§â€ï¼ˆMonitoringï¼‰ã€‚æˆ‘ä¼šç”¨ä¸€äº›é…·ç‚«çš„å·¥å…·ï¼Œæ¯”å¦‚Prometheuså’ŒGrafanaï¼Œè¿˜æœ‰å…¶ä»–ä¸€ç³»åˆ—çš„å·¥å…·æ¥ç›¯ç€ç³»ç»Ÿçš„è¿è¡ŒçŠ¶æ€ã€‚æ¯”å¦‚ï¼Œæµé‡æ˜¯ä¸æ˜¯çªç„¶é£™å‡äº†ï¼ŸæœåŠ¡å™¨æ˜¯ä¸æ˜¯å¼€å§‹å‘çƒ­äº†ï¼Ÿä¸€æ—¦å‘ç°å¼‚å¸¸ï¼Œæˆ‘å°±èƒ½ç¬¬ä¸€æ—¶é—´å‡ºæ‰‹ï¼Œé˜²æ­¢é—®é¢˜æ‰©å¤§ã€‚å¯ä»¥è¯´ï¼Œç›‘æ§å·¥å…·æ˜¯æˆ‘çš„çœ¼ç›ï¼Œå¸®æˆ‘éšæ—¶æŒæ¡ç³»ç»Ÿçš„å¥åº·çŠ¶æ€ã€‚

ä½ å¯èƒ½ä¼šé—®ï¼Œç›‘æ§åˆ°åº•åœ¨ç›‘æ§ä»€ä¹ˆï¼Ÿå…¶å®å†…å®¹å¯ä¸°å¯Œäº†ï¼ä»æœ€åŸºç¡€çš„CPUã€å†…å­˜ä½¿ç”¨ç‡ï¼Œåˆ°åº”ç”¨å±‚é¢çš„å“åº”æ—¶é—´ã€é”™è¯¯ç‡ï¼Œå†åˆ°ä¸šåŠ¡æŒ‡æ ‡æ¯”å¦‚è®¢å•é‡ã€ç”¨æˆ·æ´»è·ƒåº¦ï¼Œæˆ‘éƒ½è¦å…³æ³¨ã€‚æœ‰æ—¶å€™æˆ‘è§‰å¾—è‡ªå·±åƒä¸ª"æ•°å­—ä¾¦æ¢"ï¼Œé€šè¿‡å„ç§å›¾è¡¨å’Œæ•°æ®æ¥æ¨ç†ç³»ç»Ÿçš„å¥åº·çŠ¶å†µã€‚æœ€æœ‰æ„æ€çš„æ˜¯ï¼Œæœ‰æ—¶å€™ä¸€ä¸ªå°å°çš„å¼‚å¸¸æ³¢åŠ¨ï¼Œå°±èƒ½å¸®æˆ‘æå‰å‘ç°å¤§é—®é¢˜ï¼Œè¿™ç§æ„Ÿè§‰å°±åƒç ´æ¡ˆä¸€æ ·åˆºæ¿€ï¼
	
è‡ªåŠ¨åŒ–ï¼šè§£æ”¾åŒæ‰‹çš„ç§˜è¯€
è¯´å®è¯ï¼Œé‡å¤å¹²åŒæ ·çš„äº‹æƒ…çœŸçš„å¾ˆæ— èŠã€‚æ‰€ä»¥ï¼ŒSREçš„åº§å³é“­ä¹‹ä¸€å°±æ˜¯â€œèƒ½è‡ªåŠ¨åŒ–çš„ç»ä¸æ‰‹åŠ¨â€ã€‚æˆ‘çš„å·¥ä½œä¹‹ä¸€å°±æ˜¯å†™è„šæœ¬å’Œç¨‹åºï¼Œè®©ç³»ç»Ÿè‡ªåŠ¨å¤„ç†ä¸€äº›å¸¸è§é—®é¢˜ï¼Œæ¯”å¦‚è‡ªåŠ¨æ‰©å±•æœåŠ¡å™¨å®¹é‡ï¼Œè‡ªåŠ¨é‡å¯æœåŠ¡ï¼Œç”šè‡³è‡ªåŠ¨é¢„è­¦ã€‚æœ‰äº†è‡ªåŠ¨åŒ–ï¼Œæˆ‘å¯ä»¥æŠŠæ—¶é—´èŠ±åœ¨æ›´é‡è¦ã€æ›´æœ‰æŒ‘æˆ˜çš„äº‹æƒ…ä¸Šã€‚

AIæ—¶ä»£çš„åˆ°æ¥ï¼Œè‡ªåŠ¨åŒ–åˆæœ‰äº†æ–°çš„èµ‹èƒ½ã€‚è™½ç„¶æˆ‘ä»¬ç°åœ¨è¿˜ä¸èƒ½å®Œå…¨åšåˆ°è®©ç³»ç»Ÿ"å­¦ä¼šæ€è€ƒ"ï¼Œä½†æˆ‘ä»¬æ­£åœ¨ç§¯æå­¦ä¹ å’Œæ¢ç´¢è¿™ä¸ªæ–¹å‘ã€‚æ¯”å¦‚ï¼Œæˆ‘ä»¬å¼€å§‹å°è¯•ç”¨AIæ¥åˆ†æå†å²æ•…éšœæ¨¡å¼ï¼Œè™½ç„¶è¿˜åœ¨æ‘¸ç´¢é˜¶æ®µï¼›æˆ‘ä»¬ä¹Ÿåœ¨ç ”ç©¶å¦‚ä½•æ ¹æ®ä¸šåŠ¡æµé‡è¶‹åŠ¿è¿›è¡Œæ›´æ™ºèƒ½çš„èµ„æºè°ƒæ•´ï¼›ç”šè‡³å¼€å§‹å®éªŒä»æµ·é‡æ—¥å¿—ä¸­è¯†åˆ«å¼‚å¸¸æ¨¡å¼çš„å¯èƒ½æ€§ã€‚è¿™æ¡ä»"è¢«åŠ¨å“åº”"åˆ°"ä¸»åŠ¨é¢„é˜²"çš„è·¯è¿˜å¾ˆé•¿ï¼Œä½†æˆ‘ä»¬æ­£ç´§è·Ÿè¿™ä¸ªè¶‹åŠ¿ï¼Œä¸€æ­¥æ­¥å‘"æ™ºèƒ½åŒ–è‡ªåŠ¨åŒ–"çš„ç›®æ ‡è¿ˆè¿›ï¼
	
å¯æ‰©å±•æ€§ï¼šä»å®¹åº”å¯¹é«˜å³°
æˆ‘ç»å¸¸è·Ÿæœ‹å‹å¼€ç©ç¬‘è¯´ï¼Œæˆ‘çš„å·¥ä½œå°±æ˜¯è®©â€œåŒåä¸€â€ä¸å®•æœºã€‚æ¯æ¬¡å¤§ä¿ƒã€ç›´æ’­çˆ†ç«æ—¶ï¼Œæµé‡éƒ½ä¼šæš´æ¶¨ï¼Œè¿™å¯¹ç³»ç»Ÿæ˜¯ä¸ªå¤§è€ƒéªŒã€‚è€Œæˆ‘çš„ä»»åŠ¡å°±æ˜¯æå‰è®¾è®¡å¥½ç³»ç»Ÿï¼Œè®©å®ƒèƒ½çµæ´»æ‰©å±•ï¼Œæ‰›ä½å‹åŠ›ã€‚çœ‹åˆ°ç”¨æˆ·ä¹°ä¹°ä¹°ã€åˆ·åˆ·åˆ·æ—¶ï¼Œæˆ‘éƒ½è§‰å¾—ç‰¹åˆ«éª„å‚²ï¼Œå› ä¸ºè¿™èƒŒåæœ‰æˆ‘çš„åŠªåŠ›ã€‚

å¯æ‰©å±•æ€§è¿™ä¸ªè¯å¬èµ·æ¥å¾ˆé«˜å¤§ä¸Šï¼Œå…¶å®å°±æ˜¯è®©ç³»ç»Ÿèƒ½å¤Ÿ"å¼¹æ€§ä¼¸ç¼©"ã€‚å°±åƒæˆ‘ä»¬å¹³æ—¶å«å¤–å–ä¸€æ ·ï¼Œå¹³å¸¸æ—¶å€™å‡ ä¸ªéª‘æ‰‹å°±å¤Ÿäº†ï¼Œä½†ä¸€åˆ°é¥­ç‚¹æˆ–è€…ä¸‹é›¨å¤©ï¼Œè®¢å•æš´å¢ï¼Œå¹³å°å°±ä¼šä¸´æ—¶è°ƒåŠ¨æ›´å¤šéª‘æ‰‹æ¥é€é¤ã€‚ç³»ç»Ÿä¹Ÿæ˜¯è¿™æ ·ï¼Œå¹³æ—¶å‡ å°æœåŠ¡å™¨å°±èƒ½åº”ä»˜ï¼Œä½†æµé‡é«˜å³°æ—¶å°±è‡ªåŠ¨"å¬å”¤"æ›´å¤šæœåŠ¡å™¨æ¥å¸®å¿™ï¼Œå¿™å®Œäº†å†"è§£æ•£"ã€‚æœ€æœ‰æˆå°±æ„Ÿçš„æ˜¯ï¼Œå½“ç”¨æˆ·åœ¨è´­ç‰©èŠ‚ç–¯ç‹‚ä¸‹å•æ—¶ï¼Œç³»ç»Ÿä¾ç„¶ä¸æ»‘æµç•…ï¼Œé‚£ç§æ„Ÿè§‰å°±åƒæ˜¯æˆåŠŸç»„ç»‡äº†ä¸€åœºå®Œç¾çš„"åŒåä¸€"ï¼
	
äº‹ä»¶ç®¡ç†ï¼šå±æœºä¸­çš„å†·é™å¤§è„‘
å½“ç„¶ï¼Œå¤©æœ‰ä¸æµ‹é£äº‘ï¼Œå†å®Œç¾çš„ç³»ç»Ÿä¹Ÿæœ‰å¯èƒ½çªç„¶å´©äº†ã€‚è¿™ä¸ªæ—¶å€™ï¼Œæˆ‘å°±å˜æˆäº†â€œå±æœºå¤„ç†ä¸“å®¶â€ã€‚å½“è­¦æŠ¥å“èµ·ï¼Œæˆ‘ä¼šè¿…é€Ÿå®šä½é—®é¢˜ï¼Œæƒ³åŠæ³•æ¢å¤æœåŠ¡ï¼ŒåŒæ—¶ç»„ç»‡å›¢é˜Ÿæ€»ç»“å¤ç›˜ï¼Œé¿å…åŒæ ·çš„é”™è¯¯å†æ¬¡å‘ç”Ÿã€‚è™½ç„¶ç´§å¼ ï¼Œä½†æ¯æ¬¡æˆåŠŸè§£å†³é—®é¢˜åï¼Œé‚£ç§æˆå°±æ„ŸçœŸçš„è¶…çˆ½ã€‚

äº‹ä»¶ç®¡ç†å…¶å®å°±åƒæ˜¯å½“"æ•°å­—ä¸–ç•Œçš„æ€¥è¯ŠåŒ»ç”Ÿ"ã€‚å½“ç³»ç»Ÿ"ç—…å€’"çš„æ—¶å€™ï¼Œæˆ‘éœ€è¦åœ¨æœ€çŸ­æ—¶é—´å†…è¯Šæ–­ç—…å› ã€å¼€å‡ºè¯æ–¹ã€å®æ–½æ²»ç–—ã€‚æœ‰æ—¶å€™æ˜¯"æ„Ÿå†’å‘çƒ§"ï¼ˆå°æ•…éšœï¼‰ï¼Œæœ‰æ—¶å€™æ˜¯"å¿ƒè„éª¤åœ"ï¼ˆç³»ç»Ÿå´©æºƒï¼‰ï¼Œä¸åŒçš„"ç—…æƒ…"éœ€è¦ä¸åŒçš„"æ²»ç–—æ–¹æ¡ˆ"ã€‚æœ€å…³é”®çš„æ˜¯ä¿æŒå†·é™ï¼Œå› ä¸ºæ…Œä¹±åªä¼šè®©é—®é¢˜å˜å¾—æ›´ç³Ÿã€‚æ¯æ¬¡åŒ–é™©ä¸ºå¤·åï¼Œæˆ‘éƒ½ä¼šè¯¦ç»†è®°å½•æ•´ä¸ªè¿‡ç¨‹ï¼Œè¿™æ ·ä¸‹æ¬¡é‡åˆ°ç±»ä¼¼é—®é¢˜æ—¶ï¼Œå°±èƒ½æ›´å¿«æ›´å‡†åœ°"ä¸‹è¯"äº†ã€‚
	
è¯´çœŸçš„ï¼ŒSREè¿™ä¸ªèŒä¸šå¯¹æˆ‘æ¥è¯´ä¸åªæ˜¯å·¥ä½œï¼Œæ›´æ˜¯ä¸€ç§æŒ‘æˆ˜å’Œä¹è¶£ã€‚æˆ‘æ—¢èƒ½åŠ¨æ‰‹å†™ä»£ç ï¼Œåˆèƒ½è®¾è®¡ç³»ç»Ÿï¼Œè¿˜èƒ½å¿«é€Ÿè§£å†³é—®é¢˜ï¼Œæ¯å¤©éƒ½å……æ»¡æ–°é²œæ„Ÿã€‚

å›æƒ³èµ·åˆšå…¥è¡Œçš„æ—¶å€™ï¼Œæˆ‘ä»¥ä¸ºSREå°±æ˜¯ä¸ª"æ•‘ç«é˜Ÿå‘˜"ï¼Œå“ªé‡Œæœ‰é—®é¢˜å°±å¾€å“ªé‡Œå†²ã€‚ä½†éšç€ç»éªŒçš„ç§¯ç´¯ï¼Œæˆ‘å‘ç°SREæ›´åƒæ˜¯ä¸€ä¸ª"ç³»ç»ŸåŒ»ç”Ÿ"å…¼"å»ºç­‘å¸ˆ"çš„è§’è‰²ã€‚æˆ‘ä»¬ä¸ä»…è¦æ²»ç—…æ•‘äººï¼Œæ›´è¦ä»æ ¹æœ¬ä¸Šè®¾è®¡å‡ºæ›´å¥åº·ã€æ›´å¼ºå£®çš„ç³»ç»Ÿã€‚æ¯å½“çœ‹åˆ°è‡ªå·±å‚ä¸è®¾è®¡çš„ç³»ç»Ÿç¨³å®šè¿è¡Œï¼Œä¸ºåƒä¸‡ç”¨æˆ·æä¾›æœåŠ¡æ—¶ï¼Œé‚£ç§æˆå°±æ„Ÿæ˜¯æ— æ³•ç”¨è¨€è¯­å½¢å®¹çš„ã€‚

è¿™å°±æ˜¯æˆ‘çš„SREä¸–ç•Œï¼Œä¸€ä¸ªå……æ»¡æŒ‘æˆ˜ä½†åˆæå…¶æœ‰è¶£çš„æŠ€æœ¯é¢†åŸŸã€‚å¦‚æœä½ ä¹Ÿå¯¹è¿™ä¸ªé¢†åŸŸæ„Ÿå…´è¶£ï¼Œæ¬¢è¿éšæ—¶äº¤æµæ¢è®¨ï¼

æ¥ä¸‹æ¥ï¼Œæˆ‘ä¼šæ€»ç»“å¹¶åˆ†äº«SREæ—¥å¸¸å·¥ä½œå†…å®¹ï¼ŒåŒæ—¶æ¬¢è¿å¤§å®¶å’Œæˆ‘ä¸€èµ·äº’åŠ¨ï¼Œäº’ç›¸äº¤æµ ï¼šï¼‰

----- English 
Exploring SRE: Standing at the Forefront of Reliability
Hi everyone, I'm an SRE, which stands for "Site Reliability Engineer." Simply put, my job is to ensure that various websites, apps, and services don't break down and can run stably and efficiently. I may not wear a cape, but I consider myself a "behind-the-scenes hero" in some sense, because my mission is to rescue system crashes and ensure user experience. So, what exactly do I do every day? Let's take a look together!

Reliability: The Core Mission of SRE
As an SRE, my most important responsibility is to make systems "reliable." For example, when you order takeout late at night, buy movie tickets through an app, or scroll through short videos, these services must run normally, otherwise you might go crazy, right? To achieve this, I set some goals, like "the system must be online 99.9% of the time," and then optimize architecture and fix vulnerabilities to make the system stable and robust.

Monitoring: SRE's All-Seeing Eyes
My daily work is inseparable from "monitoring." I use some cool tools like Prometheus and Grafana, along with other series of tools to keep an eye on the system's operational status. For instance, is traffic suddenly spiking? Are the servers starting to overheat? Once I detect anomalies, I can take action immediately to prevent problems from escalating. You could say that monitoring tools are my eyes, helping me stay on top of the system's health status at all times.

Automation: The Secret to Freeing Your Hands
Honestly, doing the same repetitive tasks is really boring. So, one of SRE's mottos is "automate everything that can be automated, never do it manually." Part of my job is writing scripts and programs to let the system automatically handle common issues, such as automatically scaling server capacity, automatically restarting services, and even automatic alerts. With automation, I can spend time on more important and challenging tasks.

Scalability: Handling Peak Traffic with Ease
I often joke with friends that my job is to prevent systems during events like Black Friday from crashing. Every time there are major promotions or live streams go viral, traffic surges dramatically, which is a major test for the system. My task is to design the system in advance so it can scale flexibly and withstand the pressure. When I see users shopping and browsing frantically, I feel particularly proud because my efforts are behind all of this.

Incident Management: The Calm Mind in Crisis
Of course, unexpected things happen, and even the most perfect systems can suddenly crash. At times like these, I become a "crisis management expert." When alarms go off, I quickly locate problems, find ways to restore services, while organizing the team to summarize and review to prevent the same mistakes from happening again. Although it's stressful, the sense of achievement after successfully solving problems is absolutely amazing.

Honestly, the SRE profession is not just a job for me, but also a challenge and source of enjoyment. I can both write code and design systems, as well as solve problems quickly. Every day is full of fresh experiences.

Next, I will summarize and share the daily work content of SRE, and I welcome everyone to interact and exchange ideas with me :)

Sep 9,2025
----- Chinese
æ¢ç´¢SREï¼šK8Sé›†ç¾¤PODè°ƒåº¦é—®é¢˜æè¿°
å¤§å®¶å¥½ï¼Œä»Šå¤©åˆ†äº«ä¸€ä¸ªæ—¥å¸¸å·¥ä½œä¸­é‡åˆ°çš„äº§çº¿é—®é¢˜ï¼Œæ¬¢è¿å¤§å®¶ä¸€èµ·å¤´è„‘é£æš´ã€‚è¿™æ˜¯ä¸€ä¸ªå…¸å‹çš„Kubernetesè°ƒåº¦å™¨è¡Œä¸ºå¯¼è‡´çš„èµ„æºåˆ†é…ä¸å‡é—®é¢˜ï¼Œçœ‹ä¼¼ç®€å•çš„Podå‰¯æœ¬éƒ¨ç½²ï¼Œå´å› ä¸ºè°ƒåº¦ç­–ç•¥çš„ç¼ºå¤±å¼•å‘äº†ä¸¥é‡çš„ç”Ÿäº§äº‹æ•…ã€‚è¿™ä¸ªæ¡ˆä¾‹å……åˆ†å±•ç¤ºäº†K8sé»˜è®¤è°ƒåº¦è¡Œä¸ºçš„æ½œåœ¨é£é™©ï¼Œä»¥åŠåœ¨é«˜å¹¶å‘å¤šçº¿ç¨‹åº”ç”¨åœºæ™¯ä¸‹ï¼Œå•ç‚¹èµ„æºè€—å°½å¯èƒ½å¸¦æ¥çš„ä¸¥é‡åæœã€‚
æˆ‘ä»¬ä½¿ç”¨ Kubernetes é›†ç¾¤éƒ¨ç½²äº†ä¸€ç³»åˆ—æœåŠ¡ï¼Œå…¶ä¸­ä¸€ä¸ªæ•°æ®æ”¶é›†appï¼Œç”¨äºæ‰«æç”Ÿäº§ç¯å¢ƒä¸­çº¦3000ä¸ªè™šæ‹Ÿæœºä¸Šè¿è¡Œçš„åº”ç”¨ï¼Œæ”¶é›†åº”ç”¨æš´éœ²çš„æŒ‡æ ‡æ•°æ®ï¼Œé…ç½®å¦‚ä¸‹ï¼š

é›†ç¾¤æ¶æ„è¯¦æƒ…ï¼š
- K8S é›†ç¾¤ï¼š3 ä¸ªä¸»èŠ‚ç‚¹ï¼ˆæ§åˆ¶å¹³é¢ï¼‰ï¼Œ4 ä¸ªå·¥ä½œèŠ‚ç‚¹ï¼ˆè®¡ç®—èµ„æºï¼‰
- æ¯ä¸ªå·¥ä½œèŠ‚ç‚¹é…ç½®ï¼š16æ ¸CPUï¼Œ32GBå†…å­˜ï¼Œæ‰¿è½½å¤šä¸ªä¸šåŠ¡åº”ç”¨
- é›†ç¾¤ç‰ˆæœ¬ï¼šKubernetes v1.24ï¼Œä½¿ç”¨é»˜è®¤è°ƒåº¦å™¨é…ç½®
- ç½‘ç»œæ’ä»¶ï¼šCalicoï¼Œå­˜å‚¨ï¼šNFSå…±äº«å­˜å‚¨

æ•°æ®æ”¶é›†åº”ç”¨æ¶æ„ï¼š
- æ•°æ®æ”¶é›†appï¼šéƒ¨ç½²äº† 6 ä¸ª Pod (replicas=6)ï¼Œä»¥æé«˜ä»»åŠ¡çš„å¹¶å‘å¤„ç†èƒ½åŠ›
- æ¯ä¸ªPodèµ„æºé…ç½®ï¼šrequests(2æ ¸CPU, 4GBå†…å­˜)ï¼Œlimits(4æ ¸CPU, 8GBå†…å­˜)
- appè¿è¡Œæ—¶ä¸ºæ¯ä¸ªæ‰«æå¯¹è±¡åˆ›å»ºçº¿ç¨‹ï¼Œæ¯ä¸ªçº¿ç¨‹è´Ÿè´£å¤„ç†ä¸€éƒ¨åˆ†æ‰«æä»»åŠ¡
- æ‰«æå¯¹è±¡ç±»å‹ï¼šä¸»è¦æ˜¯é’ˆå¯¹ç”Ÿäº§ç¯å¢ƒçš„æ‰€æœ‰è™šæ‹Ÿæœºä¸Šè¿è¡Œçš„åº”ç”¨ï¼Œæ”¶é›†åº”ç”¨æš´éœ²çš„æŒ‡æ ‡ metricã€‚
- å¤§æ¦‚æœ‰3000ä¸ªVMï¼Œæ¯ä¸ªVMå¯èƒ½æœ‰1ï½2ä¸ªåº”ç”¨ï¼Œæ ¹æ®é…ç½®æ¯ä¸ªåº”ç”¨éœ€è¦æ‰«æ1ï½2ä¸ªç«¯å£

è°ƒç”¨æ¨¡å¼å’Œè´Ÿè½½ç‰¹å¾ï¼š
- appé€šè¿‡ K8S çš„ Service ç«¯å£ä»¥ 1 åˆ†é’Ÿçš„é—´éš”è¢«æŒç»­è°ƒç”¨
- Service é€šè¿‡è´Ÿè½½å‡è¡¡å°†è¯·æ±‚åˆ†é…åˆ°å…·ä½“çš„ Podï¼Œå¹¶æ‰§è¡Œæ‰«æä»»åŠ¡
- æ¯æ¬¡è°ƒç”¨è§¦å‘å…¨é‡æ‰«æï¼Œé¢„è®¡å¤„ç†æ—¶é—´45-55ç§’
- é«˜å³°æœŸå¹¶å‘è¯·æ±‚å¯èƒ½å¯¼è‡´å¤šä¸ªæ‰«æä»»åŠ¡é‡å æ‰§è¡Œ
- åº”ç”¨é‡‡ç”¨å¼‚æ­¥å¤„ç†æ¨¡å¼ï¼Œæ”¯æŒä»»åŠ¡é˜Ÿåˆ—å’Œç»“æœç¼“å­˜

é—®é¢˜æè¿°
æœåŠ¡é€šè¿‡CI/CDéƒ¨ç½²åï¼Œæ‰€æœ‰PODå‰¯æœ¬éƒ½è¢«è°ƒåº¦åˆ°äº†å·¥ä½œèŠ‚ç‚¹d01(å¦‚å›¾)ï¼Œè€Œæ²¡æœ‰å‡åŒ€åˆ†å¸ƒåˆ°å…¶ä»–å·¥ä½œèŠ‚ç‚¹ã€‚è¿™ç§è°ƒåº¦ä¸å‡è¡¡å¯¼è‡´å¦‚ä¸‹é—®é¢˜ï¼š

çº¿ç¨‹èµ„æºè€—å°½çš„è¯¦ç»†è¿‡ç¨‹ï¼š
æ¯ä¸ª Pod åˆ›å»ºå¤§é‡çº¿ç¨‹å¤„ç†æ‰«æä»»åŠ¡ã€‚å½“ 6 ä¸ª Pod å…¨éƒ¨é›†ä¸­åœ¨å·¥ä½œèŠ‚ç‚¹d01ä¸Šè¿è¡Œæ—¶ï¼Œçº¿ç¨‹æ•°é‡å¿«é€Ÿå¢é•¿ï¼Œæœ€ç»ˆè€—å°½äº†èŠ‚ç‚¹çš„çº¿ç¨‹èµ„æºã€‚å…·ä½“è¡¨ç°ä¸ºï¼š
- æ¯ä¸ªPodä¸ºäº†å¤„ç†åˆ†é…çš„æ‰«æä»»åŠ¡åˆ›å»ºäº†å¤§é‡å·¥ä½œçº¿ç¨‹ï¼Œçº¿ç¨‹æ•°é‡è¿œè¶…é¢„æœŸ
- 6ä¸ªPodå…¨éƒ¨é›†ä¸­åœ¨åŒä¸€èŠ‚ç‚¹è¿è¡Œæ—¶ï¼Œæ€»çº¿ç¨‹æ•°é‡æ€¥å‰§å¢é•¿ï¼Œç”¨äºå¹¶å‘å¤„ç†æ‰€æœ‰VMåº”ç”¨çš„metricsæ”¶é›†
- åŠ ä¸ŠåŒèŠ‚ç‚¹ä¸Šè¿è¡Œçš„å…¶ä»–podï¼Œç³»ç»Ÿè¿›ç¨‹ã€kubeletã€å®¹å™¨è¿è¡Œæ—¶ç­‰åŸºç¡€æœåŠ¡çº¿ç¨‹ï¼Œæ€»çº¿ç¨‹æ•°é€æ¸é€¼è¿‘ç³»ç»Ÿä¸Šé™65535

ç³»ç»Ÿå´©æºƒçš„è¿é”ååº”ï¼š
å½“èŠ‚ç‚¹d01éƒ¨ç½²çš„ä»»æ„æœåŠ¡å°è¯•åˆ›å»ºæ›´å¤šçº¿ç¨‹æ—¶ï¼Œå› èŠ‚ç‚¹å·²è¾¾åˆ°çº¿ç¨‹ä¸Šé™ï¼Œå¯¼è‡´æœåŠ¡æŠ¥é”™ï¼Œæ—¥å¿—æ˜¾ç¤ºï¼š`OpenBLAS blas_thread_init: pthread_create failed for thread 44 of 48: Resource temporarily unavailable`ã€‚è¯¥é”™è¯¯è¡¨æ˜çº¿ç¨‹åˆ›å»ºå¤±è´¥ï¼ŒåŸå› æ˜¯èŠ‚ç‚¹æ— æ³•ä¸ºæ–°çº¿ç¨‹åˆ†é…èµ„æºã€‚åŒæ—¶ï¼Œç³»ç»Ÿè¿˜å‡ºç°äº†`-bash: fork: Cannot allocate memory`çš„é”™è¯¯ï¼Œè¡¨æ˜è¿åŸºæœ¬çš„è¿›ç¨‹åˆ›å»ºéƒ½æ— æ³•å®Œæˆã€‚

èŠ‚ç‚¹å´©æºƒçš„å…·ä½“å½±å“ï¼š
å·¥ä½œèŠ‚ç‚¹d01å½»åº•å´©æºƒï¼Œæ— æ³•ç»§ç»­è¿è¡Œä»»åŠ¡ï¼Œå¯¹æ•´ä¸ªé›†ç¾¤çš„ç¨³å®šæ€§é€ æˆäº†å½±å“ï¼š
- kubeletè¿›ç¨‹å› æ— æ³•åˆ›å»ºæ–°çº¿ç¨‹è€Œåœæ­¢å“åº”ï¼ŒèŠ‚ç‚¹çŠ¶æ€å˜ä¸ºNotReady
- æ‰€æœ‰è¿è¡Œåœ¨è¯¥èŠ‚ç‚¹çš„Podè¢«æ ‡è®°ä¸ºTerminatingï¼Œä½†æ— æ³•æ­£å¸¸ç»ˆæ­¢
- kube-proxyæ— æ³•æ›´æ–°iptablesè§„åˆ™ï¼Œå¯¼è‡´Serviceæµé‡è·¯ç”±å¼‚å¸¸
- èŠ‚ç‚¹ä¸Šçš„å…¶ä»–ä¸šåŠ¡åº”ç”¨ä¹Ÿå—åˆ°å½±å“ï¼Œå‡ºç°çº§è”æ•…éšœ
- é›†ç¾¤è‡ªåŠ¨å°è¯•åœ¨å…¶ä»–èŠ‚ç‚¹é‡æ–°è°ƒåº¦Podï¼Œä½†ç”±äºåäº²å’Œæ€§é…ç½®ç¼ºå¤±ï¼Œå¯èƒ½å†æ¬¡å‡ºç°è°ƒåº¦ä¸å‡
- ç›‘æ§ç³»ç»Ÿæ˜¾ç¤ºè¯¥èŠ‚ç‚¹CPUä½¿ç”¨ç‡100%ï¼Œè´Ÿè½½å¹³å‡å€¼è¶…è¿‡50ï¼ˆ16æ ¸æœºå™¨æ­£å¸¸è´Ÿè½½åº”åœ¨16ä»¥ä¸‹ï¼‰
å‚è€ƒç‚¹

Linuxç³»ç»Ÿçº¿ç¨‹é™åˆ¶æœºåˆ¶ï¼š
LinuxèŠ‚ç‚¹çº¿ç¨‹ä¸Šé™ä¸º 65,535ï¼ˆé»˜è®¤é…ç½®ï¼Œé€šè¿‡ /proc/sys/kernel/threads-max å’Œ ulimit -u æ§åˆ¶ï¼‰ã€‚è¿™ä¸ªé™åˆ¶åŒ…å«ä»¥ä¸‹å‡ ä¸ªå±‚é¢ï¼š
- ç³»ç»Ÿçº§é™åˆ¶ï¼š`/proc/sys/kernel/threads-max` æ§åˆ¶æ•´ä¸ªç³»ç»Ÿçš„æœ€å¤§çº¿ç¨‹æ•°
- ç”¨æˆ·çº§é™åˆ¶ï¼š`ulimit -u` æ§åˆ¶å•ä¸ªç”¨æˆ·å¯åˆ›å»ºçš„æœ€å¤§è¿›ç¨‹/çº¿ç¨‹æ•°
- è¿›ç¨‹çº§é™åˆ¶ï¼šæ¯ä¸ªè¿›ç¨‹é»˜è®¤å¯åˆ›å»ºçš„çº¿ç¨‹æ•°å—è™šæ‹Ÿå†…å­˜é™åˆ¶å½±å“
- å®¹å™¨çº§é™åˆ¶ï¼šDockerå®¹å™¨ç»§æ‰¿å®¿ä¸»æœºçš„çº¿ç¨‹é™åˆ¶ï¼Œä½†å¯é€šè¿‡cgroupè¿›è¡Œé¢å¤–çº¦æŸ

åº”ç”¨å¤šçº¿ç¨‹æ¨¡å‹åˆ†æï¼š
æ•°æ®æ”¶é›†æœåŠ¡ä¾èµ–å¤šçº¿ç¨‹æ¨¡å‹ï¼Œæ¯ä¸ªçº¿ç¨‹è´Ÿè´£ä¸€éƒ¨åˆ†æ‰«æå¯¹è±¡ã€‚ç”±äºæ‰«æå¯¹è±¡æ•°é‡è¾ƒå¤§ä¸”é—´éš”ä¸€åˆ†é’Ÿè¢«è°ƒç”¨ï¼ŒæœåŠ¡éœ€è¦åˆ›å»ºè¶³å¤Ÿå¤šçš„çº¿ç¨‹å¹¶å‘æ‰§è¡Œä»»åŠ¡ï¼Œä»¥ä¿éšœæ•°æ®æ”¶é›†çš„æ•ˆç‡ã€‚å…·ä½“çº¿ç¨‹ä½¿ç”¨æ¨¡å¼ï¼š
- I/Oå¯†é›†å‹ä»»åŠ¡ï¼šæ‰«ææ•°æ®åº“ã€æ–‡ä»¶ç³»ç»Ÿã€ç½‘ç»œAPIç­‰ï¼Œéœ€è¦å¤§é‡çº¿ç¨‹ç­‰å¾…I/Oå“åº”
- çº¿ç¨‹æ± åŠ¨æ€è°ƒæ•´ï¼šæ ¹æ®ä»»åŠ¡é˜Ÿåˆ—é•¿åº¦åŠ¨æ€åˆ›å»ºå’Œé”€æ¯çº¿ç¨‹
- çº¿ç¨‹ç”Ÿå‘½å‘¨æœŸï¼šå•ä¸ªæ‰«æä»»åŠ¡çº¿ç¨‹å­˜æ´»æ—¶é—´30-60ç§’ï¼Œä½†é«˜å¹¶å‘æ—¶é‡å æ‰§è¡Œ
- å†…å­˜å ç”¨ï¼šæ¯ä¸ªçº¿ç¨‹å ç”¨çº¦8MBæ ˆç©ºé—´ï¼Œ6000ä¸ªçº¿ç¨‹éœ€è¦çº¦48GBå†…å­˜

ç³»ç»Ÿè®¾è®¡è€ƒé‡ï¼š
é»˜è®¤çº¿ç¨‹ä¸Šé™ï¼ˆ65,535ï¼‰æ˜¯ä¸ºäº†æ”¯æŒé«˜å¹¶å‘éœ€æ±‚ã€‚å¦‚æœä¸Šé™è¾ƒä½ï¼ŒæœåŠ¡å¯èƒ½æ— æ³•å……åˆ†åˆ©ç”¨ CPU èµ„æºï¼Œå¯¼è‡´æ€§èƒ½ç“¶é¢ˆã€‚ä½†åœ¨Kubernetesç¯å¢ƒä¸­ï¼Œè¿™ä¸ªè®¾è®¡å‡è®¾å­˜åœ¨é—®é¢˜ï¼š
- å•èŠ‚ç‚¹é›†ä¸­é£é™©ï¼šæ‰€æœ‰Podè°ƒåº¦åˆ°åŒä¸€èŠ‚ç‚¹æ—¶ï¼Œçº¿ç¨‹éœ€æ±‚è¢«æ”¾å¤§6å€
- èµ„æºç«äº‰åŠ å‰§ï¼šå¤šä¸ªPodåŒæ—¶åˆ›å»ºçº¿ç¨‹ï¼Œå¯¼è‡´èµ„æºäº‰æŠ¢å’Œä¸Šä¸‹æ–‡åˆ‡æ¢å¼€é”€
- æ•…éšœå½±å“æ‰©å¤§ï¼šå•èŠ‚ç‚¹æ•…éšœå½±å“æ‰€æœ‰å‰¯æœ¬ï¼Œè¿èƒŒäº†åˆ†å¸ƒå¼ç³»ç»Ÿçš„å®¹é”™åŸåˆ™
- ç›‘æ§ç›²åŒºï¼šä¼ ç»Ÿç›‘æ§å…³æ³¨CPUã€å†…å­˜ï¼Œä½†å¿½ç•¥äº†çº¿ç¨‹æ•°è¿™ä¸ªå…³é”®æŒ‡æ ‡
é—®é¢˜æ€»ç»“
ç”±äº K8S è°ƒåº¦é—®é¢˜ï¼Œæ‰€æœ‰ Pod è¢«é›†ä¸­è°ƒåº¦åˆ°å•ä¸ªå·¥ä½œèŠ‚ç‚¹ï¼Œå¯¼è‡´çº¿ç¨‹èµ„æºè¿…é€Ÿè€—å°½ï¼Œæœ€ç»ˆå¼•å‘èŠ‚ç‚¹å´©æºƒã€‚å°½ç®¡çº¿ç¨‹ä¸Šé™ï¼ˆ65,535ï¼‰è®¾è®¡ç”¨äºæ”¯æŒé«˜å¹¶å‘éœ€æ±‚ï¼Œä½†åœ¨èµ„æºåˆ†é…ä¸å‡çš„æƒ…å†µä¸‹ï¼Œè¿™ä¸€ä¸Šé™åè€Œæˆä¸ºäº†ç“¶é¢ˆï¼Œé™åˆ¶äº†æœåŠ¡çš„æ­£å¸¸è¿è¡Œã€‚

æ ¹æœ¬åŸå› åˆ†æï¼š
1. è°ƒåº¦ç­–ç•¥ç¼ºå¤±ï¼šDeploymenté…ç½®ä¸­ç¼ºå°‘Podåäº²å’Œæ€§ï¼ˆpodAntiAffinityï¼‰è§„åˆ™ï¼Œå¯¼è‡´è°ƒåº¦å™¨å°†æ‰€æœ‰Podæ”¾ç½®åœ¨åŒä¸€èŠ‚ç‚¹
2. èµ„æºè¯·æ±‚ä¸å‡†ç¡®ï¼šPodçš„resource requestsè®¾ç½®è¿‡ä½ï¼Œè°ƒåº¦å™¨è®¤ä¸ºå•ä¸ªèŠ‚ç‚¹æœ‰è¶³å¤Ÿèµ„æºæ‰¿è½½æ‰€æœ‰Pod
3. ç›‘æ§æŒ‡æ ‡ä¸å…¨é¢ï¼šç¼ºå°‘çº¿ç¨‹æ•°ã€æ–‡ä»¶æè¿°ç¬¦ç­‰å…³é”®ç³»ç»Ÿèµ„æºçš„ç›‘æ§
4. å®¹é‡è§„åˆ’ä¸è¶³ï¼šæœªè€ƒè™‘å¤šçº¿ç¨‹åº”ç”¨åœ¨å•èŠ‚ç‚¹é›†ä¸­éƒ¨ç½²æ—¶çš„èµ„æºæ”¾å¤§æ•ˆåº”

å½±å“èŒƒå›´è¯„ä¼°ï¼š
- ç›´æ¥å½±å“ï¼šæ•°æ®æ”¶é›†æœåŠ¡å®Œå…¨ä¸­æ–­ï¼ŒPodçŠ¶æ€å˜ä¸ºCrashLoopBackOffï¼Œå½±å“ä¸‹æ¸¸8ä¸ªä¾èµ–ç³»ç»Ÿ
- é—´æ¥å½±å“ï¼šèŠ‚ç‚¹d01å´©æºƒå¯¼è‡´è¯¥èŠ‚ç‚¹ä¸Šå…¶ä»–åº”ç”¨æœåŠ¡å¼‚å¸¸ï¼Œæ— æ³•åˆ›å»ºæ–°è¿›ç¨‹
- ä¸šåŠ¡æŸå¤±ï¼šæ•°æ®æ”¶é›†ä¸­æ–­ï¼Œå¯¹ç”Ÿäº§ç¯å¢ƒçš„å®æ—¶ç›‘æ§å½±å“è¾ƒå¤§
- æ¢å¤æˆæœ¬ï¼šéœ€è¦æ‰‹åŠ¨é‡å¯èŠ‚ç‚¹d01ï¼Œé‡æ–°è°ƒåº¦æ‰€æœ‰Pod
- é—®é¢˜ç‰¹å¾ï¼šé—´æ­‡æ€§å¤ç°ï¼ˆæ¯éš”å‡ å¤©å‘ç”Ÿä¸€æ¬¡ï¼‰ï¼Œé‡å¯åæš‚æ—¶ç¼“è§£ä½†æœªè§£å†³æ ¹å› 

ç»éªŒæ•™è®­ï¼š
è¿™ä¸ªæ¡ˆä¾‹å……åˆ†è¯´æ˜äº†åœ¨Kubernetesç¯å¢ƒä¸­ï¼Œä¸ä»…è¦å…³æ³¨ä¼ ç»Ÿçš„CPUã€å†…å­˜èµ„æºï¼Œè¿˜è¦è€ƒè™‘çº¿ç¨‹ã€æ–‡ä»¶æè¿°ç¬¦ã€ç½‘ç»œè¿æ¥ç­‰ç³»ç»Ÿçº§èµ„æºçš„åˆç†åˆ†é…ã€‚åŒæ—¶ï¼Œåˆé€‚çš„è°ƒåº¦ç­–ç•¥å’Œå…¨é¢çš„ç›‘æ§ä½“ç³»æ˜¯ä¿éšœé«˜å¯ç”¨æœåŠ¡çš„å…³é”®åŸºç¡€è®¾æ–½ã€‚

è¯·å…³æ³¨åç»­ï¼Œå’±ä»¬ç»§ç»­èŠé—®é¢˜çš„è§£å†³æ–¹æ¡ˆå’Œè¿½è¸ªè¿‡ç¨‹ï¼

----- English
Exploring SRE: K8S Cluster POD Scheduling Issue Description
Hi everyone, today I'd like to share a production issue I encountered in my daily work. Welcome everyone to brainstorm together.
Application infrastructureï¼š
We deployed a series of services using a Kubernetes cluster, including a data collection app that scans approximately 8,500 objects and collects corresponding data. The configuration is as follows:

- K8S Cluster: 3 master nodes, 4 worker nodes
- Data collection app: 6 Pods deployed (replicas=6) to improve task concurrency processing capability
- Application runtime: The application creates a thread for each scanning object, with each thread responsible for handling a portion of the scanning tasks
- Invocation pattern: The app is invoked continuously through K8S Service port at 1-minute intervals
- Load distribution: The K8S Service uses load balancing to distribute requests to specific Pods for executing scanning tasks
Issue Description
After the service was deployed via CI/CD, all POD replicas were scheduled to worker node 2 (as shown in the diagram below), rather than being evenly distributed across other worker nodes.
This uneven scheduling caused the following problems:
1. Each Pod created a large number of threads to handle scanning tasks. When all 6 Pods were concentrated on worker node 2, the thread count rapidly increased, eventually exhausting the node's thread resources.
2. When any service deployed on node 2 attempted to create additional threads, the node had reached its thread limit, causing service failures. The logs showed:
"OpenBLAS blas_thread_init: pthread_create failed for thread 44 of 48: Resource temporarily unavailable."
This error indicated thread creation failure because the node could not allocate resources for new threads.
3. Worker node 2 completely crashed and could not continue running tasks, affecting the stability of the entire cluster.
Reference Points
- Linux node thread limit is 65,535 (default configuration, controlled by /proc/sys/kernel/threads-max and ulimit -u).
- The data collection service relied on a multi-threaded model, with each thread responsible for a portion of scanning objects. Due to the large number of scanning objects and being called at one-minute intervals, the service needed to create sufficient threads to execute tasks concurrently, ensuring data collection efficiency.
- The default thread limit (65,535) was designed to support high concurrency requirements. If the limit was too low, the service might not have been able to fully utilize CPU resources, leading to performance bottlenecks.
Issue Summary
Due to K8S scheduling issues, all Pods were concentrated on a single worker node, causing thread resources to be quickly exhausted and ultimately leading to node crash. Although the thread limit (65,535) was designed to support high concurrency requirements, in cases of uneven resource allocation, this limit became a bottleneck that restricted normal service operation.
Stay tuned for the next part where we'll discuss the root cause analysis and solution implementation!

Sep 12,2025
----- Chinese
æ¢ç´¢ SREï¼šK8S é›†ç¾¤ Pod è°ƒåº¦é—®é¢˜ä¿®å¤
å¤§å®¶å¥½ï¼Œå’±æ¥èŠèŠä¸Šç¯‡å¸–å­æåˆ°çš„é—®é¢˜æ˜¯å¦‚ä½•è§£å†³çš„ã€‚è¿™æ˜¯ä¸€ä¸ªå…¸å‹çš„Kubernetesè°ƒåº¦ç­–ç•¥é…ç½®æ¡ˆä¾‹ï¼Œé€šè¿‡åˆç†çš„è°ƒåº¦çº¦æŸå¯ä»¥æœ‰æ•ˆé¿å…èµ„æºåˆ†é…ä¸å‡çš„é—®é¢˜ã€‚

é—®é¢˜å›é¡¾
ä¸Šç¯‡æåˆ°ç”±äº K8S è°ƒåº¦é—®é¢˜ï¼Œæ•°æ®æ”¶é›†appçš„æ‰€æœ‰ Pod è¢«é›†ä¸­è°ƒåº¦åˆ°å•ä¸ªå·¥ä½œèŠ‚ç‚¹d01ã€‚æ¯ä¸ª Pod ä¼šåˆ›å»ºå¤§é‡çº¿ç¨‹æ¥å¹¶å‘å¤„ç†ä»»åŠ¡ï¼Œæœ€ç»ˆå¯¼è‡´çº¿ç¨‹èµ„æºè€—å°½ï¼ˆçº¿ç¨‹ä¸Šé™ä¸º 65,535ï¼‰ï¼ŒèŠ‚ç‚¹å´©æºƒï¼ŒæœåŠ¡åœæ­¢è¿è¡Œã€‚è¿™ç§å•ç‚¹æ•…éšœä¸ä»…å½±å“äº†æ•°æ®æ”¶é›†æœåŠ¡æœ¬èº«ï¼Œè¿˜æ³¢åŠåˆ°åŒèŠ‚ç‚¹ä¸Šè¿è¡Œçš„å…¶ä»–ä¸šåŠ¡åº”ç”¨ï¼Œé€ æˆäº†è¿é”ååº”ã€‚

æ•…éšœå½±å“å›é¡¾ï¼š
- æ•°æ®æ”¶é›†æœåŠ¡å®Œå…¨ä¸­æ–­ï¼Œå½±å“ç”Ÿäº§ç¯å¢ƒç›‘æ§
- èŠ‚ç‚¹d01ä¸Šçš„å…¶ä»–åº”ç”¨ä¹Ÿå—åˆ°å½±å“ï¼Œå‡ºç°çº§è”æ•…éšœ
- é›†ç¾¤æ•´ä½“èµ„æºåˆ©ç”¨ç‡ä¸å‡ï¼Œé€ æˆèµ„æºæµªè´¹
- é—®é¢˜å…·æœ‰é—´æ­‡æ€§å¤ç°ç‰¹å¾ï¼Œå¢åŠ äº†æ’æŸ¥éš¾åº¦

è§£å†³æ€è·¯: å¦‚ä½•è®©åº”ç”¨çš„Pod ç›¸å¯¹å‡åŒ€åˆ†å¸ƒåˆ°æ‰€æœ‰å·¥ä½œèŠ‚ç‚¹ä¸Šã€‚

æ ¸å¿ƒç›®æ ‡ï¼š
- é¿å…æ‰€æœ‰Podå‰¯æœ¬é›†ä¸­åœ¨å•ä¸€èŠ‚ç‚¹
- å……åˆ†åˆ©ç”¨é›†ç¾¤çš„è®¡ç®—èµ„æº
- æé«˜æœåŠ¡çš„å®¹é”™èƒ½åŠ›å’Œå¯ç”¨æ€§
- å‡å°‘å•ç‚¹æ•…éšœå¯¹æ•´ä¸ªæœåŠ¡çš„å½±å“
	
æ–¹æ¡ˆ 1ï¼šPod åäº²å’Œæ€§
Anti-Affinityæ˜¯ä¸€ç§é€šè¿‡è°ƒåº¦çº¦æŸé¿å… Pod èšé›†åœ¨åŒä¸€èŠ‚ç‚¹çš„æ–¹æ³•ï¼Œé€‚ç”¨äºéœ€è¦æœåŠ¡é«˜å¯ç”¨æ€§æˆ–èµ„æºéš”ç¦»çš„åœºæ™¯ã€‚è¿™æ˜¯è§£å†³Podè°ƒåº¦ä¸å‡é—®é¢˜çš„ç»å…¸æ–¹æ¡ˆï¼Œåœ¨ä¼ä¸šçº§Kuberneteséƒ¨ç½²ä¸­è¢«å¹¿æ³›é‡‡ç”¨ã€‚

æ ¸å¿ƒåŸç†ï¼š
Podåäº²å’Œæ€§é€šè¿‡æ ‡ç­¾é€‰æ‹©å™¨å’Œæ‹“æ‰‘åŸŸçš„ç»„åˆï¼ŒæŒ‡å¯¼Kubernetesè°ƒåº¦å™¨é¿å…å°†å…·æœ‰ç›¸åŒç‰¹å¾çš„Podè°ƒåº¦åˆ°åŒä¸€ä¸ªæ‹“æ‰‘åŸŸï¼ˆå¦‚åŒä¸€èŠ‚ç‚¹ã€åŒä¸€å¯ç”¨åŒºï¼‰ã€‚è¿™ç§æœºåˆ¶å¯ä»¥æœ‰æ•ˆé˜²æ­¢å•ç‚¹æ•…éšœï¼Œæé«˜æœåŠ¡çš„åˆ†å¸ƒå¼ç‰¹æ€§ã€‚

é…ç½®è§„åˆ™è¯¦è§£ï¼š
é€šè¿‡å®šä¹‰è°ƒåº¦çº¦æŸï¼ŒåŸºäº Pod æ ‡ç­¾è®¾ç½®åäº²å’Œæ€§è§„åˆ™ã€‚ä¾‹å¦‚ï¼šé¿å…åŒä¸€æœåŠ¡çš„å¤šä¸ª Pod å‰¯æœ¬è°ƒåº¦åˆ°åŒä¸€èŠ‚ç‚¹ã€‚é…ç½®æ—¶éœ€è¦æŒ‡å®šï¼š
- æ ‡ç­¾é€‰æ‹©å™¨ï¼šå®šä¹‰å“ªäº›Podä¹‹é—´å­˜åœ¨åäº²å’Œå…³ç³»
- æ‹“æ‰‘é”®ï¼šå®šä¹‰åäº²å’Œæ€§çš„ä½œç”¨èŒƒå›´
- çº¦æŸå¼ºåº¦ï¼šå®šä¹‰è¿åè§„åˆ™æ—¶çš„å¤„ç†æ–¹å¼

çº¦æŸç±»å‹å¯¹æ¯”ï¼š
- å¼ºåˆ¶è§„åˆ™ï¼ˆrequiredDuringSchedulingIgnoredDuringExecutionï¼‰ï¼šPod è°ƒåº¦æ—¶å¿…é¡»æ»¡è¶³åäº²å’Œæ€§è¦æ±‚ï¼Œå¦åˆ™æ— æ³•è¢«è°ƒåº¦ã€‚ä¼˜ç‚¹æ˜¯ä¸¥æ ¼ä¿è¯åˆ†å¸ƒï¼Œç¼ºç‚¹æ˜¯å¯èƒ½å¯¼è‡´Podæ— æ³•è°ƒåº¦ï¼ˆå¦‚èŠ‚ç‚¹èµ„æºä¸è¶³æ—¶ï¼‰ã€‚
- è½¯è§„åˆ™ï¼ˆpreferredDuringSchedulingIgnoredDuringExecutionï¼‰ï¼šè°ƒåº¦æ—¶å°½é‡æ»¡è¶³åäº²å’Œæ€§ï¼Œæ— æ³•æ»¡è¶³æ—¶å…è®¸è°ƒåº¦åˆ°ä¸ç¬¦åˆè§„åˆ™çš„èŠ‚ç‚¹ã€‚ä¼˜ç‚¹æ˜¯ä¿è¯Podèƒ½å¤Ÿè°ƒåº¦ï¼Œç¼ºç‚¹æ˜¯åœ¨èµ„æºç´§å¼ æ—¶å¯èƒ½æ— æ³•å®Œå…¨é¿å…èšé›†ã€‚

æ‹“æ‰‘èŒƒå›´é…ç½®ï¼š
é€šè¿‡ topologyKey å®šä¹‰åäº²å’Œæ€§ç”Ÿæ•ˆçš„èŒƒå›´ï¼Œå¸¸ç”¨é€‰é¡¹åŒ…æ‹¬ï¼š
- `kubernetes.io/hostname`ï¼šæŒ‰èŠ‚ç‚¹ä¸»æœºååˆ†å¸ƒï¼Œç¡®ä¿Podåˆ†æ•£åˆ°ä¸åŒèŠ‚ç‚¹
- `topology.kubernetes.io/zone`ï¼šæŒ‰å¯ç”¨åŒºåˆ†å¸ƒï¼Œé€‚ç”¨äºå¤šAZéƒ¨ç½²
- `topology.kubernetes.io/region`ï¼šæŒ‰åœ°åŸŸåˆ†å¸ƒï¼Œé€‚ç”¨äºè·¨åœ°åŸŸå®¹ç¾
	
æ–¹æ¡ˆ 2ï¼šPod æ‹“æ‰‘åˆ†å¸ƒçº¦æŸ
maxSkew æ˜¯ Podæ‹“æ‰‘åˆ†å¸ƒçº¦æŸçš„ä¸€éƒ¨åˆ†ï¼Œç”¨äºé™åˆ¶ Pod åœ¨ä¸åŒèŠ‚ç‚¹æˆ–åŒºåŸŸä¹‹é—´çš„åˆ†å¸ƒåå·®ã€‚ç›¸æ¯” Pod åäº²å’Œæ€§ï¼ŒmaxSkew æä¾›äº†æ›´çµæ´»çš„è°ƒåº¦ç­–ç•¥ï¼Œå…è®¸ä¸€å®šçš„åˆ†å¸ƒåå·®ã€‚è¿™æ˜¯Kubernetes 1.19+ç‰ˆæœ¬å¼•å…¥çš„æ–°ç‰¹æ€§ï¼Œä¸ºPodåˆ†å¸ƒæä¾›äº†æ›´ç²¾ç»†çš„æ§åˆ¶èƒ½åŠ›ã€‚

æŠ€æœ¯ä¼˜åŠ¿ï¼š
æ‹“æ‰‘åˆ†å¸ƒçº¦æŸç›¸æ¯”ä¼ ç»Ÿçš„åäº²å’Œæ€§å…·æœ‰ä»¥ä¸‹ä¼˜åŠ¿ï¼š
- æ•°é‡åŒ–æ§åˆ¶ï¼šå¯ä»¥ç²¾ç¡®æ§åˆ¶Podåœ¨ä¸åŒæ‹“æ‰‘åŸŸä¹‹é—´çš„æ•°é‡å·®å¼‚
- çµæ´»æ€§æ›´é«˜ï¼šæ”¯æŒå¤šç§åˆ†å¸ƒç­–ç•¥ï¼Œé€‚åº”ä¸åŒçš„ä¸šåŠ¡éœ€æ±‚
- èµ„æºåˆ©ç”¨ç‡æ›´ä¼˜ï¼šé¿å…äº†åäº²å’Œæ€§å¯èƒ½å¯¼è‡´çš„èµ„æºæµªè´¹
- é…ç½®æ›´ç›´è§‚ï¼šé€šè¿‡æ•°å€¼ç›´æ¥è¡¨è¾¾åˆ†å¸ƒè¦æ±‚ï¼Œæ˜“äºç†è§£å’Œç»´æŠ¤

å®šä¹‰åˆ†å¸ƒåå·®è¯¦è§£ï¼š
maxSkew æŒ‡å®š Pod åœ¨ä¸åŒæ‹“æ‰‘åŸŸï¼ˆå¦‚èŠ‚ç‚¹ï¼‰ä¹‹é—´çš„æ•°é‡å·®å¼‚ã€‚ä¾‹å¦‚ï¼š
- `maxSkew: 1` è¡¨ç¤ºæ¯ä¸ªèŠ‚ç‚¹ä¹‹é—´çš„ Pod æ•°é‡å·®å¼‚æœ€å¤šä¸º 1ï¼Œå®ç°æœ€å‡åŒ€çš„åˆ†å¸ƒ
- `maxSkew: 2` å…è®¸èŠ‚ç‚¹é—´æœ€å¤š2ä¸ªPodçš„å·®å¼‚ï¼Œåœ¨ä¿è¯åˆ†å¸ƒçš„åŒæ—¶æä¾›æ›´å¤šè°ƒåº¦çµæ´»æ€§
- å¯¹äº6ä¸ªPodå‰¯æœ¬åœ¨4ä¸ªèŠ‚ç‚¹çš„åœºæ™¯ï¼ŒmaxSkew: 1å¯èƒ½å¯¼è‡´åˆ†å¸ƒä¸º[2,2,1,1]ï¼ŒmaxSkew: 2åˆ™å¯èƒ½ä¸º[3,2,1,0]

è°ƒåº¦ç­–ç•¥é…ç½®ï¼š
- whenUnsatisfiable: DoNotScheduleï¼šå¦‚æœæ— æ³•æ»¡è¶³åˆ†å¸ƒçº¦æŸï¼Œåˆ™ Pod ä¸ä¼šè¢«è°ƒåº¦ã€‚è¿™æ˜¯ä¸¥æ ¼æ¨¡å¼ï¼Œç¡®ä¿åˆ†å¸ƒè¦æ±‚å¾—åˆ°æ»¡è¶³ã€‚
- whenUnsatisfiable: ScheduleAnywayï¼šå³ä½¿æ— æ³•æ»¡è¶³çº¦æŸä¹Ÿä¼šè°ƒåº¦Podï¼Œä½†ä¼šå°½é‡æ»¡è¶³åˆ†å¸ƒè¦æ±‚ã€‚é€‚ç”¨äºå¯¹å¯ç”¨æ€§è¦æ±‚é«˜äºåˆ†å¸ƒè¦æ±‚çš„åœºæ™¯ã€‚

å®é™…åº”ç”¨åœºæ™¯ï¼š
åœ¨æˆ‘ä»¬çš„æ•°æ®æ”¶é›†åº”ç”¨æ¡ˆä¾‹ä¸­ï¼Œä½¿ç”¨maxSkew: 1å¯ä»¥ç¡®ä¿6ä¸ªPodåœ¨4ä¸ªå·¥ä½œèŠ‚ç‚¹ä¸Šçš„åˆ†å¸ƒä¸º[2,2,1,1]æˆ–[2,1,1,1,1]ï¼ˆå¦‚æœæœ‰5ä¸ªèŠ‚ç‚¹ï¼‰ï¼Œé¿å…æ‰€æœ‰Podé›†ä¸­åœ¨å•ä¸€èŠ‚ç‚¹çš„é—®é¢˜ã€‚
è°ƒåº¦å™¨ä¼šå°½é‡ä¿è¯ Pod å‡åŒ€åˆ†å¸ƒåœ¨ä¸åŒèŠ‚ç‚¹ã€‚
	
é—®é¢˜è§£å†³
åœ¨è¯„ä¼°äº†ä¸¤ç§æ–¹æ¡ˆåï¼Œæˆ‘é€‰æ‹©äº†æ–¹æ¡ˆ 2:maxSkewï¼Œå› ä¸ºå®ƒå¯ä»¥æ›´çµæ´»åœ°æ§åˆ¶ Pod çš„åˆ†å¸ƒåå·®ï¼ŒåŒæ—¶å…è®¸ä¸€å®šç¨‹åº¦çš„è°ƒåº¦å®¹å¿æ€§ï¼Œé¿å…å› å¼ºåˆ¶è§„åˆ™å¯¼è‡´è°ƒåº¦å¤±è´¥ã€‚
åœ¨é…ç½®äº†é€‚å½“çš„ maxSkew å‚æ•°åï¼Œé€šè¿‡ CI/CD é‡æ–°éƒ¨ç½²äº†åº”ç”¨ï¼Œå…¶Pod å‰¯æœ¬æˆåŠŸåˆ†å¸ƒåˆ°å·¥ä½œèŠ‚ç‚¹ä¸Šï¼Œæ•´ä¸ªæœåŠ¡å’Œé›†ç¾¤è¿è¡Œç¨³å®šã€‚
	
åç»­æˆ‘ä»¬æ¥èŠèŠè¿™ä¸ªé—®é¢˜æ˜¯å¦‚ä½•è¢«å‘ç°çš„ï¼Œæ¬¢è¿å¤§å®¶ä¸€èµ·è®¨è®º

----- English
Exploring SRE: K8S Cluster Pod Scheduling Issue Fix

Hello everyone, let's discuss how we fixed the issue mentioned in the last story:Exploring SRE: K8S Cluster POD Scheduling Issue Description

Issue Recap
As mentioned in the previous post, due to K8S scheduling issues, all data collection app Pods were concentrated on a single worker node. Each Pod created a large number of threads to handle tasks concurrently, ultimately leading to thread resource exhaustion (thread limit of 65,535), causing the node to crash, and service failure.
Solution Approach: How to distribute application Pods relatively evenly across all worker nodes.

Solution 1: Pod Anti-Affinity
Anti-Affinity is a method that uses scheduling constraints to prevent Pods from clustering on the same node, suitable for scenarios requiring high service availability or resource isolation.
Configuration rules: Define scheduling constraints by setting anti-affinity rules based on Pod labels. For example: prevent multiple replicas of the same Pod from being scheduled to the same node.
Constraint types:
- Hard rules (requiredDuringSchedulingIgnoredDuringExecution): Pods must satisfy anti-affinity requirements during scheduling, otherwise they cannot be scheduled
- Soft rules (preferredDuringSchedulingIgnoredDuringExecution): Attempt to satisfy anti-affinity during scheduling, but allow scheduling to non-compliant nodes when requirements cannot be met
Topology scope: Define the scope where anti-affinity takes effect through topologyKey, such as distribution by node hostname (kubernetes.io/hostname).

Solution 2: Pod Topology Spread Constraints
maxSkew is part of Pod topology spread constraints, used to limit the distribution deviation of Pods across different nodes or zones. Compared to Pod anti-affinity, maxSkew provides more flexible scheduling strategies, allowing certain distribution deviations.
Define distribution deviation: maxSkew specifies the quantity difference of Pods between different topology domains (such as nodes). For example: maxSkew: 1 means the Pod quantity difference between each node is at most 1.
Scheduling strategy: whenUnsatisfiable: DoNotSchedule: If distribution constraints cannot be satisfied, the Pod will not be scheduled.
The scheduler will try to ensure Pods are evenly distributed across different nodes.

Issue Fix
After evaluating both solutions, I chose Solution 2: maxSkew, because it can more flexibly control Pod distribution deviation while allowing a certain degree of scheduling tolerance, avoiding scheduling failures caused by hard rules.
After configuring appropriate maxSkew parameters, the application was redeployed through CI/CD, and its Pod replicas were successfully distributed across worker nodes, with the entire service and cluster running stably.

Summary
Anti-Affinity is about separation: "Keep Pod A away from Pod B."
maxSkew is about balance: "Spread all these identical pods out as evenly as you can."

Next, we'll discuss how this problem was discovered. Welcome everyone to join the discussion.

Sep 15, 2025
----- Chinese
æ¢ç´¢ SREï¼šK8S é›†ç¾¤ POD è°ƒåº¦é—®é¢˜è¿½è¸ªè¿‡ç¨‹

å¤§å®¶å¥½ï¼Œå‰ç¯‡æ–‡ç« ä»‹ç»äº†é—®é¢˜çš„ä¿®å¤è¿‡ç¨‹ï¼Œä»Šå¤©æ¥èŠèŠå®ƒæ˜¯å¦‚ä½•è¢«å‘ç°çš„ã€‚å¤§å®¶å¯ä»¥ä»ä¸­äº†è§£SREåœ¨å¤„ç†é—®é¢˜æ—¶çš„æ ‡å‡†æ“ä½œæµç¨‹å’Œä¸€äº›æ€è€ƒè¿‡ç¨‹ã€‚
é—®é¢˜åˆç°ï¼šæŸå·¥ä½œæ—¥ä¸‹åˆ14:32ï¼Œç›‘æ§ç³»ç»Ÿï¼ˆPrometheus + AlertManagerï¼‰æ¢æµ‹åˆ°å¼‚å¸¸åï¼Œè‡ªåŠ¨å‘å‡ºCriticalçº§åˆ«å‘Šè­¦å¹¶æ¨é€åˆ°å³æ—¶èŠå¤©å·¥å…·ï¼Œæç¤ºK8Sé›†ç¾¤ä¸­æœ‰6ä¸ªPodçŠ¶æ€ä¸ºCrashLoopBackOffï¼Œä¸”é—´éš”15åˆ†é’ŸæŒç»­æ¨é€3æ¬¡ä»¥ä¸Šã€‚å€¼ç­SREæŒ‰ç…§å‘Šè­¦å¤„ç†SOPï¼ˆStandard Operating Procedureï¼Œæ ‡å‡†æ“ä½œæµç¨‹ï¼‰ï¼Œåœ¨æ”¶åˆ°ç¬¬äºŒæ¬¡æ¨é€åç«‹å³ä»‹å…¥ï¼Œé€šè¿‡kubectlå‘½ä»¤ç¡®è®¤éå¶å‘æƒ…å†µã€‚
	
åˆæ­¥åˆ†æï¼š
é€šè¿‡kubectl logsæŸ¥çœ‹Podæ—¥å¿—ï¼Œå‘ç°å…³é”®é”™è¯¯ä¿¡æ¯ï¼šthread_init: Resource temporarily unavailableã€‚CrashLoopBackOffçŠ¶æ€è¡¨æ˜Podåå¤å¯åŠ¨å¤±è´¥ï¼Œ"Resource temporarily unavailable"æç¤ºç³»ç»Ÿèµ„æºä¸è¶³ã€‚åŸºäºé”™è¯¯ç‰¹å¾ï¼ŒSREå›¢é˜Ÿé‡‡ç”¨"è‡ªä¸‹è€Œä¸Š"æ’æŸ¥ç­–ç•¥ï¼šä¼˜å…ˆæ£€æŸ¥åº•å±‚åŸºç¡€è®¾æ–½ï¼Œå†åˆ†æä¸Šå±‚åº”ç”¨é—®é¢˜ï¼Œè¿™æ ·èƒ½æ›´å¿«å®šä½æ ¹æœ¬åŸå› ã€‚
	
å®šä½å·¥ä½œèŠ‚ç‚¹ï¼šèµ„æºè€—å°½åˆç°ç«¯å€ª
é€šè¿‡kubectl get pods -o wideæ£€æŸ¥è°ƒåº¦æƒ…å†µï¼Œå‘ç°ä¸€ä¸ªå¼‚å¸¸ç°è±¡ï¼šæ•°æ®æ”¶é›†åº”ç”¨çš„å…¨éƒ¨6ä¸ªPodéƒ½è¢«è°ƒåº¦åˆ°äº†åŒä¸€ä¸ªå·¥ä½œèŠ‚ç‚¹d01ä¸Šï¼Œè¿èƒŒäº†é«˜å¯ç”¨éƒ¨ç½²åŸåˆ™ã€‚SSHç™»å½•èŠ‚ç‚¹d01è¿›è¡Œåˆ†ææ—¶ï¼Œé‡åˆ°ç³»ç»Ÿçº§é”™è¯¯ï¼š
-bash: fork: Cannot allocate memory
åˆæ­¥ç°è±¡ï¼šèŠ‚ç‚¹æ— æ³•åˆ›å»ºæ–°è¿›ç¨‹ï¼Œæç¤ºâ€œCannot allocate memoryâ€ï¼Œè¡¨æ˜èŠ‚ç‚¹å¯èƒ½å·²ç»è€—å°½äº†èµ„æºï¼ˆå¦‚å†…å­˜æˆ–çº¿ç¨‹ï¼‰ã€‚
è‡³æ­¤ï¼Œå†…å­˜è€—å°½çš„æ ¹æœ¬åŸå› å°šä¸æ˜ç¡®ã€‚
åŸºäºSRE"å¯ç”¨æ€§ä¼˜å…ˆ"åŸåˆ™ï¼Œå›¢é˜Ÿå†³å®šç«‹å³é‡å¯èŠ‚ç‚¹d01ï¼šæ—¢èƒ½å¿«é€Ÿæ¢å¤æœåŠ¡ï¼Œåˆèƒ½ä¸ºåç»­åˆ†æäº‰å–æ—¶é—´ã€‚é‡å¯åæœåŠ¡æ¢å¤æ­£å¸¸ï¼Œä½†æ ¹æœ¬é—®é¢˜å°šæœªè§£å†³ã€‚
	
é—®é¢˜å¤ç°ï¼šé—´æ­‡æ€§ä¸­æ–­
å‡ å¤©åçš„21:15å¼€å§‹ï¼Œç±»ä¼¼é—®é¢˜å†æ¬¡å‘ç”Ÿï¼Œç›‘æ§ç³»ç»ŸæŒç»­æ¨é€å‘Šè­¦ï¼Œd01èŠ‚ç‚¹ä¸Šçš„å¤šä¸ªPodå†æ¬¡CrashLoopBackOffï¼ŒèŠ‚ç‚¹èµ„æºå†æ¬¡è€—å°½ã€‚åŸºäºä¸Šæ¬¡ç»éªŒï¼Œå›¢é˜Ÿç›´æ¥æ‰§è¡ŒèŠ‚ç‚¹é‡å¯ï¼Œ25åˆ†é’Ÿå†…æ¢å¤æœåŠ¡ã€‚
é—®é¢˜ç‰¹ç‚¹ï¼š
æ¯éš”4-6å¤©å¤ç°ä¸€æ¬¡ï¼Œè¡¨ç°å‡ºæ˜æ˜¾çš„é—´æ­‡æ€§æ¨¡å¼ã€‚
é‡å¯åé—®é¢˜æš‚æ—¶ç¼“è§£ï¼Œä½†è¿™ç§ä¸´æ—¶æ–¹æ¡ˆå­˜åœ¨æ˜æ˜¾å±€é™ï¼šæ— æ³•é¢„æµ‹ä¸‹æ¬¡æ•…éšœæ—¶é—´ï¼Œå½±å“ç³»ç»Ÿé•¿æœŸç¨³å®šæ€§ï¼Œä¸”é¢‘ç¹é‡å¯å¢åŠ è¿ç»´è´Ÿæ‹…ã€‚

æ·±å…¥æ’æŸ¥ï¼šå…³é”®å‘ç°
åˆæ˜¯å‡ å¤©åï¼Œå›¢é˜Ÿåˆ©ç”¨ç›¸å¯¹ç¨³å®šçš„æ—¶é—´çª—å£å¯¹d01èŠ‚ç‚¹è¿›è¡Œå…¨é¢æ£€æŸ¥ï¼Œå‘ç°äº†é—®é¢˜çš„çœŸæ­£æ ¹æºï¼šèŠ‚ç‚¹ä¸Šè¿è¡Œç€6ä¸ªç›¸åŒçš„Javaè¿›ç¨‹ï¼Œæ¯ä¸ªè¿›ç¨‹å¹³å‡åˆ›å»º8000+ä¸ªçº¿ç¨‹ï¼Œæ€»çº¿ç¨‹æ•°çº¦48000ä¸ªï¼Œæ¥è¿‘Linuxç³»ç»Ÿé»˜è®¤çš„65535çº¿ç¨‹ä¸Šé™ã€‚
è¿›ä¸€æ­¥è°ƒæŸ¥ç¡®è®¤ï¼Œè¿™äº›Javaè¿›ç¨‹å‡å±äºæ•°æ®æ”¶é›†åº”ç”¨çš„Podå‰¯æœ¬ï¼Œé‡‡ç”¨äº†"ä¸€ä¸ªç›‘æ§ç«¯ç‚¹ä¸€ä¸ªçº¿ç¨‹"çš„è®¾è®¡æ¨¡å¼ã€‚åœ¨å¤§è§„æ¨¡ç›‘æ§åœºæ™¯ä¸‹ï¼ˆ3000 VMs Ã— 1-2åº”ç”¨ Ã— 1-2ç«¯å£ â‰ˆ 6750ä¸ªç›‘æ§ç«¯ç‚¹ï¼‰ï¼Œè¿™ç§çº¿ç¨‹æ¨¡å‹å¯¼è‡´å•ä¸ªPodåˆ›å»ºè¿‡å¤šçº¿ç¨‹ã€‚åŠ ä¸ŠKuberneteså°†6ä¸ªPodå…¨éƒ¨è°ƒåº¦åˆ°åŒä¸€èŠ‚ç‚¹d01ï¼Œæœ€ç»ˆè§¦å‘äº†ç³»ç»Ÿèµ„æºè€—å°½ã€‚
	
æ•…äº‹çš„åç»­å’Œå…¶ä»–ç»†èŠ‚ï¼Œå¤§å®¶å¯ä»¥å‚è€ƒå‰é¢çš„å¸–å­ã€‚åœ¨æ­¤æˆ‘ç®€è¦è¯´æ˜ä¸‹SREçš„ä¸»è¦èŒè´£ï¼š

ç›‘æ§ä¸å‘Šè­¦ï¼šå»ºç«‹å…¨é¢çš„ç›‘æ§ä½“ç³»ï¼Œç¡®ä¿ç³»ç»Ÿå¼‚å¸¸èƒ½å¤Ÿè¢«åŠæ—¶å‘ç°å’Œå‡†ç¡®æ¨é€ï¼Œä¸ºå¿«é€Ÿå“åº”å¥ å®šåŸºç¡€ã€‚

ä¼˜å…ˆæ¢å¤æœåŠ¡ï¼šéµå¾ª"å¯ç”¨æ€§ä¼˜å…ˆ"åŸåˆ™ï¼Œåœ¨æ•…éšœå‘ç”Ÿæ—¶é¦–å…ˆç¡®ä¿ä¸šåŠ¡åŠŸèƒ½æ¢å¤ï¼Œé¿å…å½±å“æ‰©å¤§ã€‚

é—®é¢˜å®šä½ä¸ä¿®å¤ï¼šè¿™å¾€å¾€æ˜¯SREå·¥ä½œä¸­æœ€å…·æŒ‘æˆ˜æ€§çš„éƒ¨åˆ†ã€‚è®¸å¤šå¤æ‚é—®é¢˜éœ€è¦æŒç»­è·Ÿè¸ªã€åå¤å°è¯•ï¼Œæœ‰æ—¶ç”šè‡³éœ€è¦æ¨å€’é‡æ¥ã€‚åƒæœ¬æ¬¡Podè°ƒåº¦é—®é¢˜ï¼Œä»æœ€åˆçš„ä¸´æ—¶é‡å¯åˆ°æœ€ç»ˆå‘ç°çº¿ç¨‹æ¨¡å‹ç¼ºé™·ï¼Œéœ€è¦SREå…·å¤‡è€å¿ƒç»†è‡´çš„åˆ†æèƒ½åŠ›ï¼Œèƒ½å¤ŸæŠ½ä¸å‰¥èŒ§èˆ¬åœ°ä»è¡¨é¢ç°è±¡æ·±å…¥åˆ°æ ¹æœ¬åŸå› ï¼Œè¿™ç§æ·±åº¦åˆ†æèƒ½åŠ›æ˜¯æˆä¸ºä¼˜ç§€SREçš„å…³é”®ã€‚


----- English
Exploring SRE: K8S Cluster Pod Scheduling Issue Track Process

Hello everyone, let's discuss how we track the issue mentioned in the last story:Exploring SRE: K8S Cluster POD Scheduling Issue Description

Initial Analysis:
CrashLoopBackOff indicates that Pods cannot start normally, possibly due to application issues, resource limitations, or underlying node problems. Pod error logs showed the same issue mentioned in previous posts: "thread_init: Resource temporarily unavailable."
To restore service more quickly, we decided to start from the underlying node resources and gradually investigate the root cause.

Locating Worker Node: Resource Exhaustion Emerges
By checking the scheduling status of problematic Pods, we found that these Pods were all running on worker node d01. When we SSH'd into node d01 for deeper analysis, we encountered the following anomaly when switching users:
-bash: fork: Cannot allocate memory

Initial observation: The node could not create new processes, showing "Cannot allocate memory," indicating that the node had likely exhausted resources (such as memory or threads).
At this point, the root cause of memory exhaustion was still unclear.
Without more clues, to restore service functionality and maintain cluster stability, SRE team decided to restart node d01.

Problem Recurrence: Intermittent Interruptions
On the evening of May 4th, similar problems occurred again. The monitoring system continuously pushed alerts, multiple Pods on node d01 experienced CrashLoopBackOff again, and node resources were exhausted once more. The team restarted the node again to restore service.

Problem characteristics:
- Recurred every few days, showing intermittent behavior
- Problems were temporarily alleviated after restart, but root cause remained unresolved

Deep Investigation: Key Discovery
Couple of days later, the team conducted a comprehensive check of node d01 and discovered multiple identical Java processes on the node, with each process using an average of 8000+ threads.
Further investigation confirmed that these Java processes all belonged to Pod replicas of the data collection app, responsible for scanning 8k+ objects and processing data. Multiple replicas of the app being scheduled simultaneously on d01 and consuming massive threads became the core reason for resource exhaustion.

For the continuation of this story, you can refer to previous posts. Here I'll briefly explain the main responsibilities of SRE:
- Monitoring and Alerting: Effectively monitor the cluster and ensure monitoring tools automatically and promptly push alerts
- Priority Service Recovery: Quickly restore functionality and ensure alerts are cleared
- Problem Location and Fix: Deep analysis of root causes, follow up on problem fixes

Sep 18, 2025
----- Chinese
æµ·å› é‡Œå¸Œæ³•åˆ™ï¼šé¢„é˜²å°é—®é¢˜ï¼Œé¿å…å¤§ç¾éš¾
å¤§å®¶å¥½ï¼Œä»Šå¤©æˆ‘ä»¬æ¥ä¸€èµ·å­¦ä¹ ä¸‹å®‰å…¨ç”Ÿäº§çš„ç†è®ºçŸ¥è¯†ï¼Œä»æµ·å› é‡Œå¸Œæ³•åˆ™å¼€å§‹ã€‚
	
1ï¸âƒ£ æµ·å› é‡Œå¸Œæ³•åˆ™
å¦‚å›¾2ï¼ŒHeinrichæ³•åˆ™æœ€åˆæ¥æºäºå·¥ä¸šå®‰å…¨é¢†åŸŸï¼Œç”±å®‰å…¨ä¸“å®¶Herbert William Heinrichåœ¨20ä¸–çºª30å¹´ä»£æå‡ºã€‚ä»–é€šè¿‡åˆ†æ55ä¸‡èµ·å·¥ä¸šäº‹æ•…æ¡ˆä¾‹ï¼Œå‘ç°äº†ä¸€ä¸ªæƒŠäººçš„è§„å¾‹ï¼š
â€¼ï¸â€¼ï¸â€¼ï¸ æ¯ä¸€èµ·ä¸¥é‡äº‹æ•…çš„èƒŒåï¼Œå¾€å¾€æœ‰29èµ·è½»å¾®äº‹æ•…ï¼Œä»¥åŠ300èµ·æœªé‚äº‹ä»¶æˆ–éšæ‚£ã€‚

è¿™ä¸ª1:29:300çš„æ¯”ä¾‹å°±åƒä¸€åº§"äº‹æ•…é‡‘å­—å¡”"ï¼Œå¡”å°–æ˜¯ä¸¥é‡äº‹æ•…ï¼Œä¸­é—´æ˜¯è½»å¾®äº‹æ•…ï¼Œåº•éƒ¨æ˜¯å¤§é‡çš„æœªé‚äº‹ä»¶å’Œéšæ‚£ã€‚Heinriché€šè¿‡å¤§é‡æ•°æ®è¯æ˜ï¼Œé‡å¤§äº‹æ•…ç»ä¸æ˜¯å¶ç„¶å‘ç”Ÿçš„ï¼Œè€Œæ˜¯ç”±ä¸€ç³»åˆ—è¾ƒå°é—®é¢˜çš„ç§¯ç´¯å¯¼è‡´çš„ã€‚

æ¢å¥è¯è¯´ï¼Œæ¯å½“æˆ‘ä»¬çœ‹åˆ°ä¸€ä¸ªä¸¥é‡äº‹æ•…æ—¶ï¼ŒèƒŒåå…¶å®å·²ç»æœ‰æ— æ•°æ¬¡"å·®ç‚¹å‡ºäº‹"çš„è­¦å‘Šä¿¡å·ã€‚å¦‚æœæˆ‘ä»¬èƒ½åŠæ—¶è¯†åˆ«å’Œå¤„ç†è¿™äº›å°é—®é¢˜ï¼Œå°±å¯ä»¥æœ‰æ•ˆåœ°é¢„é˜²æ›´ä¸¥é‡çš„åæœã€‚è¿™å°±åƒåŒ»ç”Ÿé€šè¿‡ä½“æ£€å‘ç°æ—©æœŸç—‡çŠ¶ï¼ŒåŠæ—¶æ²»ç–—å°±èƒ½é¿å…é‡ç—…ä¸€æ ·ã€‚

æ›´æœ‰è¶£çš„æ˜¯ï¼ŒHeinrichè¿˜å‘ç°äº†å¦ä¸€ä¸ªè§„å¾‹ï¼šåœ¨è¿™äº›äº‹æ•…ä¸­ï¼Œ88%æ˜¯ç”±äººçš„ä¸å®‰å…¨è¡Œä¸ºé€ æˆçš„ï¼Œ10%æ˜¯ç”±ç‰©çš„ä¸å®‰å…¨çŠ¶æ€é€ æˆçš„ï¼Œåªæœ‰2%æ˜¯ç”±ä¸å¯æŠ—æ‹’çš„è‡ªç„¶ç¾å®³é€ æˆçš„ã€‚è¿™å‘Šè¯‰æˆ‘ä»¬ï¼Œç»å¤§å¤šæ•°äº‹æ•…å…¶å®æ˜¯å¯ä»¥é¢„é˜²çš„ï¼
	
2ï¸âƒ£ åœ¨SREé¢†åŸŸï¼Œæˆ‘ä»¬è‡´åŠ›äºé€šè¿‡æé«˜ç³»ç»Ÿçš„å¯é æ€§å’Œç¨³å®šæ€§ï¼Œæ¥ä¸ºç”¨æˆ·æä¾›å§‹ç»ˆå¦‚ä¸€çš„æœåŠ¡ä½“éªŒã€‚ä¸ºäº†å®ç°è¿™ä¸€ç›®æ ‡ï¼Œæˆ‘ä»¬ä¸ä»…è¦è§£å†³å·²ç»å‘ç”Ÿçš„é—®é¢˜ï¼Œæ›´éœ€è¦ä»æ½œåœ¨é—®é¢˜ä¸­å­¦ä¹ ï¼Œé˜²æ‚£äºæœªç„¶ã€‚åœ¨è¿™ä¸€è¿‡ç¨‹ä¸­ï¼ŒHeinrichæ³•åˆ™ä¸ºæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªæå…·ä»·å€¼çš„è§†è§’ã€‚

åœ¨SREå®è·µä¸­ï¼ŒHeinrichæ³•åˆ™æœ‰ç€å¹¿æ³›çš„åº”ç”¨åœºæ™¯ã€‚å°½ç®¡å®ƒæºäºå®‰å…¨é¢†åŸŸï¼Œä½†å…¶æ ¸å¿ƒæ€æƒ³â€”â€”é€šè¿‡å…³æ³¨å’Œè§£å†³å°é—®é¢˜æ¥é¿å…å¤§é—®é¢˜â€”â€”ä¸SREçš„ç›®æ ‡é«˜åº¦å¥‘åˆã€‚è®©æˆ‘ä»¬çœ‹çœ‹è¿™ä¸ªæ³•åˆ™åœ¨æŠ€æœ¯é¢†åŸŸçš„ç¥å¥‡è¡¨ç°ï¼š

 å°é—®é¢˜ç§¯ç´¯å¯¼è‡´å¤§æ•…éšœ
åœ¨å¤æ‚çš„åˆ†å¸ƒå¼ç³»ç»Ÿä¸­ï¼Œå‡ ä¹æ‰€æœ‰çš„é‡å¤§ç³»ç»Ÿæ•…éšœï¼ˆå¦‚å…¨ç«™å®•æœºã€æ•°æ®ä¸¢å¤±ç­‰ï¼‰éƒ½ä¸æ˜¯å•ä¸€äº‹ä»¶çš„ç»“æœï¼Œè€Œæ˜¯ç”±å¤šä¸ªå°é—®é¢˜å åŠ å¯¼è‡´çš„ã€‚å°±åƒå¤šç±³è¯ºéª¨ç‰Œä¸€æ ·ï¼Œä¸€ä¸ªå°é—®é¢˜è§¦å‘å¦ä¸€ä¸ªé—®é¢˜ï¼Œæœ€ç»ˆå½¢æˆè¿é”ååº”ã€‚

æ¯”å¦‚ï¼šä¸€ä¸ªçœ‹ä¼¼æ— å®³çš„å†…å­˜æ³„æ¼ï¼ˆå°é—®é¢˜ï¼‰â†’ å¯¼è‡´æœåŠ¡å™¨æ€§èƒ½ä¸‹é™ï¼ˆè½»å¾®äº‹æ•…ï¼‰â†’ è§¦å‘è‡ªåŠ¨æ‰©å®¹ä½†é…ç½®æœ‰è¯¯ï¼ˆä¸­ç­‰é—®é¢˜ï¼‰â†’ æœ€ç»ˆå¯¼è‡´æ•´ä¸ªæœåŠ¡é›†ç¾¤å´©æºƒï¼ˆä¸¥é‡äº‹æ•…ï¼‰ã€‚å¦‚å‰æ–‡æåˆ°çš„å®ä¾‹ï¼Œk8sé›†ç¾¤ä¸­æ•°æ®æ”¶é›†appçš„PODå‰¯æœ¬ä¸åˆç†è°ƒåº¦ï¼Œæœ€åˆåªæ˜¯å½±å“å•ä¸ªå·¥ä½œèŠ‚ç‚¹çš„ç¨³å®šï¼Œä½†å¦‚æœä¸åŠæ—¶å¤„ç†ï¼Œå°±ä¼šåƒç—…æ¯’ä¸€æ ·æ‰©æ•£ï¼Œæœ€ç»ˆå¯¼è‡´æ•´ä¸ªé›†ç¾¤çš„ä¸ç¨³å®šç”šè‡³å´©æºƒã€‚

è¿™å°±æ˜¯ä¸ºä»€ä¹ˆä¼˜ç§€çš„SREå·¥ç¨‹å¸ˆæ€»æ˜¯å¯¹"å°å¼‚å¸¸"ä¿æŒé«˜åº¦æ•æ„Ÿâ€”â€”ä»–ä»¬çŸ¥é“ï¼Œä»Šå¤©çš„å°æ³¢åŠ¨å¯èƒ½å°±æ˜¯æ˜å¤©å¤§æ•…éšœçš„é¢„å…†ï¼
	
 é‡è§†æœªé‚äº‹ä»¶ï¼ˆNear Missï¼‰
æœªé‚äº‹ä»¶æ˜¯æŒ‡é‚£äº›æ²¡æœ‰ç›´æ¥å½±å“åˆ°ç”Ÿäº§ç¯å¢ƒï¼Œä½†æš´éœ²äº†æ½œåœ¨é—®é¢˜çš„äº‹ä»¶ã€‚
 è¿™ä¸€ç‚¹æˆ‘ä¹Ÿæœ‰è¯è¦è¯´ï¼Œè¯·å…³æ³¨åç»­åˆ†äº«
	
 æ„å»ºâ€œå…ç–«ç³»ç»Ÿâ€
SREçš„æ ¸å¿ƒä¹‹ä¸€æ˜¯æ„å»ºå¯è§‚æµ‹æ€§å’Œè‡ªåŠ¨åŒ–çš„å·¥å…·é“¾ï¼Œä»¥å¿«é€Ÿå‘ç°å’Œä¿®å¤é—®é¢˜ã€‚é€šè¿‡ç›‘æ§å’Œæ—¥å¿—ç³»ç»Ÿï¼ŒSREå›¢é˜Ÿå¯ä»¥æ•æ‰åˆ°å°é—®é¢˜çš„ä¿¡å·ï¼Œæ¯”å¦‚é”™è¯¯ç‡çš„è½»å¾®ä¸Šå‡ã€å»¶è¿Ÿçš„çŸ­æš‚æ³¢åŠ¨ç­‰ã€‚è¿™äº›â€œå°ä¿¡å·â€æ­£æ˜¯Heinrichæ³•åˆ™ä¸­éšæ‚£çš„è¡¨ç°ï¼ŒåŠæ—¶å“åº”æ˜¯é¢„é˜²ç¾éš¾çš„å…³é”®ã€‚

 å¤§å®¶æœ‰æƒ³è¿‡æ²¡æœ‰ï¼ŒåŠæ—¶å“åº”çš„å‰ææ˜¯æ”¶åˆ°é€šçŸ¥ï¼Œé‚£å¦‚ä½•é«˜æ•ˆåœ°æ”¶åˆ°é€šçŸ¥å‘¢ï¼Ÿ
å‘é‚®ä»¶ï¼Ÿä¼šè¢«æ·¹æ²¡åœ¨ä¸€å †é‚®ä»¶ä¸­ï¼Œæ¯éš”5åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡é‚®ç®±ä¹Ÿä¸å¯èƒ½ï¼›
å‘å³æ—¶æ¶ˆæ¯ï¼Ÿç»“æœç±»ä¼¼ï¼Œå¤§å®¶æœ‰å•¥å»ºè®®è¯·ğŸ™‹
	
3ï¸âƒ£ æ–¹æ³•è®ºæ€»ç»“
Heinrichæ³•åˆ™æ•™ä¼šæˆ‘ä»¬çš„ä¸ä»…ä»…æ˜¯æŠ€æœ¯å±‚é¢çš„é¢„é˜²æ€ç»´ï¼Œæ›´æ˜¯ä¸€ç§å·¥ä½œå“²å­¦ï¼š
â€¢ é‡è§†ç»†èŠ‚ï¼šä¸è¦å› ä¸ºé—®é¢˜"çœ‹èµ·æ¥å¾ˆå°"å°±å¿½è§†å®ƒï¼Œå°é—®é¢˜å¾€å¾€æ˜¯å¤§é—®é¢˜çš„ç§å­
â€¢ å»ºç«‹ä¹ æƒ¯ï¼šè®©"é¢„é˜²æ€§æ€ç»´"æˆä¸ºæ—¥å¸¸å·¥ä½œçš„ä¸€éƒ¨åˆ†ï¼Œè€Œä¸æ˜¯ç­‰é—®é¢˜å‘ç”Ÿåæ‰æƒ³èµ·æ¥
â€¢ å›¢é˜Ÿæ–‡åŒ–ï¼šè¥é€ ä¸€ä¸ªé¼“åŠ±ä¸»åŠ¨å‘ç°å’ŒæŠ¥å‘Šå°é—®é¢˜çš„ç¯å¢ƒï¼Œè€Œä¸æ˜¯"å¤šä¸€äº‹ä¸å¦‚å°‘ä¸€äº‹"
â€¢ æŒç»­æ”¹è¿›ï¼šæ¯ä¸€æ¬¡å°çš„ä¼˜åŒ–éƒ½æ˜¯åœ¨ä¸ºç³»ç»Ÿçš„é•¿æœŸç¨³å®šæ€§æŠ•èµ„

è®°ä½ï¼Œåœ¨SREçš„èŒä¸šç”Ÿæ¶¯ä¸­ï¼Œæˆ‘ä»¬çš„ä»·å€¼ä¸ä»…ä½“ç°åœ¨è§£å†³äº†å¤šå°‘ç´§æ€¥æ•…éšœï¼Œæ›´ä½“ç°åœ¨é¢„é˜²äº†å¤šå°‘æœ¬å¯èƒ½å‘ç”Ÿçš„ç¾éš¾ã€‚æ­£å¦‚é‚£å¥è¯æ‰€è¯´ï¼š"æœ€å¥½çš„æ•…éšœï¼Œå°±æ˜¯ä»æœªå‘ç”Ÿçš„æ•…éšœã€‚"

Heinrichæ³•åˆ™æé†’æˆ‘ä»¬ï¼šåœ¨è¿½æ±‚ç³»ç»Ÿé«˜å¯ç”¨çš„è·¯ä¸Šï¼Œæ²¡æœ‰ä»€ä¹ˆé—®é¢˜æ˜¯"å¤ªå°è€Œä¸å€¼å¾—å…³æ³¨çš„"ã€‚æ¯ä¸€ä¸ªè¢«åŠæ—¶å‘ç°å’Œè§£å†³çš„å°é—®é¢˜ï¼Œéƒ½å¯èƒ½æ‹¯æ•‘äº†ä¸€æ¬¡é‡å¤§äº‹æ•…ï¼Œä¿æŠ¤äº†æ— æ•°ç”¨æˆ·çš„ä½“éªŒã€‚è¿™å°±æ˜¯SREå·¥ä½œçš„é­…åŠ›æ‰€åœ¨â€”â€”æˆ‘ä»¬æ˜¯ç³»ç»Ÿç¨³å®šæ€§çš„å®ˆæŠ¤è€…ï¼Œä¹Ÿæ˜¯ç”¨æˆ·ä½“éªŒçš„éšå½¢è‹±é›„ï¼
----- English
Heinrich's Rule: Prevent Small Problems, Avoid Major Disasters

Hello everyone, today let's learn about safety production theory together (with a different learning homepage image ğŸ˜‚), starting with Heinrich's Rule.

1ï¸âƒ£ Heinrich's Rule
As shown in Figure 2, Heinrich's Rule originally comes from the industrial safety field, proposed by safety expert Herbert William Heinrich in the 1930s. It describes the pattern of accident occurrence:
â€¼ï¸â€¼ï¸â€¼ï¸ Behind every serious accident, there are often 29 minor accidents and 300 near misses or hazards.
In other words, major accidents are often triggered by a series of smaller problems that accumulate or unresolved hazards. If we can identify and address these small problems in time, we can effectively prevent more serious consequences.

2ï¸âƒ£ In the SRE field, we are committed to providing users with a consistent service experience by improving system reliability and stability. To achieve this goal, we need not only to solve problems that have already occurred, but also to learn from potential problems and prevent them before they happen. In this process, Heinrich's Rule provides us with an extremely valuable perspective.

In SRE practice, Heinrich's Rule has wide application scenarios. Although it originates from the safety field, its core idea - preventing major problems by focusing on and solving small problems - is highly aligned with SRE goals. Here are several key points:

Small Problems Accumulate to Cause Major Failures
In complex distributed systems, almost all major system failures (such as site-wide outages, data loss, etc.) are not the result of a single event, but are caused by the accumulation of multiple small problems.
As mentioned in the previous example, unreasonable scheduling of data collection app POD replicas in a k8s cluster may initially only affect the stability of cluster worker nodes, but if not handled promptly, it could lead to instability or even collapse of the entire cluster.

Pay Attention to Near Miss Events
Near miss events refer to those events that do not directly affect the production environment but expose potential problems.
I also have something to say about this point, please follow subsequent sharing.

Building an "Immune System"
One of the core aspects of SRE is building observability and automated toolchains to quickly discover and fix problems. Through monitoring and logging systems, SRE teams can capture signals of small problems, such as slight increases in error rates, brief fluctuations in latency, etc. These "small signals" are exactly the manifestation of hazards in Heinrich's Rule, and timely response is key to preventing disasters.
Have you ever thought about it? The prerequisite for timely response is receiving notifications. So how can we efficiently receive notifications?
Send emails? They'll be buried in a pile of emails, and checking email every 5 minutes is impossible;
Send instant messages? The result is similar. What suggestions do you have? Please ğŸ™‹

3ï¸âƒ£ In the SRE world, every small improvement is an effective prevention against major disasters.

Sep 22, 2025
----- Chinese
Kubernetes Secret è¸©å‘æ¡ˆä¾‹
å¤§å®¶å¥½ï¼Œæ¥ä¸Šç¯‡K8S secret ç®€ä»‹ï¼Œä¸ºäº†åœ¨é›†ç¾¤ä¸­å®‰å…¨å­˜å‚¨æ•æ„Ÿæ•°æ®ï¼ˆå¦‚ç”¨æˆ·åã€å¯†ç ã€API å¯†é’¥ã€TLSè¯ä¹¦ã€Dockeré•œåƒæ‹‰å–å‡­è¯ç­‰ï¼‰ï¼Œé€šå¸¸ä½¿ç”¨ Secret æ¥è¿›è¡Œç®¡ç†ã€‚åº”ç”¨ç¨‹åºå¯ä»¥é€šè¿‡æŒ‚è½½æ–‡ä»¶æˆ–ç¯å¢ƒå˜é‡çš„æ–¹å¼è®¿é—® Secret ä¸­çš„é”®å€¼å¯¹ï¼Œå¹¶æ ¹æ®å®é™…æƒ…å†µä½¿ç”¨è¿™äº›ä¿¡æ¯ã€‚

Secret ä½œä¸º Kubernetes ä¸­å¤„ç†æ•æ„Ÿä¿¡æ¯çš„æ ¸å¿ƒç»„ä»¶ï¼Œçœ‹ä¼¼ç®€å•ï¼Œä½†åœ¨å®é™…ç”Ÿäº§ç¯å¢ƒä¸­å´ç»å¸¸å‡ºç°å„ç§æ„æƒ³ä¸åˆ°çš„é—®é¢˜ã€‚è¿™äº›é—®é¢˜å¾€å¾€ä¸æ˜¯å› ä¸º Secret æœ¬èº«çš„è®¾è®¡ç¼ºé™·ï¼Œè€Œæ˜¯ç”±äºæ“ä½œäººå‘˜å¯¹å…¶å·¥ä½œæœºåˆ¶ç†è§£ä¸å¤Ÿæ·±å…¥ï¼Œæˆ–è€…åœ¨åˆ›å»ºå’Œä½¿ç”¨è¿‡ç¨‹ä¸­å¿½ç•¥äº†ä¸€äº›ç»†èŠ‚ã€‚ä¸€ä¸ªçœ‹ä¼¼å¾®ä¸è¶³é“çš„æ¢è¡Œç¬¦ã€ç©ºæ ¼ï¼Œæˆ–è€…ç¼–ç é—®é¢˜ï¼Œéƒ½å¯èƒ½å¯¼è‡´æ•´ä¸ªåº”ç”¨ç¨‹åºæ— æ³•æ­£å¸¸å¯åŠ¨ï¼Œç”šè‡³åœ¨ç”Ÿäº§ç¯å¢ƒä¸­é€ æˆä¸¥é‡çš„æœåŠ¡ä¸­æ–­ã€‚

çœŸå®æ¡ˆä¾‹ï¼šåº”ç”¨ç¨‹åºå›  Secret çš„å€¼å†…å®¹é—®é¢˜å¯¼è‡´æ•°æ®åº“è¿æ¥å¤±è´¥ã€‚

è¿™æ˜¯ä¸€ä¸ªåœ¨ç”Ÿäº§ç¯å¢ƒä¸­å®é™…å‘ç”Ÿçš„å…¸å‹æ¡ˆä¾‹ï¼Œæ¶‰åŠåˆ°æ–‡æœ¬æ–‡ä»¶å¤„ç†ã€Base64ç¼–ç ã€ä»¥åŠä¸åŒåˆ›å»ºæ–¹å¼çš„ç»†å¾®å·®åˆ«ã€‚è¿™ä¸ªæ¡ˆä¾‹ä¸ä»…å±•ç¤ºäº†é—®é¢˜çš„è¡¨è±¡ï¼Œæ›´é‡è¦çš„æ˜¯æ­ç¤ºäº†é—®é¢˜èƒŒåçš„æ ¹æœ¬åŸå› å’Œè§£å†³æ€è·¯ã€‚
	
1ï¸âƒ£ é—®é¢˜æè¿°
åœ¨ä¸€ä¸ªç”Ÿäº§ç¯å¢ƒçš„Kubernetesé›†ç¾¤ä¸­ï¼Œæˆ‘ä»¬éœ€è¦åˆ›å»ºSecretæ¥å­˜å‚¨æ•°æ®åº“çš„è¿æ¥ä¿¡æ¯ã€‚è¿™æ˜¯ä¸€ä¸ªçœ‹ä¼¼ç®€å•çš„ä»»åŠ¡ï¼Œä½†å´éšè—ç€è®¸å¤šç»†èŠ‚é™·é˜±ã€‚è®¿é—®æ•°æ®åº“çš„å‡­è¯ä¿¡æ¯å¯ä»¥å½’çº³åˆ°genericç±»å‹çš„Secretä¸­ï¼Œåœ¨å®é™…æ“ä½œä¸­æœ‰å¦‚ä¸‹ä¸¤ç§å¸¸è§çš„åˆ›å»ºæ–¹æ³•ï¼š

æ–¹æ³•ä¸€ï¼šä»æ–‡ä»¶åˆ›å»ºSecret
```bash
kubectl create secret generic db-credentials --from-file=username=username.txt --from-file=password=password.txt
```
è¿™ç§æ–¹æ³•ä¸­ï¼Œusername.txtå’Œpassword.txtæ˜¯ä¸¤ä¸ªæ–‡æœ¬æ–‡ä»¶ï¼Œå†…å®¹åˆ†åˆ«ä¸º"root"å’Œ"password"ã€‚è¿™ç§æ–¹å¼çš„ä¼˜åŠ¿æ˜¯å¯ä»¥æ‰¹é‡å¤„ç†å¤šä¸ªæ–‡ä»¶ï¼Œç‰¹åˆ«é€‚åˆå¤„ç†å¤æ‚çš„é…ç½®æ–‡ä»¶æˆ–è¯ä¹¦æ–‡ä»¶ã€‚ä½†æ˜¯ï¼Œæ–‡ä»¶çš„åˆ›å»ºæ–¹å¼å’Œç¼–è¾‘å™¨çš„è¡Œä¸ºä¼šç›´æ¥å½±å“æœ€ç»ˆçš„Secretå†…å®¹ã€‚

æ–¹æ³•äºŒï¼šä»å‘½ä»¤è¡Œå­—é¢é‡åˆ›å»ºSecret
```bash
kubectl create secret generic db-credentials --from-literal=username=root --from-literal=password=password
```
è¿™ç§æ–¹æ³•ç›´æ¥åœ¨å‘½ä»¤è¡Œä¸­è¾“å…¥ç”¨æˆ·åå’Œå¯†ç ï¼Œé¿å…äº†æ–‡ä»¶æ“ä½œçš„å¤æ‚æ€§ã€‚è¿™ç§æ–¹å¼æ›´åŠ ç›´è§‚ï¼Œä½†åœ¨å¤„ç†åŒ…å«ç‰¹æ®Šå­—ç¬¦æˆ–å¤šè¡Œå†…å®¹çš„æƒ…å†µä¸‹å¯èƒ½ä¼šé‡åˆ°shellè½¬ä¹‰çš„é—®é¢˜ã€‚

ä¸¤ç§æ–¹æ³•çš„æŠ€æœ¯å·®å¼‚
ä»è¡¨é¢ä¸Šçœ‹ï¼Œè¿™ä¸¤ç§æ–¹æ³•åˆ›å»ºçš„Secretåº”è¯¥æ˜¯å®Œå…¨ç›¸åŒçš„ï¼Œéƒ½å¯ä»¥é€šè¿‡`kubectl get secret db-credentials -o yaml`æŸ¥çœ‹åˆ°Secretçš„å†…å®¹ã€‚ä½†å®é™…ä¸Šï¼Œå®ƒä»¬åœ¨åº•å±‚å­˜å‚¨æœºåˆ¶ä¸Šå­˜åœ¨ç»†å¾®å·®åˆ«ï¼š
- ç¬¬äºŒç§æ–¹æ³•åˆ›å»ºçš„Secretåœ¨YAMLæ–‡ä»¶ä¸­ä»¥`data`å­—æ®µå­˜å‚¨ï¼ˆBase64ç¼–ç çš„å­—ç¬¦ä¸²ï¼‰
- ç¬¬ä¸€ç§æ–¹æ³•åˆ›å»ºçš„Secretå¯èƒ½åœ¨YAMLæ–‡ä»¶ä¸­ä»¥`binaryData`å­—æ®µå­˜å‚¨ï¼Œè¿™å–å†³äºæ–‡ä»¶å†…å®¹çš„ç‰¹æ€§

åœ¨æ­£å¸¸ä½¿ç”¨æ—¶ï¼Œè¿™ä¸¤ç§æ–¹æ³•åˆ›å»ºçš„Secretåœ¨åŠŸèƒ½ä¸Šæ²¡æœ‰åŒºåˆ«ï¼ŒKubernetesä¼šè‡ªåŠ¨å¤„ç†ç¼–ç å’Œè§£ç è¿‡ç¨‹ã€‚ç„¶è€Œï¼Œæ­£æ˜¯è¿™ç§"çœ‹ä¼¼ç›¸åŒ"çš„å‡è±¡ï¼Œè®©æˆ‘ä»¬å¿½ç•¥äº†æ–‡ä»¶å¤„ç†è¿‡ç¨‹ä¸­å¯èƒ½å¼•å…¥çš„éšè—å­—ç¬¦ã€‚

æ ¹æ®åº”ç”¨ç¨‹åºçš„Deployment YAMLæ–‡ä»¶é…ç½®ï¼ŒPodå¯åŠ¨æ—¶ä¼šé€šè¿‡ç¯å¢ƒå˜é‡çš„æ–¹å¼è¯»å–Secret db-credentialsä¸­çš„usernameå’Œpasswordå€¼æ¥è¿æ¥æ•°æ®åº“ã€‚åº”ç”¨ç¨‹åºçš„é…ç½®å¦‚ä¸‹ï¼š

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-app
spec:
  template:
    spec:
      containers:
      - name: app
        image: myapp:latest
        env:
        - name: DB_USERNAME
          valueFrom:
            secretKeyRef:
              name: db-credentials
              key: username
        - name: DB_PASSWORD
          valueFrom:
            secretKeyRef:
              name: db-credentials
              key: password
```

ç„¶è€Œï¼Œåœ¨Podå¯åŠ¨æ—¶ï¼Œç¨‹åºå´æŠ¥å‡ºäº†ä»¤äººå›°æƒ‘çš„é”™è¯¯æ—¥å¿—ï¼š
```
Unable to obtain connection from database (jdbc:mysql://mysqldb:3306/mysql?Unicode=true&characterEncoding=UTF-8) for user 'root': Access denied for user 'root'@'10.4.214.53' (using password: YES)
```

è¿™ä¸ªé”™è¯¯ä¿¡æ¯ç‰¹åˆ«ä»¤äººå›°æƒ‘ï¼Œå› ä¸ºï¼š
1. é”™è¯¯æ˜¾ç¤º"using password: YES"ï¼Œè¯´æ˜å¯†ç ç¡®å®è¢«ä¼ é€’äº†
2. ç”¨æˆ·å'root'çœ‹èµ·æ¥æ˜¯æ­£ç¡®çš„
3. åŒæ ·çš„å‡­è¯åœ¨å…¶ä»–ç¯å¢ƒä¸­å·¥ä½œæ­£å¸¸
4. æ•°æ®åº“æœåŠ¡æœ¬èº«è¿è¡Œæ­£å¸¸ï¼Œå¯ä»¥é€šè¿‡å…¶ä»–æ–¹å¼è¿æ¥

	
2ï¸âƒ£ ä»æ—¥å¿—åˆ†æ
åº”ç”¨ç¨‹åºæœªèƒ½æˆåŠŸä½¿ç”¨æä¾›çš„ç”¨æˆ·åå’Œå¯†ç è¿æ¥æ•°æ®åº“ã€‚ä»é”™è¯¯æ—¥å¿—å¯ä»¥æå–å‡ºä»¥ä¸‹å…³é”®ä¿¡æ¯ï¼š

é”™è¯¯ç±»å‹åˆ†æï¼š
- `Access denied for user 'root'@'10.4.214.53'`ï¼šè¿™æ˜¯MySQLå…¸å‹çš„è®¤è¯å¤±è´¥é”™è¯¯
- `(using password: YES)`ï¼šç¡®è®¤å¯†ç ç¡®å®è¢«æä¾›äº†ï¼Œä¸æ˜¯ç©ºå¯†ç é—®é¢˜
- è¿æ¥å­—ç¬¦ä¸²æ­£ç¡®ï¼š`jdbc:mysql://mysqldb:3306/mysql`æ˜¾ç¤ºè¿æ¥å‚æ•°æ— è¯¯

åˆæ­¥æ’é™¤çš„å¯èƒ½æ€§ï¼š
1. ç½‘ç»œè¿æ¥é—®é¢˜ï¼šå¦‚æœæ˜¯ç½‘ç»œé—®é¢˜ï¼Œé”™è¯¯ä¿¡æ¯ä¼šæ˜¯"Connection refused"æˆ–"Timeout"
2. æ•°æ®åº“æœåŠ¡é—®é¢˜ï¼šæ•°æ®åº“èƒ½å¤Ÿå“åº”è¿æ¥è¯·æ±‚ï¼Œè¯´æ˜æœåŠ¡æ­£å¸¸è¿è¡Œ
3. æƒé™é…ç½®é—®é¢˜ï¼šåŒæ ·çš„ç”¨æˆ·åœ¨å…¶ä»–ç¯å¢ƒèƒ½æ­£å¸¸å·¥ä½œ
4. å¯†ç ä¸ºç©ºï¼šæ—¥å¿—æ˜ç¡®æ˜¾ç¤º"using password: YES"

å¯ç–‘çš„çº¿ç´¢ï¼š
é”™è¯¯æç¤ºæ˜¾ç¤ºæ•°æ®åº“æ‹’ç»äº†ç”¨æˆ·årootçš„è®¿é—®ï¼Œä½†è¿™é‡Œæœ‰ä¸€ä¸ªå¾®å¦™çš„ç»†èŠ‚ï¼šæ•°æ®åº“è®¤è¯ç³»ç»Ÿå¯¹å­—ç¬¦ä¸²çš„ç²¾ç¡®åŒ¹é…è¦æ±‚æå…¶ä¸¥æ ¼ã€‚ä»»ä½•é¢å¤–çš„å­—ç¬¦ï¼ŒåŒ…æ‹¬ä¸å¯è§çš„æ§åˆ¶å­—ç¬¦ï¼Œéƒ½ä¼šå¯¼è‡´è®¤è¯å¤±è´¥ã€‚è¿™è®©æˆ‘ä»¬å¼€å§‹æ€€ç–‘Secretä¸­å­˜å‚¨çš„å€¼å¯èƒ½åŒ…å«äº†ä¸€äº›ä¸å¯è§çš„å­—ç¬¦ã€‚

3ï¸âƒ£ æ’æŸ¥è¿‡ç¨‹
ç¬¬ä¸€æ­¥ï¼šéªŒè¯åŸºç¡€é…ç½®
é¦–å…ˆæ£€æŸ¥äº†åº”ç”¨ç¨‹åºçš„æ•°æ®åº“è¿æ¥é…ç½®ï¼Œç¡®è®¤æ•°æ®åº“åœ°å€ã€ç«¯å£ã€ç”¨æˆ·åå’Œå¯†ç åœ¨é…ç½®æ–‡ä»¶ä¸­å‡æ­£ç¡®æ— è¯¯ã€‚åŒæ—¶éªŒè¯äº†æ•°æ®åº“æœåŠ¡çš„å¯è¾¾æ€§ï¼š
```bash
# æµ‹è¯•æ•°æ®åº“è¿æ¥æ€§
kubectl exec -it mysql-client -- mysql -h mysqldb -u root -p
# è¿æ¥æˆåŠŸï¼Œè¯´æ˜æ•°æ®åº“æœåŠ¡æ­£å¸¸ï¼Œå‡­è¯åœ¨æ•°æ®åº“ç«¯æ˜¯æœ‰æ•ˆçš„
```

ç¬¬äºŒæ­¥ï¼šæ£€æŸ¥Secretå†…å®¹
é€šè¿‡ä»¥ä¸‹å‘½ä»¤æŸ¥çœ‹ Secret çš„å†…å®¹ï¼šâ€œkubectl get secret db-credentials -o yamlâ€ï¼Œå‘ç° Secret ä¸­çš„ username å’Œ password å­—æ®µå­˜å‚¨çš„æ˜¯ Base64 ç¼–ç åçš„å­—ç¬¦ä¸²ï¼Œè€Œä¸æ˜¯ç›´æ¥å­˜å‚¨çš„æ˜æ–‡ï¼Œè¿™æ˜¯é¢„æœŸè¡Œä¸ºã€‚
è§£ç  Secret å†…å®¹ è§£ç åå‘ç°ï¼Œusername å’Œ password çš„å€¼ç¡®å®æ˜¯æ­£ç¡®çš„æ˜æ–‡å­—ç¬¦ä¸²ã€‚
åœ¨è¿›ä¸€æ­¥è¯¦ç»†å¯¹æ¯”åï¼Œå‘ç°é—®é¢˜å‡ºç°åœ¨ Secret çš„åˆ›å»ºæ–¹å¼ ä¸Šã€‚æ“ä½œäººå‘˜ä½¿ç”¨ä¸Šè¿°åˆ›å»ºsecretçš„æ–¹æ³•ä¸€åˆ›å»ºäº† Secretã€‚å…¶ä¸­ï¼Œusername.txt å’Œ password.txt ä¸¤ä¸ªæ–‡æœ¬æ–‡ä»¶æ˜¯é€šè¿‡æ–‡æœ¬ç¼–è¾‘å™¨ vim ç”Ÿæˆçš„ï¼Œåœ¨è¾“å…¥å†…å®¹åç›´æ¥ä¿å­˜é€€å‡ºï¼Œvim é»˜è®¤ä¼šåœ¨æ–‡ä»¶æœ«å°¾æ·»åŠ ä¸€ä¸ªæ¢è¡Œç¬¦ï¼ˆ\nï¼‰ã€‚å› æ­¤ï¼Œæ–‡ä»¶çš„çœŸå®å†…å®¹åˆ†åˆ«å˜æˆäº†(root\n)å’Œ(password\n), Base64 ç¼–ç ä¼šå°†æ¢è¡Œç¬¦ä¹Ÿè§†ä¸ºæœ‰æ•ˆå­—ç¬¦ï¼Œæœ€ç»ˆå¯¼è‡´ Secret ä¸­çš„å€¼åŒ…å«äº†é¢å¤–çš„æ¢è¡Œç¬¦ã€‚
	
4ï¸âƒ£ è§£å†³æ–¹æ³•

æ–¹æ¡ˆä¸€ï¼šä¿®æ­£æ–‡ä»¶åˆ›å»ºæ–¹å¼
ä½¿ç”¨echoå‘½ä»¤çš„-nå‚æ•°æ¥ç”Ÿæˆæ²¡æœ‰æ¢è¡Œç¬¦çš„æ–‡ä»¶ï¼š
```bash
# åˆ é™¤åŸæœ‰çš„Secret
kubectl delete secret db-credentials

# ä½¿ç”¨echo -nåˆ›å»ºä¸å«æ¢è¡Œç¬¦çš„æ–‡ä»¶
echo -n "root" > username.txt
echo -n "password" > password.txt

# é‡æ–°åˆ›å»ºSecret
kubectl create secret generic db-credentials --from-file=username=username.txt --from-file=password=password.txt

# éªŒè¯å†…å®¹ï¼ˆåº”è¯¥æ²¡æœ‰é¢å¤–çš„æ¢è¡Œç¬¦ï¼‰
kubectl get secret db-credentials -o jsonpath='{.data.username}' | base64 -d | hexdump -C
```

æ–¹æ¡ˆäºŒï¼šä½¿ç”¨å‘½ä»¤è¡Œå­—é¢é‡
ç›´æ¥åœ¨å‘½ä»¤ä¸­æŒ‡å®šå†…å®¹ï¼Œé¿å…æ–‡ä»¶æ“ä½œï¼š
```bash
# åˆ é™¤åŸæœ‰çš„Secret
kubectl delete secret db-credentials

# ä½¿ç”¨--from-literalç›´æ¥æŒ‡å®šå€¼
kubectl create secret generic db-credentials --from-literal=username=root --from-literal=password=password
```

æ–¹æ¡ˆä¸‰ï¼šä½¿ç”¨YAMLæ–‡ä»¶
åˆ›å»ºä¸€ä¸ªYAMLæ–‡ä»¶æ¥å®šä¹‰Secretï¼š
```yaml
apiVersion: v1
kind: Secret
metadata:
  name: db-credentials
type: Opaque
data:
  username: cm9vdA==  # base64ç¼–ç çš„"root"
  password: cGFzc3dvcmQ=  # base64ç¼–ç çš„"password"
```

éªŒè¯å’Œé‡å¯
```bash
# éªŒè¯Secretå†…å®¹æ­£ç¡®
kubectl get secret db-credentials -o yaml

# é‡å¯åº”ç”¨Podä»¥ä½¿ç”¨æ–°çš„Secret
kubectl rollout restart deployment web-app

# æ£€æŸ¥åº”ç”¨æ—¥å¿—ç¡®è®¤è¿æ¥æˆåŠŸ
kubectl logs -f deployment/web-app
```

é¢„é˜²æªæ–½
1. æ ‡å‡†åŒ–æ–‡ä»¶åˆ›å»ºæµç¨‹ï¼šåœ¨å›¢é˜Ÿä¸­æ¨å¹¿ä½¿ç”¨`echo -n`æˆ–`printf`å‘½ä»¤åˆ›å»ºSecretæ–‡ä»¶
2. è‡ªåŠ¨åŒ–éªŒè¯ï¼šåœ¨CI/CDæµç¨‹ä¸­åŠ å…¥Secretå†…å®¹éªŒè¯æ­¥éª¤
3. æ–‡æ¡£åŒ–æœ€ä½³å®è·µï¼šè®°å½•è¿™ç±»å¸¸è§é™·é˜±ï¼Œé¿å…é‡å¤è¸©å‘
4. ä½¿ç”¨å·¥å…·è¾…åŠ©ï¼šè€ƒè™‘ä½¿ç”¨ä¸“é—¨çš„Secretç®¡ç†å·¥å…·ï¼Œå¦‚Sealed Secretsæˆ–External Secrets Operator

é—®é¢˜ä¿®å¤åï¼Œåº”ç”¨ç¨‹åºæˆåŠŸè¿æ¥åˆ°æ•°æ®åº“ï¼ŒæœåŠ¡æ¢å¤æ­£å¸¸ã€‚

----- English

Kubernetes Secret Troubleshooting Case Study

Hello everyone, following up on the previous K8S Secret introduction, Secrets are typically used to securely store sensitive data (such as usernames, passwords, API keys, etc.) in clusters. Applications can access key-value pairs in Secrets by mounting files or environment variables, and use this information according to actual needs. However, improper configuration of Secrets may cause application runtime exceptions.

Real case: Application database connection failure due to Secret value content issues.

1ï¸âƒ£ Issue Description
Creating a secret in K8S cluster to store database connection information. In general, we create generic type secret with following two methods:
1. kubectl create secret generic db-credentials --from-file=username=username.txt --from-file=password=password.txt
   username.txt and password.txt are two text files, with content "root" and "password" respectively

2. kubectl create secret generic db-credentials --from-literal=username=root --from-literal=password=password
   Directly input username and password in the command line

These two methods create identical secrets, both can be viewed through kubectl get secret db-credentials -o yaml to see the secret content
However, the secret created by the second method is stored as string type in the yaml file, while the secret created by the first method is stored as binaryData type in the yaml file
There is no difference when using secrets created by these two methods, both can be viewed through kubectl get secret db-credentials -o yaml to see the secret content

According to the application's deploy yaml file configuration, the pod reads the username and password values from Secret db-credentials to connect to the database during startup. However, during startup, the program reported the following error log:
Unable to obtain connection from database (jdbc:mysql://mysqldb:3306/mysql?Unicode=true&characterEncoding=UTF-8) for user 'root': Access denied for user 'root'@'10.4.214.53' (using password: YES)

2ï¸âƒ£ Log Analysis
The application failed to successfully connect to the database using the provided username and password. The error message shows that the database denied access for username 'root'.

3ï¸âƒ£ Troubleshooting Process
Checking database configuration, first verified the application's database connection configuration, confirming that the database address, port, username, and password were all correct.
Using the command "kubectl get secret db-credentials -o yaml" to view the Secret content, found that the username and password fields in the Secret store Base64 encoded strings, not direct plaintext, which is expected behavior.
Decoding Secret content revealed that the username and password values were indeed correct plaintext strings.
After further detailed comparison, the problem was found to be in the Secret creation method. The operator used the first method mentioned above to create the Secret. The username.txt and password.txt text files were generated using the vim text editor, and after inputting content and saving directly, vim by default adds a newline character (\n) at the end of the file. Therefore, the actual file content became (root\n) and (password\n) respectively. Base64 encoding treats the newline character as a valid character, ultimately causing the Secret values to contain extra newline characters.

4ï¸âƒ£ Solution:
Directly use the following command to generate files without newline characters: echo -n "password" > password.txt, regenerate the secret
Use the second approach mentioned in figure 3 to recreate the secret, directly specifying content in the command
Restart the application, problem resolved

Sep 23, 2025
----- Chinese
base64 ç¼–ç ç®€ä»‹
å¤§å®¶å¥½ï¼Œä»Šå¤©æˆ‘ä»¬æ¥èŠèŠä¸Šç¯‡æåˆ°çš„base64ç¼–ç æ˜¯ä¸ªå•¥ä¸œä¸œï¼Œä»¥åŠå®ƒæœ‰å•¥ç”¨å¤„ã€‚
 Base64 æ˜¯ä¸€ç§å¸¸è§çš„ç¼–ç æ–¹æ³•ï¼Œå®ƒçš„ä¸»è¦ä½œç”¨æ˜¯å°†äºŒè¿›åˆ¶æ•°æ®ï¼ˆå¦‚å›¾ç‰‡ã€æ–‡ä»¶ã€ç‰¹æ®Šå­—ç¬¦ç­‰ï¼‰è½¬æ¢æˆæ–‡æœ¬æ ¼å¼ï¼Œä»¥ä¾¿åœ¨éœ€è¦ä¼ è¾“æˆ–å­˜å‚¨çš„åœºæ™¯ä¸­ä½¿ç”¨ã€‚æ¥äº†è§£ä¸‹å®ƒçš„ä½œç”¨å’Œä¸€äº›åº”ç”¨åœºæ™¯ 
	
1ï¸âƒ£ å¾®ä¿¡èŠå¤©ä¸­çš„å›¾ç‰‡ä¼ è¾“
å½“ä½ åœ¨å¾®ä¿¡æˆ–å…¶ä»–ç¤¾äº¤è½¯ä»¶ä¸­å‘é€å›¾ç‰‡æ—¶ï¼Œå›¾ç‰‡å®é™…ä¸Šæ˜¯ä»¥äºŒè¿›åˆ¶å½¢å¼å­˜å‚¨çš„ã€‚å¦‚æœç›´æ¥ä¼ è¾“äºŒè¿›åˆ¶æ•°æ®ï¼Œå¯èƒ½ä¼šå› ä¸ºç½‘ç»œä¼ è¾“é—®é¢˜æˆ–ä¸å…¼å®¹çš„å­—ç¬¦å¯¼è‡´å›¾ç‰‡æŸåã€‚
 ç¤¾äº¤è½¯ä»¶ä¼šå°†å›¾ç‰‡ç¼–ç æˆ Base64 æ ¼å¼ï¼Œå°†å›¾ç‰‡å†…å®¹è½¬æ¢æˆä¸€æ®µæ–‡æœ¬ï¼ˆå¦‚ iVBORw0KGgoAAAANSUhEUg...ï¼‰ã€‚è¿™æ®µæ–‡æœ¬æ›´é€‚åˆåœ¨ç½‘ç»œä¸Šä¼ è¾“ï¼Œç¡®ä¿å›¾ç‰‡å¯ä»¥è¢«æ¥æ”¶æ–¹æ­£ç¡®è§£ç å¹¶æŸ¥çœ‹ã€‚
2ï¸âƒ£ å‘ç”µå­é‚®ä»¶æ—¶çš„é™„ä»¶
åœ¨å‘ç”µå­é‚®ä»¶æ—¶æ·»åŠ çš„å›¾ç‰‡ã€PDFã€Word æ–‡æ¡£ç­‰é™„ä»¶ï¼Œå®é™…ä¸Šæ˜¯æ–‡ä»¶çš„äºŒè¿›åˆ¶æ•°æ®ã€‚å¦‚æœç›´æ¥å°†è¿™äº›æ•°æ®åµŒå…¥é‚®ä»¶æ­£æ–‡ï¼Œå¯èƒ½ä¼šå› ä¸ºç‰¹æ®Šå­—ç¬¦ï¼ˆå¦‚ \0ã€\n ç­‰ï¼‰å½±å“é‚®ä»¶çš„æ ¼å¼ã€‚
 é‚®ä»¶æœåŠ¡ä¼šå°†è¿™äº›æ–‡ä»¶é€šè¿‡ Base64 ç¼–ç ï¼Œè½¬æ¢æˆå¯è¯»çš„æ–‡æœ¬æ ¼å¼ï¼ŒåµŒå…¥åˆ°é‚®ä»¶å†…å®¹ä¸­ã€‚ä¾‹å¦‚ï¼Œå›¾ç‰‡è¢«ç¼–ç æˆä¸€æ®µ Base64 æ–‡æœ¬ï¼Œæœ€ç»ˆæ¥æ”¶æ–¹å¯ä»¥è§£ç è¿˜åŸæˆå›¾ç‰‡ã€‚å®ƒèƒ½ç¡®ä¿é™„ä»¶ä¸è¢«é”™è¯¯è§£æï¼Œé¿å…é‚®ä»¶å†…å®¹è¢«ç ´åï¼ŒåŒæ—¶å…¼å®¹ä¸åŒçš„é‚®ä»¶å®¢æˆ·ç«¯ã€‚
3ï¸âƒ£ ç”ŸæˆäºŒç»´ç 
åœ¨å•†åœºæ‰«ç ä»˜æ¬¾æ—¶ï¼Œæ‰‹æœºä¼šç”Ÿæˆä¸€ä¸ªäºŒç»´ç ã€‚è¿™äº›äºŒç»´ç å…¶å®æ˜¯å­—ç¬¦ä¸²çš„ç¼–ç å½¢å¼ï¼Œå¯èƒ½åŒ…å«ä½ çš„ä»˜æ¬¾ä¿¡æ¯ã€å•†æˆ·ä¿¡æ¯ç­‰ã€‚
 äºŒç»´ç ä¸­åŒ…å«çš„å­—ç¬¦ä¸²å¯èƒ½ä¼šç”¨ Base64 è¿›è¡Œç¼–ç ï¼Œæ¯”å¦‚ä½ çš„ä»˜æ¬¾ä¿¡æ¯è¢«è½¬æ¢æˆä¸€æ®µ Base64 æ–‡æœ¬ï¼Œå³å°†æ•°æ®å‹ç¼©æˆä¸€ç§æ ‡å‡†æ ¼å¼ï¼Œå†åµŒå…¥äºŒç»´ç ä¸­ã€‚ä¾¿äºå¿«é€Ÿè¯»å–ã€åŒæ—¶é¿å…ä¸­æ–‡æˆ–ç‰¹æ®Šå­—ç¬¦å¯¼è‡´çš„æ•°æ®é”™è¯¯ã€‚
4ï¸âƒ£ ä¸Šä¼ å¤´åƒåˆ°ç¤¾äº¤åª’ä½“
æœ‰äº›å¹³å°ä¼šå°†å›¾ç‰‡ç¼–ç æˆ Base64 æ ¼å¼ï¼Œç„¶åé€šè¿‡ç½‘ç»œä¼ è¾“åˆ°æœåŠ¡å™¨ã€‚è¿™ç§æ–¹å¼å¯ä»¥é¿å…ç½‘ç»œä¸ç¨³å®šæ—¶æ–‡ä»¶æŸåã€‚
å¦‚æœæŠŠä½ çš„æ–‡ä»¶ã€å›¾ç‰‡ã€å¯†ç æƒ³è±¡æˆâ€œäºŒè¿›åˆ¶ä»£ç â€ï¼Œå®ƒä»¬å¯èƒ½åŒ…å«â€œçœ‹ä¸æ‡‚çš„å­—ç¬¦â€æˆ–â€œç‰¹æ®Šç¬¦å·â€ã€‚è¿™äº›å­—ç¬¦åœ¨ä¼ è¾“æˆ–è€…ä¿å­˜æ—¶æœ‰æ—¶ä¼šå¯¼è‡´é—®é¢˜ã€‚
Base64 å°±åƒä¸€ä¸ªâ€œç¿»è¯‘å™¨â€ï¼ŒæŠŠè¿™äº›â€œçœ‹ä¸æ‡‚çš„å­—ç¬¦â€ç¿»è¯‘æˆç”±å­—æ¯ã€æ•°å­—ç»„æˆçš„â€œäººç±»å‹å¥½å‹å­—ç¬¦â€ã€‚è¿™è®©å®ƒä»¬æ›´å®¹æ˜“åœ¨ç½‘ç»œä¸­ä¼ è¾“ã€åœ¨ç³»ç»Ÿä¸­å­˜å‚¨ï¼Œä¸”ä¸ä¼šæŸåæˆ–ä¸¢å¤±ã€‚
è™½ç„¶æˆ‘ä»¬å¯èƒ½å¯Ÿè§‰ä¸åˆ° Base64 çš„å­˜åœ¨ï¼Œä½†å®ƒèƒŒåé»˜é»˜åœ°è®©æ•°æ®ä¼ è¾“æ›´åŠ ç¨³å®šå’Œå®‰å…¨ã€‚

----- English

Introduction to Base64 Encoding

Hello everyone, today let's talk about what Base64 encoding is, which we mentioned in the previous article, and what it's used for.

Base64 is a common encoding method whose main purpose is to convert binary data (such as images, files, special characters, etc.) into text format for use in scenarios that require transmission or storage. Let's understand its functions and some application scenarios.

1ï¸âƒ£ Image Transmission in WeChat Chat
When you send images in WeChat or other social software, the images are actually stored in binary format. If binary data is transmitted directly, the images might get corrupted due to network transmission issues or incompatible characters.
Social software will encode images into Base64 format, converting the image content into a text string (like iVBORw0KGgoAAAANSUhEUg...). This text string is more suitable for network transmission, ensuring that images can be correctly decoded and viewed by the recipient.

2ï¸âƒ£ Email Attachments
When adding attachments like images, PDFs, Word documents to emails, these are actually binary data of files. If this data is directly embedded in the email body, it might affect the email format due to special characters (like \0, \n, etc.).
Email services will encode these files through Base64, converting them into readable text format and embedding them in the email content. For example, images are encoded into Base64 text, and ultimately the recipient can decode and restore them back to images. This ensures attachments are not incorrectly parsed, prevents email content corruption, and maintains compatibility with different email clients.

3ï¸âƒ£ QR Code Generation
When making payments by scanning codes in shopping malls, your phone generates a QR code. These QR codes are actually encoded forms of strings that may contain your payment information, merchant information, etc.
The strings contained in QR codes may use Base64 encoding, for example, your payment information is converted into Base64 text, compressing the data into a standard format before embedding it in the QR code. This facilitates quick reading while avoiding data errors caused by Chinese characters or special characters.

4ï¸âƒ£ Uploading Avatars to Social Media
Some platforms encode images into Base64 format and then transmit them to servers over the network. This approach can prevent file corruption when the network is unstable.

If you imagine your files, images, and passwords as "binary code," they may contain "incomprehensible characters" or "special symbols." These characters sometimes cause problems during transmission or storage.
Base64 acts like a "translator," translating these "incomprehensible characters" into "human-friendly characters" composed of letters and numbers. This makes them easier to transmit over networks and store in systems without damage or loss.
Although we may not notice the existence of Base64, it silently makes data transmission more stable and secure behind the scenes.

Sep 28,2025
----- Chinese
æ¢ç´¢SREï¼šåˆ«è®©ä½ çš„åº”ç”¨æ—¥å¿—å˜æˆâ€œè¯ç—¨â€
å¤§å®¶å¥½ï¼Œä»Šå¤©æˆ‘æ¥åˆ†äº«ä¸€ä¸ªåœ¨ç”Ÿäº§ç¯å¢ƒä¸­é‡åˆ°çš„å…¸å‹é—®é¢˜ï¼šåº”ç”¨æ—¥å¿—DEBUG/TRACEè®¾ç½®çš„å¨åŠ›ã€‚è¿™æ˜¯ä¸€ä¸ªçœ‹ä¼¼å¾®ä¸è¶³é“çš„é…ç½®å˜æ›´ï¼Œå´èƒ½åœ¨ç¬é—´è®©æ•´ä¸ªç³»ç»Ÿé™·å…¥èµ„æºè€—å°½çš„å±æœºã€‚ä½œä¸ºSREï¼Œæˆ‘ä»¬ç»å¸¸ä¼šé‡åˆ°è¿™æ ·çš„æƒ…å†µï¼šå¼€å‘å›¢é˜Ÿä¸ºäº†æ’æŸ¥é—®é¢˜ä¸´æ—¶è°ƒæ•´äº†æ—¥å¿—çº§åˆ«ï¼Œå´å¿˜è®°åœ¨é—®é¢˜è§£å†³ååŠæ—¶æ¢å¤ï¼Œæœ€ç»ˆå¯¼è‡´ç”Ÿäº§ç¯å¢ƒå‡ºç°æ€§èƒ½é—®é¢˜ã€‚

è¿™ä¸ªæ¡ˆä¾‹ä¸ä»…å±•ç¤ºäº†æ—¥å¿—çº§åˆ«å¯¹ç³»ç»Ÿæ€§èƒ½çš„ç›´æ¥å½±å“ï¼Œæ›´é‡è¦çš„æ˜¯æ­ç¤ºäº†åœ¨DevOpsåä½œä¸­ï¼Œçœ‹ä¼¼ç®€å•çš„é…ç½®ç®¡ç†å¦‚ä½•æˆä¸ºç³»ç»Ÿç¨³å®šæ€§çš„éšæ‚£ã€‚è®©æˆ‘ä»¬ä¸€èµ·æ¥çœ‹çœ‹è¿™ä¸ª"å°é…ç½®ï¼Œå¤§å½±å“"çš„çœŸå®æ•…äº‹ã€‚
1ï¸âƒ£ é—®é¢˜æè¿°ï¼š
æŸæ—¥ï¼Œç›‘æ§ç³»ç»Ÿæ˜¾ç¤ºå‡ å°è™šæ‹Ÿæœºå¼‚å¸¸âš ï¸
å‘Šè­¦ä¿¡æ¯æ±‡æ€»ï¼š
- CPUå ç”¨è¿‡é«˜ï¼šä»å¹³æ—¶çš„20-30%é£™å‡åˆ°80-90%
- å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜ï¼šä»æ­£å¸¸çš„60%å¢é•¿åˆ°95%ä»¥ä¸Š
- ç½‘ç»œå¸¦å®½å ç”¨è¿‡é«˜ï¼šå‡ºç«™æµé‡è¶…è¿‡30MB/sï¼Œæ¯”å¹³æ—¶é«˜å‡º10å€
- ç£ç›˜I/Oå¼‚å¸¸ï¼šå†™å…¥æ“ä½œé¢‘ç¹ï¼ŒIOPSè¾¾åˆ°å¹³æ—¶çš„5-8å€

ä¸šåŠ¡å½±å“ï¼š
- åº”ç”¨å“åº”æ—¶é—´ä»å¹³å‡200mså¢åŠ åˆ°2-3ç§’
- éƒ¨åˆ†APIè¯·æ±‚å¼€å§‹å‡ºç°è¶…æ—¶
- ç”¨æˆ·å¼€å§‹åé¦ˆé¡µé¢åŠ è½½ç¼“æ…¢
- æ•°æ®åº“è¿æ¥æ± æ¥è¿‘é¥±å’Œ

æ—¶é—´çº¿ç‰¹å¾ï¼š
è¿™äº›å¼‚å¸¸æŒ‡æ ‡å‡ ä¹åœ¨åŒä¸€æ—¶é—´ç‚¹å¼€å§‹å‡ºç°ï¼Œä¸”å‘ˆç°æŒç»­ä¸Šå‡è¶‹åŠ¿ï¼Œæ’é™¤äº†æ¸è¿›æ€§çš„æ€§èƒ½é€€åŒ–ï¼Œæ›´åƒæ˜¯æŸä¸ªé…ç½®å˜æ›´æˆ–ä»£ç éƒ¨ç½²å¼•èµ·çš„çªå‘é—®é¢˜ã€‚
	
2ï¸âƒ£ æ£€æŸ¥è¿‡ç¨‹ï¼š

ç¬¬ä¸€æ­¥ï¼šå…³è”æ€§åˆ†æ
ç»è¿‡æ£€æŸ¥ï¼Œå‘ç°å‡ å°å¼‚å¸¸çš„è™šæ‹Ÿæœºéƒ½éƒ¨ç½²äº†åŒä¸€ä¸ªJavaåº”ç”¨ï¼Œå¹¶äºåŒä¸€æ—¶é—´è¿›è¡Œäº†æ•´ä½“ç‰ˆæœ¬æ›´æ–°ã€‚è¿™ä¸ªæ—¶é—´ç‚¹çš„é‡åˆæ€§ç«‹å³å¼•èµ·äº†æˆ‘ä»¬çš„æ³¨æ„ï¼Œå¾ˆå¯èƒ½æ˜¯è¿™æ¬¡éƒ¨ç½²å¼•å…¥äº†é—®é¢˜ã€‚

ç¬¬äºŒæ­¥ï¼šåº”ç”¨å±‚é¢æ’æŸ¥
æ£€æŸ¥Grafanaçš„å“åº”æŒ‡æ ‡å›¾ï¼Œå‘ç°Javaåº”ç”¨æœ¬èº«çš„å…³é”®æŒ‡æ ‡è¡¨ç°å¦‚ä¸‹ï¼š
- JVMå †å†…å­˜ä½¿ç”¨ç‡ï¼šç•¥æœ‰ä¸Šå‡ï¼Œä½†ä»åœ¨æ­£å¸¸èŒƒå›´å†…
- GCçŠ¶æ€ï¼šFull GCé¢‘ç‡ç•¥å¾®åé«˜ï¼Œä½†ä¸è¶³ä»¥è§£é‡Šå¦‚æ­¤ä¸¥é‡çš„æ€§èƒ½é—®é¢˜
- çº¿ç¨‹æ± çŠ¶æ€ï¼šæ´»è·ƒçº¿ç¨‹æ•°æ­£å¸¸ï¼Œæ— æ˜æ˜¾é˜»å¡
- æ•°æ®åº“è¿æ¥æ± ï¼šä½¿ç”¨ç‡æ­£å¸¸ï¼Œå“åº”æ—¶é—´ç•¥æœ‰å¢åŠ 

ç¬¬ä¸‰æ­¥ï¼šç³»ç»Ÿå±‚é¢æ·±å…¥åˆ†æ
- é€šè¿‡`top`å’Œ`htop`å‘½ä»¤å‘ç°ï¼ŒCPUä¸»è¦æ¶ˆè€—åœ¨ç”¨æˆ·æ€ï¼Œè€Œéå†…æ ¸æ€
- ä½¿ç”¨`iotop`å‘ç°å¤§é‡çš„ç£ç›˜å†™å…¥æ“ä½œï¼Œä¸»è¦é›†ä¸­åœ¨æ—¥å¿—ç›®å½•
- ç½‘ç»œæµé‡åˆ†ææ˜¾ç¤ºï¼Œå¤§éƒ¨åˆ†å‡ºç«™æµé‡éƒ½æ˜¯å‘å¾€æ—¥å¿—æ”¶é›†ç³»ç»Ÿï¼ˆå¦‚ELKï¼‰

ç¬¬å››æ­¥ï¼šæ—¥å¿—ç³»ç»Ÿæ£€æŸ¥
æ·±å…¥æ—¥å¿—ç³»ç»Ÿæ£€æŸ¥åï¼Œå‘ç°äº†é—®é¢˜çš„æ ¹æºï¼šåº”ç”¨åœ¨ç‰ˆæœ¬æ›´æ–°åï¼Œæ—¥å¿—é…ç½®æ–‡ä»¶ä¸­çš„æ—¥å¿—çº§åˆ«ä»INFOè¢«æ„å¤–ä¿®æ”¹ä¸ºDEBUGã€‚è¿™ä¸ªçœ‹ä¼¼å¾®å°çš„å˜æ›´ï¼Œå¯¼è‡´äº†ï¼š
- æ—¥å¿—è¾“å‡ºé‡å¢åŠ äº†20-50å€
- æ¯ç§’äº§ç”Ÿçš„æ—¥å¿—æ¡æ•°ä»å‡ ç™¾æ¡å¢åŠ åˆ°æ•°ä¸‡æ¡
- å•ä¸ªæ—¥å¿—æ–‡ä»¶å¤§å°ä»å‡ MBå¿«é€Ÿå¢é•¿åˆ°å‡ GB
	
3ï¸âƒ£ é—®é¢˜ä¿®å¤
æŠŠåº”ç”¨çš„æ—¥å¿—è®¾å®šæ”¹ä¸ºINFOä»¥åï¼Œè™šæ‹ŸæœºçŠ¶æ€æ¢å¤æ­£å¸¸


4ï¸âƒ£ å…³äºåº”ç”¨æ—¥å¿—
å¯¹äºéƒ¨ç½²åœ¨ç”Ÿäº§ç¯å¢ƒä¸­çš„æœåŠ¡ï¼Œæ—¥å¿—é‚£å¯æ˜¯æ’æŸ¥é—®é¢˜çš„â€œç¦å°”æ‘©æ–¯â€ï¼Œæ˜¯äº†è§£åº”ç”¨â€œå†…å¿ƒæ´»åŠ¨â€çš„çª—å£ã€‚å¦‚æœä½ çš„åº”ç”¨æ˜¯ä¸ªæ­£åœ¨æ¬ç –çš„ç å†œï¼Œæ—¥å¿—çº§åˆ«å°±åƒæ˜¯ç å†œå‘ä¸Šçº§æ±‡æŠ¥çš„å·¥ä½œè¿›åº¦ï¼Œè¯·å‚è€ƒå›¾2ã€‚

æ—¥å¿—çº§åˆ«çš„å±‚æ¬¡ç»“æ„ï¼š
ä»å›¾2ä¸­å¯ä»¥çœ‹å‡ºï¼Œæ—¥å¿—çº§åˆ«ä»ä¸Šå¾€ä¸‹ï¼Œä¿¡æ¯é‡æ˜¯é€’å¢çš„ï¼Œå°±åƒä¸€ä¸ªä¿¡æ¯è¿‡æ»¤å™¨ï¼š

- ERRORï¼šåªæŠ¥å‘Šä¸¥é‡é”™è¯¯ï¼Œç›¸å½“äº"è€æ¿ï¼Œå‡ºå¤§äº‹äº†ï¼"
- WARNï¼šæŠ¥å‘Šæ½œåœ¨é—®é¢˜ï¼Œç›¸å½“äº"è€æ¿ï¼Œè¿™é‡Œå¯èƒ½æœ‰é£é™©"
- INFOï¼šæŠ¥å‘Šå…³é”®ä¸šåŠ¡æµç¨‹ï¼Œç›¸å½“äº"è€æ¿ï¼Œæˆ‘å®Œæˆäº†è¿™ä¸ªä»»åŠ¡"
- DEBUGï¼šæŠ¥å‘Šè¯¦ç»†çš„æ‰§è¡Œè¿‡ç¨‹ï¼Œç›¸å½“äº"è€æ¿ï¼Œæˆ‘æ¯ä¸€æ­¥éƒ½åœ¨è¿™æ ·åš..."
- TRACEï¼šæŠ¥å‘Šæœ€ç»†ç²’åº¦çš„æ‰§è¡Œç»†èŠ‚ï¼Œç›¸å½“äº"è€æ¿ï¼Œæˆ‘ç°åœ¨åœ¨æƒ³ä»€ä¹ˆï¼Œå‡†å¤‡åšä»€ä¹ˆï¼Œæ­£åœ¨åšä»€ä¹ˆ..."

DEBUGå’ŒTRACEçš„"è¯ç—¨"ç‰¹æ€§ï¼š
å°¤å…¶æ˜¯DEBUGå’ŒTRACEï¼Œå®ƒä»¬ç®€ç›´å°±æ˜¯æ—¥å¿—ç•Œçš„â€œè¯ç—¨â€ï¼Œåªè¦èƒ½è¯´çš„ä¸€è‚¡è„‘çš„éƒ½è¦è·Ÿä½ è¯´ï¼åœ¨æˆ‘ä»¬è¿™ä¸ªæ¡ˆä¾‹ä¸­ï¼š
- DEBUGçº§åˆ«ä¼šè®°å½•æ¯ä¸ªæ–¹æ³•çš„è¿›å…¥å’Œé€€å‡º
- TRACEçº§åˆ«ç”šè‡³ä¼šè®°å½•æ¯ä¸ªå˜é‡çš„èµ‹å€¼è¿‡ç¨‹
- ä¸€ä¸ªç®€å•çš„HTTPè¯·æ±‚å¯èƒ½äº§ç”Ÿå‡ ç™¾æ¡æ—¥å¿—è®°å½•
- æ•°æ®åº“æŸ¥è¯¢ä¼šè®°å½•SQLè¯­å¥ã€å‚æ•°ç»‘å®šã€ç»“æœé›†å¤„ç†ç­‰æ¯ä¸ªç»†èŠ‚

ç”Ÿäº§ç¯å¢ƒçš„æ—¥å¿—ç­–ç•¥ï¼š
åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œæˆ‘ä»¬é€šå¸¸é‡‡ç”¨"é‡‘å­—å¡”"ç­–ç•¥ï¼š
- ERROR/WARNï¼šå…¨é‡è®°å½•ï¼Œè¿™äº›æ˜¯å¿…é¡»å…³æ³¨çš„
- INFOï¼šé€‰æ‹©æ€§è®°å½•å…³é”®ä¸šåŠ¡èŠ‚ç‚¹
- DEBUG/TRACEï¼šé»˜è®¤å…³é—­ï¼Œä»…åœ¨æ•…éšœæ’æŸ¥æ—¶ä¸´æ—¶å¼€å¯
	
5ï¸âƒ£ DEBUG/TRACEçš„åŒé¢æ€§åˆ†æï¼š
é‚£ä¹ˆå¼€å¯äº†DEBUG/TRACEçš„æ—¥å¿—è¾“å‡ºï¼Œåˆ°åº•æ˜¯å¥½æ˜¯åï¼Ÿè¯·å‚è€ƒå›¾3
âœ… ä¼˜åŠ¿æ–¹é¢ï¼š
1. é—®é¢˜æ’æŸ¥èƒ½åŠ›å¼ºï¼šèƒ½å¤Ÿæä¾›æœ€è¯¦ç»†çš„æ‰§è¡Œè½¨è¿¹ï¼Œå¿«é€Ÿå®šä½é—®é¢˜æ ¹å› 
2. å¼€å‘è°ƒè¯•å‹å¥½ï¼šåœ¨å¼€å‘å’Œæµ‹è¯•ç¯å¢ƒä¸­ï¼Œèƒ½å¤Ÿæ¸…æ™°çœ‹åˆ°ä»£ç æ‰§è¡Œæµç¨‹
3. æ€§èƒ½ç“¶é¢ˆè¯†åˆ«ï¼šå¯ä»¥ç²¾ç¡®æµ‹é‡æ¯ä¸ªæ–¹æ³•çš„æ‰§è¡Œæ—¶é—´
4. ä¸šåŠ¡é€»è¾‘éªŒè¯ï¼šèƒ½å¤ŸéªŒè¯å¤æ‚ä¸šåŠ¡é€»è¾‘çš„æ¯ä¸ªåˆ†æ”¯æ˜¯å¦æŒ‰é¢„æœŸæ‰§è¡Œ

âŒ åŠ£åŠ¿æ–¹é¢ï¼š
1. æ€§èƒ½å½±å“ä¸¥é‡ï¼š
   - CPUæ¶ˆè€—ï¼šå­—ç¬¦ä¸²æ‹¼æ¥ã€æ ¼å¼åŒ–æ“ä½œæ¶ˆè€—å¤§é‡CPUèµ„æº
   - å†…å­˜å‹åŠ›ï¼šå¤§é‡æ—¥å¿—å¯¹è±¡åˆ›å»ºå¢åŠ GCå‹åŠ›
   - I/Oç“¶é¢ˆï¼šé¢‘ç¹çš„ç£ç›˜å†™å…¥æ“ä½œå½±å“æ•´ä½“æ€§èƒ½
   - ç½‘ç»œå¸¦å®½ï¼šæ—¥å¿—ä¼ è¾“å ç”¨å¤§é‡ç½‘ç»œèµ„æº

2. å­˜å‚¨æˆæœ¬é«˜æ˜‚ï¼š
   - æ—¥å¿—æ–‡ä»¶å¤§å°å¯èƒ½å¢é•¿20-50å€
   - éœ€è¦æ›´å¤šçš„ç£ç›˜ç©ºé—´å’Œå¤‡ä»½èµ„æº
   - æ—¥å¿—æ£€ç´¢å’Œåˆ†æå˜å¾—å›°éš¾

3. å®‰å…¨é£é™©ï¼š
   - å¯èƒ½æ„å¤–è®°å½•æ•æ„Ÿä¿¡æ¯ï¼ˆå¯†ç ã€ä»¤ç‰Œç­‰ï¼‰
   - å¢åŠ äº†ä¿¡æ¯æ³„éœ²çš„é£é™©

ç”Ÿäº§ç¯å¢ƒæœ€ä½³å®è·µï¼š
é€šå¸¸ï¼ŒDEBUGçº§åˆ«çš„æ—¥å¿—ä¸»è¦ç”¨äºå¼€å‘å’Œæµ‹è¯•ç¯å¢ƒã€‚åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œä¸ºäº†ä¿è¯åº”ç”¨æ€§èƒ½ã€ç¨³å®šæ€§å’Œå®‰å…¨æ€§ï¼Œåº”é¿å…é•¿æœŸå¼€å¯DEBUGçº§åˆ«æ—¥å¿—ã€‚

ä¸´æ—¶å¼€å¯ç­–ç•¥ï¼š
å¦‚æœéœ€è¦åœ¨ç”Ÿäº§ç¯å¢ƒæ’æŸ¥ç‰¹å®šé—®é¢˜ï¼Œå¯ä»¥è€ƒè™‘ï¼š
1. æ—¶é—´é™åˆ¶ï¼šè®¾ç½®è‡ªåŠ¨å…³é—­æ—¶é—´ï¼Œé¿å…å¿˜è®°å›æ»š
2. èŒƒå›´é™åˆ¶ï¼šåªå¯¹ç‰¹å®šæ¨¡å—æˆ–ç‰¹å®šç”¨æˆ·å¼€å¯DEBUGæ—¥å¿—
3. æ¡ä»¶è§¦å‘ï¼šåŸºäºç‰¹å®šæ¡ä»¶ï¼ˆå¦‚é”™è¯¯ç‡é˜ˆå€¼ï¼‰è‡ªåŠ¨å¼€å¯
4. å®æ—¶ç›‘æ§ï¼šç›‘æ§ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µï¼Œå¿…è¦æ—¶è‡ªåŠ¨é™çº§

ç›‘æ§å’Œå‘Šè­¦æœºåˆ¶ï¼š
ç›‘æ§ç³»ç»Ÿä¹Ÿéœ€è¦å¯¹DEBUG/TRACEçš„æ—¥å¿—è®¾ç½®ä»¥åŠè¾“å‡ºçŠ¶æ€æœ‰ä¸€ä¸ªå®æ—¶çš„ç›‘æ§ï¼ŒåŒ…æ‹¬ï¼š
- æ—¥å¿—çº§åˆ«é…ç½®å˜æ›´å‘Šè­¦
- æ—¥å¿—è¾“å‡ºé‡å¼‚å¸¸å‘Šè­¦
- ç³»ç»Ÿèµ„æºä½¿ç”¨ç‡å‘Šè­¦
- è‡ªåŠ¨åŒ–çš„æ—¥å¿—çº§åˆ«å›æ»šæœºåˆ¶

è¿™æ ·å¯ä»¥åœ¨é—®é¢˜å‡ºç°çš„ç¬¬ä¸€æ—¶é—´ä»‹å…¥è¿›è¡Œæ£€æŸ¥ã€ç¡®è®¤ï¼Œå¿…è¦æ—¶è¿›è¡Œä¿®æ”¹ï¼Œé¿å…åƒæœ¬æ¡ˆä¾‹è¿™æ ·çš„ç”Ÿäº§äº‹æ•…ã€‚

----- English

Exploring SRE: Don't Let Your Application Logs Become "Chatterboxes"

Hello everyone, today I'm sharing a production environment issue I encountered: the power of application log DEBUG/TRACE settings.

1ï¸âƒ£ Problem Description: One day, the monitoring system showed several virtual machines with anomalies:
- High CPU usage
- High memory usage
- High network bandwidth usage (> 30 MB/s)

2ï¸âƒ£ Investigation Process: After investigation, we found that all the abnormal virtual machines had deployed the same Java application and had undergone a complete version update at the same time.
Checking Grafana's response metric charts, we found that the Java application's own JVM metrics and GC status were relatively normal, just slightly elevated.
Log system inspection revealed that after the version update, the application had enabled DEBUG log settings.

3ï¸âƒ£ Problem Resolution: After changing the application's log setting to INFO, the virtual machine status returned to normal.

4ï¸âƒ£ About Application Logs: For services deployed in production environments, logs are the "Sherlock Holmes" for troubleshooting issues - they're the window into understanding the application's "inner thoughts." If your application is a developer working hard, log levels are like the developer reporting work progress to supervisors, please refer to Figure 2.
From the chart, you can see that log levels increase in information volume from top to bottom. Especially DEBUG and TRACE - they're simply the "chatterboxes" of the logging world, telling you everything they possibly can!

5ï¸âƒ£ So is enabling DEBUG/TRACE log output good or bad? Please refer to Figure 3.
Typically, DEBUG level logs are mainly used in development and testing environments. In production environments, to ensure application performance, stability, and security, long-term enabling of DEBUG level logs should be avoided. If you need to troubleshoot specific issues in production, consider temporarily and selectively enabling DEBUG logs, and promptly disable them or revert to higher levels (such as INFO or ERROR) after resolving the problem. Additionally, monitoring systems need real-time monitoring of DEBUG/TRACE log settings and output status, allowing for immediate intervention, inspection, confirmation, and necessary modifications when issues arise.

