Sep 7, 2025
----- Chinese
探索SRE：站在可靠性的最前线
大家好，我是一名SRE，也就是“网站可靠性工程师”（Site Reliability Engineer）。说白了，我的工作就是让各种网站、App和服务不出问题，能稳定、高效地运行。我身后没有披风，但我觉得自己是某种意义上的“幕后英雄”，因为我的任务就是拯救系统崩溃，保障用户体验。那么，我每天到底在做什么呢？跟我一起看看吧！
	
可靠性：SRE的核心使命
作为SRE，我最重要的职责就是让系统“靠谱”。比如，当你深夜点外卖、用App买电影票或者刷短视频时，这些服务都得正常运行，不然你可能会抓狂，对吧？为了做到这一点，我会设定一些目标，比如“系统99.9%的时间都要在线”，然后通过优化架构、修复漏洞，来让系统变得稳定又坚强。
	
监控：SRE的千里眼
我的日常离不开“监控”（Monitoring）。我会用一些酷炫的工具，比如Prometheus和Grafana，还有其他一系列的工具来盯着系统的运行状态。比如，流量是不是突然飙升了？服务器是不是开始发热了？一旦发现异常，我就能第一时间出手，防止问题扩大。可以说，监控工具是我的眼睛，帮我随时掌握系统的健康状态。
	
自动化：解放双手的秘诀
说实话，重复干同样的事情真的很无聊。所以，SRE的座右铭之一就是“能自动化的绝不手动”。我的工作之一就是写脚本和程序，让系统自动处理一些常见问题，比如自动扩展服务器容量，自动重启服务，甚至自动预警。有了自动化，我可以把时间花在更重要、更有挑战的事情上。
	
可扩展性：从容应对高峰
我经常跟朋友开玩笑说，我的工作就是让“双十一”不宕机。每次大促、直播爆火时，流量都会暴涨，这对系统是个大考验。而我的任务就是提前设计好系统，让它能灵活扩展，扛住压力。看到用户买买买、刷刷刷时，我都觉得特别骄傲，因为这背后有我的努力。
	
事件管理：危机中的冷静大脑
当然，天有不测风云，再完美的系统也有可能突然崩了。这个时候，我就变成了“危机处理专家”。当警报响起，我会迅速定位问题，想办法恢复服务，同时组织团队总结复盘，避免同样的错误再次发生。虽然紧张，但每次成功解决问题后，那种成就感真的超爽。
	
说真的，SRE这个职业对我来说不只是工作，更是一种挑战和乐趣。我既能动手写代码，又能设计系统，还能快速解决问题，每天都充满新鲜感。
接下来，我会总结并分享SRE日常工作内容，同时欢迎大家和我一起互动，互相交流 ：）

----- English 
Exploring SRE: Standing at the Forefront of Reliability
Hi everyone, I'm an SRE, which stands for "Site Reliability Engineer." Simply put, my job is to ensure that various websites, apps, and services don't break down and can run stably and efficiently. I may not wear a cape, but I consider myself a "behind-the-scenes hero" in some sense, because my mission is to rescue system crashes and ensure user experience. So, what exactly do I do every day? Let's take a look together!

Reliability: The Core Mission of SRE
As an SRE, my most important responsibility is to make systems "reliable." For example, when you order takeout late at night, buy movie tickets through an app, or scroll through short videos, these services must run normally, otherwise you might go crazy, right? To achieve this, I set some goals, like "the system must be online 99.9% of the time," and then optimize architecture and fix vulnerabilities to make the system stable and robust.

Monitoring: SRE's All-Seeing Eyes
My daily work is inseparable from "monitoring." I use some cool tools like Prometheus and Grafana, along with other series of tools to keep an eye on the system's operational status. For instance, is traffic suddenly spiking? Are the servers starting to overheat? Once I detect anomalies, I can take action immediately to prevent problems from escalating. You could say that monitoring tools are my eyes, helping me stay on top of the system's health status at all times.

Automation: The Secret to Freeing Your Hands
Honestly, doing the same repetitive tasks is really boring. So, one of SRE's mottos is "automate everything that can be automated, never do it manually." Part of my job is writing scripts and programs to let the system automatically handle common issues, such as automatically scaling server capacity, automatically restarting services, and even automatic alerts. With automation, I can spend time on more important and challenging tasks.

Scalability: Handling Peak Traffic with Ease
I often joke with friends that my job is to prevent systems during events like Black Friday from crashing. Every time there are major promotions or live streams go viral, traffic surges dramatically, which is a major test for the system. My task is to design the system in advance so it can scale flexibly and withstand the pressure. When I see users shopping and browsing frantically, I feel particularly proud because my efforts are behind all of this.

Incident Management: The Calm Mind in Crisis
Of course, unexpected things happen, and even the most perfect systems can suddenly crash. At times like these, I become a "crisis management expert." When alarms go off, I quickly locate problems, find ways to restore services, while organizing the team to summarize and review to prevent the same mistakes from happening again. Although it's stressful, the sense of achievement after successfully solving problems is absolutely amazing.

Honestly, the SRE profession is not just a job for me, but also a challenge and source of enjoyment. I can both write code and design systems, as well as solve problems quickly. Every day is full of fresh experiences.

Next, I will summarize and share the daily work content of SRE, and I welcome everyone to interact and exchange ideas with me :)

Sep 9,2025
----- Chinese
探索SRE：K8S集群POD调度问题描述
大家好，今天分享一个日常工作中遇到的产线问题，欢迎大家一起头脑风暴。
我们使用 Kubernetes 集群部署了一系列服务，其中一个数据收集app，用于扫描约 8,500 个对象并收集相应数据，配置如下：
K8S 集群：3 个主节点，4 个工作节点。
数据收集app:部署了 6 个 Pod (replicas=6)，以提高任务的并发处理能力。
app运行时为每个扫描对象创建线程，每个线程负责处理一部分扫描任务。
app通过 K8S 的 Service 端口以 1 分钟的间隔被持续调用。
Service 通过负载均衡将请求分配到具体的 Pod ，并执行扫描任务。
﻿#SRE﻿
问题描述
服务通过CI/CD部署后，所有POD副本都被调度到了工作节点 2(如图)，而没有均匀分布到其他工作节点。这种调度不均衡导致如下问题：
每个 Pod 创建大量线程处理扫描任务。当 6 个 Pod 全部集中在工作节点 2 上运行时，线程数量快速增长，最终耗尽了节点的线程资源。
当节点2部署的任意服务尝试创建更多线程时，因节点已达到线程上限，导致服务报错，日志显示：OpenBLAS blas_thread_init: pthread_create failed for thread 44 of 48: Resource temporarily unavailable。该错误表明线程创建失败，原因是节点无法为新线程分配资源。
工作节点 2 彻底崩溃，无法继续运行任务，对整个集群的稳定性造成了影响。
参考点
Linux节点线程上限为 65,535（默认配置，通过 /proc/sys/kernel/threads-max 和 ulimit -u 控制）。
数据收集服务依赖多线程模型，每个线程负责一部分扫描对象。由于扫描对象数量较大且间隔一分钟被调用，服务需要创建足够多的线程并发执行任务，以保障数据收集的效率。
默认线程上限（65,535）是为了支持高并发需求。如果上限较低，服务可能无法充分利用 CPU 资源，导致性能瓶颈。
问题总结
由于 K8S 调度问题，所有 Pod 被集中调度到单个工作节点，导致线程资源迅速耗尽，最终引发节点崩溃。尽管线程上限（65,535）设计用于支持高并发需求，但在资源分配不均的情况下，这一上限反而成为了瓶颈，限制了服务的正常运行。
由于平台对笔记的篇幅限制，无法添加更多内容，请关注后续，咱们继续聊

----- English
Exploring SRE: K8S Cluster POD Scheduling Issue Description
Hi everyone, today I'd like to share a production issue I encountered in my daily work. Welcome everyone to brainstorm together.
Application infrastructure：
We deployed a series of services using a Kubernetes cluster, including a data collection app that scans approximately 8,500 objects and collects corresponding data. The configuration is as follows:

- K8S Cluster: 3 master nodes, 4 worker nodes
- Data collection app: 6 Pods deployed (replicas=6) to improve task concurrency processing capability
- Application runtime: The application creates a thread for each scanning object, with each thread responsible for handling a portion of the scanning tasks
- Invocation pattern: The app is invoked continuously through K8S Service port at 1-minute intervals
- Load distribution: The K8S Service uses load balancing to distribute requests to specific Pods for executing scanning tasks
Issue Description
After the service was deployed via CI/CD, all POD replicas were scheduled to worker node 2 (as shown in the diagram below), rather than being evenly distributed across other worker nodes.
This uneven scheduling caused the following problems:
1. Each Pod created a large number of threads to handle scanning tasks. When all 6 Pods were concentrated on worker node 2, the thread count rapidly increased, eventually exhausting the node's thread resources.
2. When any service deployed on node 2 attempted to create additional threads, the node had reached its thread limit, causing service failures. The logs showed:
"OpenBLAS blas_thread_init: pthread_create failed for thread 44 of 48: Resource temporarily unavailable."
This error indicated thread creation failure because the node could not allocate resources for new threads.
3. Worker node 2 completely crashed and could not continue running tasks, affecting the stability of the entire cluster.
Reference Points
- Linux node thread limit is 65,535 (default configuration, controlled by /proc/sys/kernel/threads-max and ulimit -u).
- The data collection service relied on a multi-threaded model, with each thread responsible for a portion of scanning objects. Due to the large number of scanning objects and being called at one-minute intervals, the service needed to create sufficient threads to execute tasks concurrently, ensuring data collection efficiency.
- The default thread limit (65,535) was designed to support high concurrency requirements. If the limit was too low, the service might not have been able to fully utilize CPU resources, leading to performance bottlenecks.
Issue Summary
Due to K8S scheduling issues, all Pods were concentrated on a single worker node, causing thread resources to be quickly exhausted and ultimately leading to node crash. Although the thread limit (65,535) was designed to support high concurrency requirements, in cases of uneven resource allocation, this limit became a bottleneck that restricted normal service operation.
Stay tuned for the next part where we'll discuss the root cause analysis and solution implementation!

Sep 12,2025
----- Chinese
探索 SRE：K8S 集群 Pod 调度问题修复
大家好，咱来聊聊上篇帖子提到的问题是如何解决的。
问题回顾
上篇提到由于 K8S 调度问题，数据收集app的所有 Pod 被集中调度到单个工作节点。每个 Pod 会创建大量线程来并发处理任务，最终导致线程资源耗尽（线程上限为 65,535），节点崩溃，服务停止运行。
	
解决思路: 如何让应用的Pod 相对均匀分布到所有工作节点上。
	
方案 1：Pod 反亲和性
Anti-Affinity是一种通过调度约束避免 Pod 聚集在同一节点的方法，适用于需要服务高可用性或资源隔离的场景。
配置规则：通过定义调度约束，基于 Pod 标签设置反亲和性规则。例如：避免同一服务的多个 Pod 副本调度到同一节点。
约束类型：强制规则（requiredDuringSchedulingIgnoredDuringExecution）：Pod 调度时必须满足反亲和性要求，否则无法被调度；软规则（preferredDuringSchedulingIgnoredDuringExecution）：调度时尽量满足反亲和性，无法满足时允许调度到不符合规则的节点。
拓扑范围：通过 topologyKey 定义反亲和性生效的范围，比如按节点主机名（kubernetes.io/hostname）分布。
	
方案 2：Pod 拓扑分布约束
maxSkew 是 Pod拓扑分布约束的一部分，用于限制 Pod 在不同节点或区域之间的分布偏差。相比 Pod 反亲和性，maxSkew 提供了更灵活的调度策略，允许一定的分布偏差。
定义分布偏差：maxSkew 指定 Pod 在不同拓扑域（如节点）之间的数量差异。例如：maxSkew: 1 表示每个节点之间的 Pod 数量差异最多为 1。
调度策略：whenUnsatisfiable: DoNotSchedule：如果无法满足分布约束，则 Pod 不会被调度。
调度器会尽量保证 Pod 均匀分布在不同节点。
	
问题解决
在评估了两种方案后，我选择了方案 2:maxSkew，因为它可以更灵活地控制 Pod 的分布偏差，同时允许一定程度的调度容忍性，避免因强制规则导致调度失败。
在配置了适当的 maxSkew 参数后，通过 CI/CD 重新部署了应用，其Pod 副本成功分布到工作节点上，整个服务和集群运行稳定。
	
后续我们来聊聊这个问题是如何被发现的，欢迎大家一起讨论

----- English
Exploring SRE: K8S Cluster Pod Scheduling Issue Fix

Hello everyone, let's discuss how we fixed the issue mentioned in the last story:Exploring SRE: K8S Cluster POD Scheduling Issue Description

Issue Recap
As mentioned in the previous post, due to K8S scheduling issues, all data collection app Pods were concentrated on a single worker node. Each Pod created a large number of threads to handle tasks concurrently, ultimately leading to thread resource exhaustion (thread limit of 65,535), causing the node to crash, and service failure.
Solution Approach: How to distribute application Pods relatively evenly across all worker nodes.

Solution 1: Pod Anti-Affinity
Anti-Affinity is a method that uses scheduling constraints to prevent Pods from clustering on the same node, suitable for scenarios requiring high service availability or resource isolation.
Configuration rules: Define scheduling constraints by setting anti-affinity rules based on Pod labels. For example: prevent multiple replicas of the same Pod from being scheduled to the same node.
Constraint types:
- Hard rules (requiredDuringSchedulingIgnoredDuringExecution): Pods must satisfy anti-affinity requirements during scheduling, otherwise they cannot be scheduled
- Soft rules (preferredDuringSchedulingIgnoredDuringExecution): Attempt to satisfy anti-affinity during scheduling, but allow scheduling to non-compliant nodes when requirements cannot be met
Topology scope: Define the scope where anti-affinity takes effect through topologyKey, such as distribution by node hostname (kubernetes.io/hostname).

Solution 2: Pod Topology Spread Constraints
maxSkew is part of Pod topology spread constraints, used to limit the distribution deviation of Pods across different nodes or zones. Compared to Pod anti-affinity, maxSkew provides more flexible scheduling strategies, allowing certain distribution deviations.
Define distribution deviation: maxSkew specifies the quantity difference of Pods between different topology domains (such as nodes). For example: maxSkew: 1 means the Pod quantity difference between each node is at most 1.
Scheduling strategy: whenUnsatisfiable: DoNotSchedule: If distribution constraints cannot be satisfied, the Pod will not be scheduled.
The scheduler will try to ensure Pods are evenly distributed across different nodes.

Issue Fix
After evaluating both solutions, I chose Solution 2: maxSkew, because it can more flexibly control Pod distribution deviation while allowing a certain degree of scheduling tolerance, avoiding scheduling failures caused by hard rules.
After configuring appropriate maxSkew parameters, the application was redeployed through CI/CD, and its Pod replicas were successfully distributed across worker nodes, with the entire service and cluster running stably.

Summary
Anti-Affinity is about separation: "Keep Pod A away from Pod B."
maxSkew is about balance: "Spread all these identical pods out as evenly as you can."

Next, we'll discuss how this problem was discovered. Welcome everyone to join the discussion.

Sep 15, 2025
----- Chinese
探索 SRE：K8S 集群 POD 调度问题追踪过程

问题初现：4月29日，监控系统探测到异常后，自动发出告警并推送到即时聊天工具，提示K8S集群中有多个Pod状态为CrashLoopBackOff，且间隔15分钟持续推送3次以上。排除偶发情况以后，值班SRE介入。
	
初步分析：
CrashLoopBackOff 表示Pod无法正常启动，可能由于应用本身问题、资源限制或底层节点问题导致。Pod错误log如之前帖子提到的：thread_init: Resource temporarily unavailable。
为了更快恢复服务，决定从底层节点资源入手，逐步排查问题根源。
	
定位工作节点：资源耗尽初现端倪
通过检查问题Pod的调度情况，发现这些Pod均运行在工作节点d01上。通过SSH登录到节点d01尝试深入分析，却发现在切换用户时出现以下异常：
-bash: fork: Cannot allocate memory
初步现象：节点无法创建新进程，提示“Cannot allocate memory”，表明节点可能已经耗尽了资源（如内存或线程）。
至此，内存耗尽的根本原因尚不明确。
在没有更多线索的情况下，为了恢复服务功能并保持集群的稳定性，团队决定重启节点d01。
	
问题复现：间歇性中断
5月4日晚，类似问题再次发生，监控系统持续推送告警，d01节点上的多个Pod再次CrashLoopBackOff，节点资源再次耗尽。团队再次重启节点，恢复服务。
问题特点：
每隔几天复现一次，表现出间歇性。
重启后问题暂时缓解，但未解决根因。
	
深入排查：关键发现
劳动节后，团队对d01节点进行全面检查，发现节点上存在多个相同的Java进程，且每个进程平均使用8000+线程。
进一步调查确认，这些Java进程均属于数据收集app的pod副本，负责扫描8k+对象并处理数据。app的多个副本同时调度在d01上消耗大量线程成为资源耗尽的核心原因。
	
故事的后续，大家可以参考前面的帖子，在此我简要说明下SRE的主要职责：
监控与告警：对集群进行有效监控并确保监控工具自动且及时推送告警。
优先恢复服务：快速恢复功能，确保告警消除。
问题定位与修复：深入分析根因，跟进修复问题。


----- English
Exploring SRE: K8S Cluster Pod Scheduling Issue Track Process

Hello everyone, let's discuss how we track the issue mentioned in the last story:Exploring SRE: K8S Cluster POD Scheduling Issue Description

Initial Analysis:
CrashLoopBackOff indicates that Pods cannot start normally, possibly due to application issues, resource limitations, or underlying node problems. Pod error logs showed the same issue mentioned in previous posts: "thread_init: Resource temporarily unavailable."
To restore service more quickly, we decided to start from the underlying node resources and gradually investigate the root cause.

Locating Worker Node: Resource Exhaustion Emerges
By checking the scheduling status of problematic Pods, we found that these Pods were all running on worker node d01. When we SSH'd into node d01 for deeper analysis, we encountered the following anomaly when switching users:
-bash: fork: Cannot allocate memory

Initial observation: The node could not create new processes, showing "Cannot allocate memory," indicating that the node had likely exhausted resources (such as memory or threads).
At this point, the root cause of memory exhaustion was still unclear.
Without more clues, to restore service functionality and maintain cluster stability, SRE team decided to restart node d01.

Problem Recurrence: Intermittent Interruptions**
On the evening of May 4th, similar problems occurred again. The monitoring system continuously pushed alerts, multiple Pods on node d01 experienced CrashLoopBackOff again, and node resources were exhausted once more. The team restarted the node again to restore service.

Problem characteristics:
- Recurred every few days, showing intermittent behavior
- Problems were temporarily alleviated after restart, but root cause remained unresolved

Deep Investigation: Key Discovery
Couple of days later, the team conducted a comprehensive check of node d01 and discovered multiple identical Java processes on the node, with each process using an average of 8000+ threads.
Further investigation confirmed that these Java processes all belonged to Pod replicas of the data collection app, responsible for scanning 8k+ objects and processing data. Multiple replicas of the app being scheduled simultaneously on d01 and consuming massive threads became the core reason for resource exhaustion.

For the continuation of this story, you can refer to previous posts. Here I'll briefly explain the main responsibilities of SRE:
- **Monitoring and Alerting**: Effectively monitor the cluster and ensure monitoring tools automatically and promptly push alerts
- **Priority Service Recovery**: Quickly restore functionality and ensure alerts are cleared
- **Problem Location and Fix**: Deep analysis of root causes, follow up on problem fixes

Sep 18, 2025
----- Chinese
海因里希法则：预防小问题，避免大灾难
大家好，今天我们来一起学习下安全生产的理论知识(换一个学习的首页图片😂)，从海因里希法则开始。
	
1️⃣ 海因里希法则
如图2，Heinrich法则最初来源于工业安全领域，由安全专家Herbert William Heinrich在20世纪30年代提出。它描述了事故的发生规律：
‼️‼️‼️ 每一起严重事故的背后，往往有29起轻微事故，以及300起未遂事件或隐患。
换句话说，重大事故往往是由一系列较小的问题积累或未被解决的隐患所引发的。如果我们能及时识别和处理这些小问题，就可以有效地预防更严重的后果。
	
2️⃣ 在SRE领域，我们致力于通过提高系统的可靠性和稳定性，来为用户提供始终如一的服务体验。为了实现这一目标，我们不仅要解决已经发生的问题，更需要从潜在问题中学习，防患于未然。在这一过程中，Heinrich法则为我们提供了一个极具价值的视角。
在SRE实践中，Heinrich法则有着广泛的应用场景。尽管它源于安全领域，但其核心思想——通过关注和解决小问题来避免大问题——与SRE的目标高度契合。以下是几个关键点：
 小问题积累导致大故障
在复杂的分布式系统中，几乎所有的重大系统故障（如全站宕机、数据丢失等）都不是单一事件的结果，而是由多个小问题叠加导致的。
 如前文提到的实例，k8s集群中数据收集app的POD副本不合理调度可能最初只是影响集群工作节点的稳定，但如果不及时处理，可能会导致整个集群的不稳定甚至崩溃。
	
 重视未遂事件（Near Miss）
未遂事件是指那些没有直接影响到生产环境，但暴露了潜在问题的事件。
 这一点我也有话要说，请关注后续分享
	
 构建“免疫系统”
SRE的核心之一是构建可观测性和自动化的工具链，以快速发现和修复问题。通过监控和日志系统，SRE团队可以捕捉到小问题的信号，比如错误率的轻微上升、延迟的短暂波动等。这些“小信号”正是Heinrich法则中隐患的表现，及时响应是预防灾难的关键。
 大家有想过没有，及时响应的前提是收到通知，那如何高效地收到通知呢？
发邮件？会被淹没在一堆邮件中，每隔5分钟检查一次邮箱也不可能；
发即时消息？结果类似，大家有啥建议请🙋
	
3️⃣ 想起小时候看过宣传节能灯的公益广告：再小的支持也是一种力量！在SRE的世界里，每一次小的改进，都是对大灾难的有效预防。
----- English
Heinrich's Rule: Prevent Small Problems, Avoid Major Disasters

Hello everyone, today let's learn about safety production theory together (with a different learning homepage image 😂), starting with Heinrich's Rule.

1️⃣ Heinrich's Rule
As shown in Figure 2, Heinrich's Rule originally comes from the industrial safety field, proposed by safety expert Herbert William Heinrich in the 1930s. It describes the pattern of accident occurrence:
‼️‼️‼️ Behind every serious accident, there are often 29 minor accidents and 300 near misses or hazards.
In other words, major accidents are often triggered by a series of smaller problems that accumulate or unresolved hazards. If we can identify and address these small problems in time, we can effectively prevent more serious consequences.

2️⃣ In the SRE field, we are committed to providing users with a consistent service experience by improving system reliability and stability. To achieve this goal, we need not only to solve problems that have already occurred, but also to learn from potential problems and prevent them before they happen. In this process, Heinrich's Rule provides us with an extremely valuable perspective.

In SRE practice, Heinrich's Rule has wide application scenarios. Although it originates from the safety field, its core idea - preventing major problems by focusing on and solving small problems - is highly aligned with SRE goals. Here are several key points:

**Small Problems Accumulate to Cause Major Failures**
In complex distributed systems, almost all major system failures (such as site-wide outages, data loss, etc.) are not the result of a single event, but are caused by the accumulation of multiple small problems.
As mentioned in the previous example, unreasonable scheduling of data collection app POD replicas in a k8s cluster may initially only affect the stability of cluster worker nodes, but if not handled promptly, it could lead to instability or even collapse of the entire cluster.

**Pay Attention to Near Miss Events**
Near miss events refer to those events that do not directly affect the production environment but expose potential problems.
I also have something to say about this point, please follow subsequent sharing.

**Building an "Immune System"**
One of the core aspects of SRE is building observability and automated toolchains to quickly discover and fix problems. Through monitoring and logging systems, SRE teams can capture signals of small problems, such as slight increases in error rates, brief fluctuations in latency, etc. These "small signals" are exactly the manifestation of hazards in Heinrich's Rule, and timely response is key to preventing disasters.
Have you ever thought about it? The prerequisite for timely response is receiving notifications. So how can we efficiently receive notifications?
Send emails? They'll be buried in a pile of emails, and checking email every 5 minutes is impossible;
Send instant messages? The result is similar. What suggestions do you have? Please 🙋

3️⃣ In the SRE world, every small improvement is an effective prevention against major disasters.

Sep 21, 2025
----- Chinese
Kubernetes Secret 简介
大家好，聊过安全生产法则以后，我来继续分享K8S使用过程中发生的案例 
这是一个隐秘且很容易踩坑的案例，从介绍K8S的secret开始。
在 K8S 中，Secret 是一种专门用于存储敏感信息（如密码、API 密钥、证书等）的资源类型。它可以帮助我们更安全地管理和使用这些数据。
1️⃣ 为什么要用 Secret
在管理敏感信息时，直接将这些数据存储在代码或配置文件中可能存在以下问题：
⚠️ 安全风险：敏感信息容易明文暴露，特别是在版本控制系统（如 Git）中。
⚠️ 缺乏灵活性：不同环境中的敏感信息可能不同，直接硬编码会导致修改困难。
⚠️ 权限管理不足：无法有效控制哪些用户或应用可以访问这些数据。
使用 Secret 的好处包括：
👍 安全存储：将敏感信息与应用解耦，降低泄露风险。
👍 权限控制：通过 Kubernetes 的 RBAC 机制，确保只有被授权的应用可以访问对应的 Secret。
👍 动态更新：支持敏感信息的动态更新，无需重新部署应用。
	
2️⃣ Secret 中的值需要以 Base64 编码存储，这并不是为了加密，而是出于以下原因：
格式兼容：Base64 可以将数据转换为只包含 ASCII 字符的字符串，确保即使是二进制数据也能安全地存储在 YAML/JSON 文件中。
减少明文暴露：虽然不是加密手段，但 Base64 可以在一定程度上避免数据被直接识别。
API 要求：Kubernetes 的 API 设计要求 Secret 的值以 Base64 编码形式存储。
‼️‼️‼️ 需要注意的是，Base64 并不能真正保护数据的安全。如果需要更高的安全性，可以结合外部密钥管理系统使用。
	
3️⃣ 应用可以通过以下方式使用 Secret 中存储的敏感信息：
注入环境变量，如图3所示：
将 Secret 的值注入到 Pod 的环境变量中，应用可以通过环境变量直接读取这些数据。这种方式适合简单的配置场景。
挂载到文件系统
将 Secret 的值以文件形式挂载到容器内，应用可以通过读取文件来获取敏感信息。这种方式适合管理复杂或大数据量的敏感内容。
	
通过合理使用 Secret，可以帮助开发者更安全、高效地管理敏感信息。接下来我将具体说明案例内容以及如何修复。

----- English

Introduction to Kubernetes Secret

Hello everyone, after discussing safety production rules, I'll continue sharing cases that occurred during K8S usage.
This is a hidden and easily overlooked case, starting with an introduction to K8S secrets.

In K8S, Secret is a resource type specifically designed for storing sensitive information (such as passwords, API keys, certificates, etc.). It helps us manage and use this data more securely.

1️⃣ Why Use Secret

When managing sensitive information, directly storing this data in code or configuration files may present the following problems:
⚠️ Security risks: Sensitive information is easily exposed in plain text, especially in version control systems (like Git).
⚠️ Lack of flexibility: Sensitive information may differ across environments, and hard-coding makes modifications difficult.
⚠️ Insufficient permission management: Cannot effectively control which users or applications can access this data.

Benefits of using Secret include:
👍 Secure storage: Decouples sensitive information from applications, reducing exposure risks.
👍 Permission control: Through Kubernetes' RBAC mechanism, ensures only authorized applications can access corresponding Secrets.
👍 Dynamic updates: Supports dynamic updates of sensitive information without redeploying applications.

2️⃣ Values in Secret need to be stored in Base64 encoding, which is not for encryption but for the following reasons:

Format compatibility: Base64 can convert data into strings containing only ASCII characters, ensuring even binary data can be safely stored in YAML/JSON files.
Reduce plain text exposure: While not an encryption method, Base64 can prevent data from being directly identified to some extent.
API requirements: Kubernetes' API design requires Secret values to be stored in Base64 encoded format.
‼️‼️‼️ It's important to note that Base64 cannot truly protect data security. If higher security is needed, it can be used in combination with external key management systems.

3️⃣ Applications can use sensitive information stored in Secret through the following methods:

Inject environment variables, as shown in Figure 3:
Inject Secret values into Pod environment variables, allowing applications to directly read this data through environment variables. This method is suitable for simple configuration scenarios.

Mount to file system:
Mount Secret values as files into containers, allowing applications to obtain sensitive information by reading files. This method is suitable for managing complex or large-volume sensitive content.

By properly using Secret, developers can manage sensitive information more securely and efficiently. Next, I will specifically explain the case content and how to fix it.

Sep 22, 2025
----- Chinese
Kubernetes Secret 踩坑案例
大家好，接上篇K8S secret 简介，为了在集群中安全存储敏感数据（如用户名、密码、API 密钥等），通常使用 Secret 来进行管理。应用程序可以通过挂载文件或环境变量的方式访问 Secret 中的键值对，并根据实际情况使用这些信息。然而，Secret 的使用若出现配置不当，可能会导致应用程序运行异常。
	
真实案例：应用程序因 Secret 的值内容问题导致数据库连接失败。
	
1️⃣ 问题描述
创建secret，用于存储数据库的连接信息。访问数据库的命令可以归纳到generic类型的secret中,有如下两种创建方法:
1. kubectl create secret generic db-credentials --from-file=username=username.txt --from-file=password=password.txt
     username.txt和password.txt是两个文本文件,内容分别为root,内容为password
  
2. kubectl create secret generic db-credentials --from-literal=username=root --from-literal=password=password
     直接在命令行中输入用户名和密码
  
这两种方法创建的secret是一样的,都可以通过kubectl get secret db-credentials -o yaml查看到secret的内容
但是第二种方法创建的secret在yaml文件中是以string类型存储的,而第一种方法创建的secret在yaml文件中是以binaryData类型存储的
这两种方法创建的secret在使用时没有区别,都可以通过kubectl get secret db-credentials -o yaml查看到secret的内容

根据应用程序的deploy yaml文件配置，pod启动时会读取Secret db-credentials中的 username 和 password 值来连接数据库。然而，在启动时，程序报错误日志如下：
Unable to obtain connection from database (jdbc:mysql://mysqldb:3306/mysql?Unicode=true&characterEncoding=UTF-8) for user 'root': Access denied for user 'root'@'10.4.214.53' (using password: YES)

	
2️⃣ 从日志分析
应用程序未能成功使用提供的用户名和密码连接数据库。错误提示显示，数据库拒绝了用户名 root 的访问。

3️⃣ 排查过程
检查数据库配置，首先检查了应用程序的数据库连接配置，确认数据库地址、端口、用户名和密码均正确无误。
通过以下命令查看 Secret 的内容：“kubectl get secret db-credentials -o yaml”，发现 Secret 中的 username 和 password 字段存储的是 Base64 编码后的字符串，而不是直接存储的明文，这是预期行为。
解码 Secret 内容 解码后发现，username 和 password 的值确实是正确的明文字符串。
在进一步详细对比后，发现问题出现在 Secret 的创建方式 上。操作人员使用上述创建secret的方法一创建了 Secret。其中，username.txt 和 password.txt 两个文本文件是通过文本编辑器 vim 生成的，在输入内容后直接保存退出，vim 默认会在文件末尾添加一个换行符（\n）。因此，文件的真实内容分别变成了(root\n)和(password\n), Base64 编码会将换行符也视为有效字符，最终导致 Secret 中的值包含了额外的换行符。
	
4️⃣ 解决方法：
直接使用以下命令生成没有换行符的文件：echo -n "password" > password.txt，重新生成secret
使用图3的方案二创建重新secret，在命令中直接指定内容
重启应用，问题修复

----- English

Kubernetes Secret Troubleshooting Case Study

Hello everyone, following up on the previous K8S Secret introduction, Secrets are typically used to securely store sensitive data (such as usernames, passwords, API keys, etc.) in clusters. Applications can access key-value pairs in Secrets by mounting files or environment variables, and use this information according to actual needs. However, improper configuration of Secrets may cause application runtime exceptions.

Real case: Application database connection failure due to Secret value content issues.

1️⃣ Issue Description
Creating a secret in K8S cluster to store database connection information. In general, we create generic type secret with following two methods:
1. kubectl create secret generic db-credentials --from-file=username=username.txt --from-file=password=password.txt
   username.txt and password.txt are two text files, with content "root" and "password" respectively

2. kubectl create secret generic db-credentials --from-literal=username=root --from-literal=password=password
   Directly input username and password in the command line

These two methods create identical secrets, both can be viewed through kubectl get secret db-credentials -o yaml to see the secret content
However, the secret created by the second method is stored as string type in the yaml file, while the secret created by the first method is stored as binaryData type in the yaml file
There is no difference when using secrets created by these two methods, both can be viewed through kubectl get secret db-credentials -o yaml to see the secret content

According to the application's deploy yaml file configuration, the pod reads the username and password values from Secret db-credentials to connect to the database during startup. However, during startup, the program reported the following error log:
Unable to obtain connection from database (jdbc:mysql://mysqldb:3306/mysql?Unicode=true&characterEncoding=UTF-8) for user 'root': Access denied for user 'root'@'10.4.214.53' (using password: YES)

2️⃣ Log Analysis
The application failed to successfully connect to the database using the provided username and password. The error message shows that the database denied access for username 'root'.

3️⃣ Troubleshooting Process
Checking database configuration, first verified the application's database connection configuration, confirming that the database address, port, username, and password were all correct.
Using the command "kubectl get secret db-credentials -o yaml" to view the Secret content, found that the username and password fields in the Secret store Base64 encoded strings, not direct plaintext, which is expected behavior.
Decoding Secret content revealed that the username and password values were indeed correct plaintext strings.
After further detailed comparison, the problem was found to be in the Secret creation method. The operator used the first method mentioned above to create the Secret. The username.txt and password.txt text files were generated using the vim text editor, and after inputting content and saving directly, vim by default adds a newline character (\n) at the end of the file. Therefore, the actual file content became (root\n) and (password\n) respectively. Base64 encoding treats the newline character as a valid character, ultimately causing the Secret values to contain extra newline characters.

4️⃣ Solution:
Directly use the following command to generate files without newline characters: echo -n "password" > password.txt, regenerate the secret
Use the second approach mentioned in figure 3 to recreate the secret, directly specifying content in the command
Restart the application, problem resolved

Sep 23, 2025
----- Chinese
base64 编码简介
大家好，今天我们来聊聊上篇提到的base64编码是个啥东东，以及它有啥用处。
 Base64 是一种常见的编码方法，它的主要作用是将二进制数据（如图片、文件、特殊字符等）转换成文本格式，以便在需要传输或存储的场景中使用。来了解下它的作用和一些应用场景 
	
1️⃣ 微信聊天中的图片传输
当你在微信或其他社交软件中发送图片时，图片实际上是以二进制形式存储的。如果直接传输二进制数据，可能会因为网络传输问题或不兼容的字符导致图片损坏。
 社交软件会将图片编码成 Base64 格式，将图片内容转换成一段文本（如 iVBORw0KGgoAAAANSUhEUg...）。这段文本更适合在网络上传输，确保图片可以被接收方正确解码并查看。
2️⃣ 发电子邮件时的附件
在发电子邮件时添加的图片、PDF、Word 文档等附件，实际上是文件的二进制数据。如果直接将这些数据嵌入邮件正文，可能会因为特殊字符（如 \0、\n 等）影响邮件的格式。
 邮件服务会将这些文件通过 Base64 编码，转换成可读的文本格式，嵌入到邮件内容中。例如，图片被编码成一段 Base64 文本，最终接收方可以解码还原成图片。它能确保附件不被错误解析，避免邮件内容被破坏，同时兼容不同的邮件客户端。
3️⃣ 生成二维码
在商场扫码付款时，手机会生成一个二维码。这些二维码其实是字符串的编码形式，可能包含你的付款信息、商户信息等。
 二维码中包含的字符串可能会用 Base64 进行编码，比如你的付款信息被转换成一段 Base64 文本，即将数据压缩成一种标准格式，再嵌入二维码中。便于快速读取、同时避免中文或特殊字符导致的数据错误。
4️⃣ 上传头像到社交媒体
有些平台会将图片编码成 Base64 格式，然后通过网络传输到服务器。这种方式可以避免网络不稳定时文件损坏。
如果把你的文件、图片、密码想象成“二进制代码”，它们可能包含“看不懂的字符”或“特殊符号”。这些字符在传输或者保存时有时会导致问题。
Base64 就像一个“翻译器”，把这些“看不懂的字符”翻译成由字母、数字组成的“人类友好型字符”。这让它们更容易在网络中传输、在系统中存储，且不会损坏或丢失。
虽然我们可能察觉不到 Base64 的存在，但它背后默默地让数据传输更加稳定和安全。

----- English

Introduction to Base64 Encoding

Hello everyone, today let's talk about what Base64 encoding is, which we mentioned in the previous article, and what it's used for.

Base64 is a common encoding method whose main purpose is to convert binary data (such as images, files, special characters, etc.) into text format for use in scenarios that require transmission or storage. Let's understand its functions and some application scenarios.

1️⃣ Image Transmission in WeChat Chat
When you send images in WeChat or other social software, the images are actually stored in binary format. If binary data is transmitted directly, the images might get corrupted due to network transmission issues or incompatible characters.
Social software will encode images into Base64 format, converting the image content into a text string (like iVBORw0KGgoAAAANSUhEUg...). This text string is more suitable for network transmission, ensuring that images can be correctly decoded and viewed by the recipient.

2️⃣ Email Attachments
When adding attachments like images, PDFs, Word documents to emails, these are actually binary data of files. If this data is directly embedded in the email body, it might affect the email format due to special characters (like \0, \n, etc.).
Email services will encode these files through Base64, converting them into readable text format and embedding them in the email content. For example, images are encoded into Base64 text, and ultimately the recipient can decode and restore them back to images. This ensures attachments are not incorrectly parsed, prevents email content corruption, and maintains compatibility with different email clients.

3️⃣ QR Code Generation
When making payments by scanning codes in shopping malls, your phone generates a QR code. These QR codes are actually encoded forms of strings that may contain your payment information, merchant information, etc.
The strings contained in QR codes may use Base64 encoding, for example, your payment information is converted into Base64 text, compressing the data into a standard format before embedding it in the QR code. This facilitates quick reading while avoiding data errors caused by Chinese characters or special characters.

4️⃣ Uploading Avatars to Social Media
Some platforms encode images into Base64 format and then transmit them to servers over the network. This approach can prevent file corruption when the network is unstable.

If you imagine your files, images, and passwords as "binary code," they may contain "incomprehensible characters" or "special symbols." These characters sometimes cause problems during transmission or storage.
Base64 acts like a "translator," translating these "incomprehensible characters" into "human-friendly characters" composed of letters and numbers. This makes them easier to transmit over networks and store in systems without damage or loss.
Although we may not notice the existence of Base64, it silently makes data transmission more stable and secure behind the scenes.

Sep 28,2025
----- Chinese
探索SRE：别让你的应用日志变成“话痨”
大家好，今天我来分享一个生产环境遇到的问题：应用日志 DEBUG/TRACE 设置的威力
1️⃣ 问题描述：某日，监控系统显示几台虚拟机异常：
 CPU占用过高
 内存使用率过高
 网络带宽占用过高 （> 30 m/s）
	
2️⃣ 检查过程：经过检查，发现几台异常的虚拟机都部署了同一个java应用，并于同一时间进行了整体版本更新。
检查grafana的响应指标图，发现java应用本身的jvm指标，GC状态相对正常，略微偏高
日志系统检查，发现应用在版本更新以后，开启了DEBUG的日志设定。
	
3️⃣ 问题修复：把应用的日志设定改为INFO以后，虚拟机状态恢复正常
	
4️⃣ 关于应用日志：对于部署在生产环境中的服务，日志那可是排查问题的“福尔摩斯”，是了解应用“内心活动”的窗口。如果你的应用是个正在搬砖的码农，日志级别就像是码农向上级汇报的工作进度，请参考图2。
从图中可以看出，日志级别从上往下，信息量是递增的。尤其是DEBUG 和 TRACE，它们简直就是日志界的“话痨”，只要能说的一股脑的都要跟你说！
	
5️⃣ 那么开启了DEBUG/TRACE的日志输出，到底是好是坏？请参考图3
通常，DEBUG 级别的日志主要用于开发和测试环境。在生产环境中，为了保证应用性能、稳定性和安全性，应避免长期开启 DEBUG 级别日志。如果需要在生产环境排查特定问题，可以考虑临时、有范围地开启 DEBUG 日志，并在问题解决后及时关闭或调回更高级别（如 INFO 或 ERROR）。同时，监控系统也需要对DEBUG/TRACE的日志设置以及输出状态有一个实时的监控，可以在问题出现的第一时间介入进行检查、确认，必要时进行修改。

----- English

Exploring SRE: Don't Let Your Application Logs Become "Chatterboxes"

Hello everyone, today I'm sharing a production environment issue I encountered: the power of application log DEBUG/TRACE settings.

1️⃣ Problem Description: One day, the monitoring system showed several virtual machines with anomalies:
- High CPU usage
- High memory usage
- High network bandwidth usage (> 30 MB/s)

2️⃣ Investigation Process: After investigation, we found that all the abnormal virtual machines had deployed the same Java application and had undergone a complete version update at the same time.
Checking Grafana's response metric charts, we found that the Java application's own JVM metrics and GC status were relatively normal, just slightly elevated.
Log system inspection revealed that after the version update, the application had enabled DEBUG log settings.

3️⃣ Problem Resolution: After changing the application's log setting to INFO, the virtual machine status returned to normal.

4️⃣ About Application Logs: For services deployed in production environments, logs are the "Sherlock Holmes" for troubleshooting issues - they're the window into understanding the application's "inner thoughts." If your application is a developer working hard, log levels are like the developer reporting work progress to supervisors, please refer to Figure 2.
From the chart, you can see that log levels increase in information volume from top to bottom. Especially DEBUG and TRACE - they're simply the "chatterboxes" of the logging world, telling you everything they possibly can!

5️⃣ So is enabling DEBUG/TRACE log output good or bad? Please refer to Figure 3.
Typically, DEBUG level logs are mainly used in development and testing environments. In production environments, to ensure application performance, stability, and security, long-term enabling of DEBUG level logs should be avoided. If you need to troubleshoot specific issues in production, consider temporarily and selectively enabling DEBUG logs, and promptly disable them or revert to higher levels (such as INFO or ERROR) after resolving the problem. Additionally, monitoring systems need real-time monitoring of DEBUG/TRACE log settings and output status, allowing for immediate intervention, inspection, confirmation, and necessary modifications when issues arise.

