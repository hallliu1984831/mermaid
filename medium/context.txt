Sep 7, 2025
----- Chinese
Êé¢Á¥¢SREÔºöÁ´ôÂú®ÂèØÈù†ÊÄßÁöÑÊúÄÂâçÁ∫ø
Â§ßÂÆ∂Â•ΩÔºåÊàëÊòØ‰∏ÄÂêçSREÔºå‰πüÂ∞±ÊòØ‚ÄúÁΩëÁ´ôÂèØÈù†ÊÄßÂ∑•Á®ãÂ∏à‚ÄùÔºàSite Reliability EngineerÔºâ„ÄÇËØ¥ÁôΩ‰∫ÜÔºåÊàëÁöÑÂ∑•‰ΩúÂ∞±ÊòØËÆ©ÂêÑÁßçÁΩëÁ´ô„ÄÅAppÂíåÊúçÂä°‰∏çÂá∫ÈóÆÈ¢òÔºåËÉΩÁ®≥ÂÆö„ÄÅÈ´òÊïàÂú∞ËøêË°å„ÄÇÊàëË∫´ÂêéÊ≤°ÊúâÊä´È£éÔºå‰ΩÜÊàëËßâÂæóËá™Â∑±ÊòØÊüêÁßçÊÑè‰πâ‰∏äÁöÑ‚ÄúÂπïÂêéËã±ÈõÑ‚ÄùÔºåÂõ†‰∏∫ÊàëÁöÑ‰ªªÂä°Â∞±ÊòØÊãØÊïëÁ≥ªÁªüÂ¥©Ê∫ÉÔºå‰øùÈöúÁî®Êà∑‰ΩìÈ™å„ÄÇÈÇ£‰πàÔºåÊàëÊØèÂ§©Âà∞Â∫ïÂú®ÂÅö‰ªÄ‰πàÂë¢ÔºüË∑üÊàë‰∏ÄËµ∑ÁúãÁúãÂêßÔºÅ
	
ÂèØÈù†ÊÄßÔºöSREÁöÑÊ†∏ÂøÉ‰ΩøÂëΩ
‰Ωú‰∏∫SREÔºåÊàëÊúÄÈáçË¶ÅÁöÑËÅåË¥£Â∞±ÊòØËÆ©Á≥ªÁªü‚ÄúÈù†Ë∞±‚Äù„ÄÇÊØîÂ¶ÇÔºåÂΩì‰Ω†Ê∑±Â§úÁÇπÂ§ñÂçñ„ÄÅÁî®App‰π∞ÁîµÂΩ±Á•®ÊàñËÄÖÂà∑Áü≠ËßÜÈ¢ëÊó∂ÔºåËøô‰∫õÊúçÂä°ÈÉΩÂæóÊ≠£Â∏∏ËøêË°åÔºå‰∏çÁÑ∂‰Ω†ÂèØËÉΩ‰ºöÊäìÁãÇÔºåÂØπÂêßÔºü‰∏∫‰∫ÜÂÅöÂà∞Ëøô‰∏ÄÁÇπÔºåÊàë‰ºöËÆæÂÆö‰∏Ä‰∫õÁõÆÊ†áÔºåÊØîÂ¶Ç‚ÄúÁ≥ªÁªü99.9%ÁöÑÊó∂Èó¥ÈÉΩË¶ÅÂú®Á∫ø‚ÄùÔºåÁÑ∂ÂêéÈÄöËøá‰ºòÂåñÊû∂ÊûÑ„ÄÅ‰øÆÂ§çÊºèÊ¥ûÔºåÊù•ËÆ©Á≥ªÁªüÂèòÂæóÁ®≥ÂÆöÂèàÂùöÂº∫„ÄÇ
	
ÁõëÊéßÔºöSREÁöÑÂçÉÈáåÁúº
ÊàëÁöÑÊó•Â∏∏Á¶ª‰∏çÂºÄ‚ÄúÁõëÊéß‚ÄùÔºàMonitoringÔºâ„ÄÇÊàë‰ºöÁî®‰∏Ä‰∫õÈÖ∑ÁÇ´ÁöÑÂ∑•ÂÖ∑ÔºåÊØîÂ¶ÇPrometheusÂíåGrafanaÔºåËøòÊúâÂÖ∂‰ªñ‰∏ÄÁ≥ªÂàóÁöÑÂ∑•ÂÖ∑Êù•ÁõØÁùÄÁ≥ªÁªüÁöÑËøêË°åÁä∂ÊÄÅ„ÄÇÊØîÂ¶ÇÔºåÊµÅÈáèÊòØ‰∏çÊòØÁ™ÅÁÑ∂È£ôÂçá‰∫ÜÔºüÊúçÂä°Âô®ÊòØ‰∏çÊòØÂºÄÂßãÂèëÁÉ≠‰∫ÜÔºü‰∏ÄÊó¶ÂèëÁé∞ÂºÇÂ∏∏ÔºåÊàëÂ∞±ËÉΩÁ¨¨‰∏ÄÊó∂Èó¥Âá∫ÊâãÔºåÈò≤Ê≠¢ÈóÆÈ¢òÊâ©Â§ß„ÄÇÂèØ‰ª•ËØ¥ÔºåÁõëÊéßÂ∑•ÂÖ∑ÊòØÊàëÁöÑÁúºÁùõÔºåÂ∏ÆÊàëÈöèÊó∂ÊéåÊè°Á≥ªÁªüÁöÑÂÅ•Â∫∑Áä∂ÊÄÅ„ÄÇ
	
Ëá™Âä®ÂåñÔºöËß£ÊîæÂèåÊâãÁöÑÁßòËØÄ
ËØ¥ÂÆûËØùÔºåÈáçÂ§çÂπ≤ÂêåÊ†∑ÁöÑ‰∫ãÊÉÖÁúüÁöÑÂæàÊó†ËÅä„ÄÇÊâÄ‰ª•ÔºåSREÁöÑÂ∫ßÂè≥Èì≠‰πã‰∏ÄÂ∞±ÊòØ‚ÄúËÉΩËá™Âä®ÂåñÁöÑÁªù‰∏çÊâãÂä®‚Äù„ÄÇÊàëÁöÑÂ∑•‰Ωú‰πã‰∏ÄÂ∞±ÊòØÂÜôËÑöÊú¨ÂíåÁ®ãÂ∫èÔºåËÆ©Á≥ªÁªüËá™Âä®Â§ÑÁêÜ‰∏Ä‰∫õÂ∏∏ËßÅÈóÆÈ¢òÔºåÊØîÂ¶ÇËá™Âä®Êâ©Â±ïÊúçÂä°Âô®ÂÆπÈáèÔºåËá™Âä®ÈáçÂêØÊúçÂä°ÔºåÁîöËá≥Ëá™Âä®È¢ÑË≠¶„ÄÇÊúâ‰∫ÜËá™Âä®ÂåñÔºåÊàëÂèØ‰ª•ÊääÊó∂Èó¥Ëä±Âú®Êõ¥ÈáçË¶Å„ÄÅÊõ¥ÊúâÊåëÊàòÁöÑ‰∫ãÊÉÖ‰∏ä„ÄÇ
	
ÂèØÊâ©Â±ïÊÄßÔºö‰ªéÂÆπÂ∫îÂØπÈ´òÂ≥∞
ÊàëÁªèÂ∏∏Ë∑üÊúãÂèãÂºÄÁé©Á¨ëËØ¥ÔºåÊàëÁöÑÂ∑•‰ΩúÂ∞±ÊòØËÆ©‚ÄúÂèåÂçÅ‰∏Ä‚Äù‰∏çÂÆïÊú∫„ÄÇÊØèÊ¨°Â§ß‰øÉ„ÄÅÁõ¥Êí≠ÁàÜÁÅ´Êó∂ÔºåÊµÅÈáèÈÉΩ‰ºöÊö¥Ê∂®ÔºåËøôÂØπÁ≥ªÁªüÊòØ‰∏™Â§ßËÄÉÈ™å„ÄÇËÄåÊàëÁöÑ‰ªªÂä°Â∞±ÊòØÊèêÂâçËÆæËÆ°Â•ΩÁ≥ªÁªüÔºåËÆ©ÂÆÉËÉΩÁÅµÊ¥ªÊâ©Â±ïÔºåÊâõ‰ΩèÂéãÂäõ„ÄÇÁúãÂà∞Áî®Êà∑‰π∞‰π∞‰π∞„ÄÅÂà∑Âà∑Âà∑Êó∂ÔºåÊàëÈÉΩËßâÂæóÁâπÂà´È™ÑÂÇ≤ÔºåÂõ†‰∏∫ËøôËÉåÂêéÊúâÊàëÁöÑÂä™Âäõ„ÄÇ
	
‰∫ã‰ª∂ÁÆ°ÁêÜÔºöÂç±Êú∫‰∏≠ÁöÑÂÜ∑ÈùôÂ§ßËÑë
ÂΩìÁÑ∂ÔºåÂ§©Êúâ‰∏çÊµãÈ£é‰∫ëÔºåÂÜçÂÆåÁæéÁöÑÁ≥ªÁªü‰πüÊúâÂèØËÉΩÁ™ÅÁÑ∂Â¥©‰∫Ü„ÄÇËøô‰∏™Êó∂ÂÄôÔºåÊàëÂ∞±ÂèòÊàê‰∫Ü‚ÄúÂç±Êú∫Â§ÑÁêÜ‰∏ìÂÆ∂‚Äù„ÄÇÂΩìË≠¶Êä•ÂìçËµ∑ÔºåÊàë‰ºöËøÖÈÄüÂÆö‰ΩçÈóÆÈ¢òÔºåÊÉ≥ÂäûÊ≥ïÊÅ¢Â§çÊúçÂä°ÔºåÂêåÊó∂ÁªÑÁªáÂõ¢ÈòüÊÄªÁªìÂ§çÁõòÔºåÈÅøÂÖçÂêåÊ†∑ÁöÑÈîôËØØÂÜçÊ¨°ÂèëÁîü„ÄÇËôΩÁÑ∂Á¥ßÂº†Ôºå‰ΩÜÊØèÊ¨°ÊàêÂäüËß£ÂÜ≥ÈóÆÈ¢òÂêéÔºåÈÇ£ÁßçÊàêÂ∞±ÊÑüÁúüÁöÑË∂ÖÁàΩ„ÄÇ
	
ËØ¥ÁúüÁöÑÔºåSREËøô‰∏™ËÅå‰∏öÂØπÊàëÊù•ËØ¥‰∏çÂè™ÊòØÂ∑•‰ΩúÔºåÊõ¥ÊòØ‰∏ÄÁßçÊåëÊàòÂíå‰πêË∂£„ÄÇÊàëÊó¢ËÉΩÂä®ÊâãÂÜô‰ª£Á†ÅÔºåÂèàËÉΩËÆæËÆ°Á≥ªÁªüÔºåËøòËÉΩÂø´ÈÄüËß£ÂÜ≥ÈóÆÈ¢òÔºåÊØèÂ§©ÈÉΩÂÖÖÊª°Êñ∞È≤úÊÑü„ÄÇ
Êé•‰∏ãÊù•ÔºåÊàë‰ºöÊÄªÁªìÂπ∂ÂàÜ‰∫´SREÊó•Â∏∏Â∑•‰ΩúÂÜÖÂÆπÔºåÂêåÊó∂Ê¨¢ËøéÂ§ßÂÆ∂ÂíåÊàë‰∏ÄËµ∑‰∫íÂä®Ôºå‰∫íÁõ∏‰∫§ÊµÅ ÔºöÔºâ

----- English 
Exploring SRE: Standing at the Forefront of Reliability
Hi everyone, I'm an SRE, which stands for "Site Reliability Engineer." Simply put, my job is to ensure that various websites, apps, and services don't break down and can run stably and efficiently. I may not wear a cape, but I consider myself a "behind-the-scenes hero" in some sense, because my mission is to rescue system crashes and ensure user experience. So, what exactly do I do every day? Let's take a look together!

Reliability: The Core Mission of SRE
As an SRE, my most important responsibility is to make systems "reliable." For example, when you order takeout late at night, buy movie tickets through an app, or scroll through short videos, these services must run normally, otherwise you might go crazy, right? To achieve this, I set some goals, like "the system must be online 99.9% of the time," and then optimize architecture and fix vulnerabilities to make the system stable and robust.

Monitoring: SRE's All-Seeing Eyes
My daily work is inseparable from "monitoring." I use some cool tools like Prometheus and Grafana, along with other series of tools to keep an eye on the system's operational status. For instance, is traffic suddenly spiking? Are the servers starting to overheat? Once I detect anomalies, I can take action immediately to prevent problems from escalating. You could say that monitoring tools are my eyes, helping me stay on top of the system's health status at all times.

Automation: The Secret to Freeing Your Hands
Honestly, doing the same repetitive tasks is really boring. So, one of SRE's mottos is "automate everything that can be automated, never do it manually." Part of my job is writing scripts and programs to let the system automatically handle common issues, such as automatically scaling server capacity, automatically restarting services, and even automatic alerts. With automation, I can spend time on more important and challenging tasks.

Scalability: Handling Peak Traffic with Ease
I often joke with friends that my job is to prevent systems during events like Black Friday from crashing. Every time there are major promotions or live streams go viral, traffic surges dramatically, which is a major test for the system. My task is to design the system in advance so it can scale flexibly and withstand the pressure. When I see users shopping and browsing frantically, I feel particularly proud because my efforts are behind all of this.

Incident Management: The Calm Mind in Crisis
Of course, unexpected things happen, and even the most perfect systems can suddenly crash. At times like these, I become a "crisis management expert." When alarms go off, I quickly locate problems, find ways to restore services, while organizing the team to summarize and review to prevent the same mistakes from happening again. Although it's stressful, the sense of achievement after successfully solving problems is absolutely amazing.

Honestly, the SRE profession is not just a job for me, but also a challenge and source of enjoyment. I can both write code and design systems, as well as solve problems quickly. Every day is full of fresh experiences.

Next, I will summarize and share the daily work content of SRE, and I welcome everyone to interact and exchange ideas with me :)

Sep 9,2025
----- Chinese
Êé¢Á¥¢SREÔºöK8SÈõÜÁæ§PODË∞ÉÂ∫¶ÈóÆÈ¢òÊèèËø∞
Â§ßÂÆ∂Â•ΩÔºå‰ªäÂ§©ÂàÜ‰∫´‰∏Ä‰∏™Êó•Â∏∏Â∑•‰Ωú‰∏≠ÈÅáÂà∞ÁöÑ‰∫ßÁ∫øÈóÆÈ¢òÔºåÊ¨¢ËøéÂ§ßÂÆ∂‰∏ÄËµ∑Â§¥ËÑëÈ£éÊö¥„ÄÇ
Êàë‰ª¨‰ΩøÁî® Kubernetes ÈõÜÁæ§ÈÉ®ÁΩ≤‰∫Ü‰∏ÄÁ≥ªÂàóÊúçÂä°ÔºåÂÖ∂‰∏≠‰∏Ä‰∏™Êï∞ÊçÆÊî∂ÈõÜappÔºåÁî®‰∫éÊâ´ÊèèÁ∫¶ 8,500 ‰∏™ÂØπË±°Âπ∂Êî∂ÈõÜÁõ∏Â∫îÊï∞ÊçÆÔºåÈÖçÁΩÆÂ¶Ç‰∏ãÔºö
K8S ÈõÜÁæ§Ôºö3 ‰∏™‰∏ªËäÇÁÇπÔºå4 ‰∏™Â∑•‰ΩúËäÇÁÇπ„ÄÇ
Êï∞ÊçÆÊî∂ÈõÜapp:ÈÉ®ÁΩ≤‰∫Ü 6 ‰∏™ Pod (replicas=6)Ôºå‰ª•ÊèêÈ´ò‰ªªÂä°ÁöÑÂπ∂ÂèëÂ§ÑÁêÜËÉΩÂäõ„ÄÇ
appËøêË°åÊó∂‰∏∫ÊØè‰∏™Êâ´ÊèèÂØπË±°ÂàõÂª∫Á∫øÁ®ãÔºåÊØè‰∏™Á∫øÁ®ãË¥üË¥£Â§ÑÁêÜ‰∏ÄÈÉ®ÂàÜÊâ´Êèè‰ªªÂä°„ÄÇ
appÈÄöËøá K8S ÁöÑ Service Á´ØÂè£‰ª• 1 ÂàÜÈíüÁöÑÈó¥ÈöîË¢´ÊåÅÁª≠Ë∞ÉÁî®„ÄÇ
Service ÈÄöËøáË¥üËΩΩÂùáË°°Â∞ÜËØ∑Ê±ÇÂàÜÈÖçÂà∞ÂÖ∑‰ΩìÁöÑ Pod ÔºåÂπ∂ÊâßË°åÊâ´Êèè‰ªªÂä°„ÄÇ
Ôªø#SREÔªø
ÈóÆÈ¢òÊèèËø∞
ÊúçÂä°ÈÄöËøáCI/CDÈÉ®ÁΩ≤ÂêéÔºåÊâÄÊúâPODÂâØÊú¨ÈÉΩË¢´Ë∞ÉÂ∫¶Âà∞‰∫ÜÂ∑•‰ΩúËäÇÁÇπ 2(Â¶ÇÂõæ)ÔºåËÄåÊ≤°ÊúâÂùáÂåÄÂàÜÂ∏ÉÂà∞ÂÖ∂‰ªñÂ∑•‰ΩúËäÇÁÇπ„ÄÇËøôÁßçË∞ÉÂ∫¶‰∏çÂùáË°°ÂØºËá¥Â¶Ç‰∏ãÈóÆÈ¢òÔºö
ÊØè‰∏™ Pod ÂàõÂª∫Â§ßÈáèÁ∫øÁ®ãÂ§ÑÁêÜÊâ´Êèè‰ªªÂä°„ÄÇÂΩì 6 ‰∏™ Pod ÂÖ®ÈÉ®ÈõÜ‰∏≠Âú®Â∑•‰ΩúËäÇÁÇπ 2 ‰∏äËøêË°åÊó∂ÔºåÁ∫øÁ®ãÊï∞ÈáèÂø´ÈÄüÂ¢ûÈïøÔºåÊúÄÁªàËÄóÂ∞Ω‰∫ÜËäÇÁÇπÁöÑÁ∫øÁ®ãËµÑÊ∫ê„ÄÇ
ÂΩìËäÇÁÇπ2ÈÉ®ÁΩ≤ÁöÑ‰ªªÊÑèÊúçÂä°Â∞ùËØïÂàõÂª∫Êõ¥Â§öÁ∫øÁ®ãÊó∂ÔºåÂõ†ËäÇÁÇπÂ∑≤ËææÂà∞Á∫øÁ®ã‰∏äÈôêÔºåÂØºËá¥ÊúçÂä°Êä•ÈîôÔºåÊó•ÂøóÊòæÁ§∫ÔºöOpenBLAS blas_thread_init: pthread_create failed for thread 44 of 48: Resource temporarily unavailable„ÄÇËØ•ÈîôËØØË°®ÊòéÁ∫øÁ®ãÂàõÂª∫Â§±Ë¥•ÔºåÂéüÂõ†ÊòØËäÇÁÇπÊó†Ê≥ï‰∏∫Êñ∞Á∫øÁ®ãÂàÜÈÖçËµÑÊ∫ê„ÄÇ
Â∑•‰ΩúËäÇÁÇπ 2 ÂΩªÂ∫ïÂ¥©Ê∫ÉÔºåÊó†Ê≥ïÁªßÁª≠ËøêË°å‰ªªÂä°ÔºåÂØπÊï¥‰∏™ÈõÜÁæ§ÁöÑÁ®≥ÂÆöÊÄßÈÄ†Êàê‰∫ÜÂΩ±Âìç„ÄÇ
ÂèÇËÄÉÁÇπ
LinuxËäÇÁÇπÁ∫øÁ®ã‰∏äÈôê‰∏∫ 65,535ÔºàÈªòËÆ§ÈÖçÁΩÆÔºåÈÄöËøá /proc/sys/kernel/threads-max Âíå ulimit -u ÊéßÂà∂Ôºâ„ÄÇ
Êï∞ÊçÆÊî∂ÈõÜÊúçÂä°‰æùËµñÂ§öÁ∫øÁ®ãÊ®°ÂûãÔºåÊØè‰∏™Á∫øÁ®ãË¥üË¥£‰∏ÄÈÉ®ÂàÜÊâ´ÊèèÂØπË±°„ÄÇÁî±‰∫éÊâ´ÊèèÂØπË±°Êï∞ÈáèËæÉÂ§ß‰∏îÈó¥Èöî‰∏ÄÂàÜÈíüË¢´Ë∞ÉÁî®ÔºåÊúçÂä°ÈúÄË¶ÅÂàõÂª∫Ë∂≥Â§üÂ§öÁöÑÁ∫øÁ®ãÂπ∂ÂèëÊâßË°å‰ªªÂä°Ôºå‰ª•‰øùÈöúÊï∞ÊçÆÊî∂ÈõÜÁöÑÊïàÁéá„ÄÇ
ÈªòËÆ§Á∫øÁ®ã‰∏äÈôêÔºà65,535ÔºâÊòØ‰∏∫‰∫ÜÊîØÊåÅÈ´òÂπ∂ÂèëÈúÄÊ±Ç„ÄÇÂ¶ÇÊûú‰∏äÈôêËæÉ‰ΩéÔºåÊúçÂä°ÂèØËÉΩÊó†Ê≥ïÂÖÖÂàÜÂà©Áî® CPU ËµÑÊ∫êÔºåÂØºËá¥ÊÄßËÉΩÁì∂È¢à„ÄÇ
ÈóÆÈ¢òÊÄªÁªì
Áî±‰∫é K8S Ë∞ÉÂ∫¶ÈóÆÈ¢òÔºåÊâÄÊúâ Pod Ë¢´ÈõÜ‰∏≠Ë∞ÉÂ∫¶Âà∞Âçï‰∏™Â∑•‰ΩúËäÇÁÇπÔºåÂØºËá¥Á∫øÁ®ãËµÑÊ∫êËøÖÈÄüËÄóÂ∞ΩÔºåÊúÄÁªàÂºïÂèëËäÇÁÇπÂ¥©Ê∫É„ÄÇÂ∞ΩÁÆ°Á∫øÁ®ã‰∏äÈôêÔºà65,535ÔºâËÆæËÆ°Áî®‰∫éÊîØÊåÅÈ´òÂπ∂ÂèëÈúÄÊ±ÇÔºå‰ΩÜÂú®ËµÑÊ∫êÂàÜÈÖç‰∏çÂùáÁöÑÊÉÖÂÜµ‰∏ãÔºåËøô‰∏Ä‰∏äÈôêÂèçËÄåÊàê‰∏∫‰∫ÜÁì∂È¢àÔºåÈôêÂà∂‰∫ÜÊúçÂä°ÁöÑÊ≠£Â∏∏ËøêË°å„ÄÇ
Áî±‰∫éÂπ≥Âè∞ÂØπÁ¨îËÆ∞ÁöÑÁØáÂπÖÈôêÂà∂ÔºåÊó†Ê≥ïÊ∑ªÂä†Êõ¥Â§öÂÜÖÂÆπÔºåËØ∑ÂÖ≥Ê≥®ÂêéÁª≠ÔºåÂí±‰ª¨ÁªßÁª≠ËÅä

----- English
Exploring SRE: K8S Cluster POD Scheduling Issue Description
Hi everyone, today I'd like to share a production issue I encountered in my daily work. Welcome everyone to brainstorm together.
Application infrastructureÔºö
We deployed a series of services using a Kubernetes cluster, including a data collection app that scans approximately 8,500 objects and collects corresponding data. The configuration is as follows:

- K8S Cluster: 3 master nodes, 4 worker nodes
- Data collection app: 6 Pods deployed (replicas=6) to improve task concurrency processing capability
- Application runtime: The application creates a thread for each scanning object, with each thread responsible for handling a portion of the scanning tasks
- Invocation pattern: The app is invoked continuously through K8S Service port at 1-minute intervals
- Load distribution: The K8S Service uses load balancing to distribute requests to specific Pods for executing scanning tasks
Issue Description
After the service was deployed via CI/CD, all POD replicas were scheduled to worker node 2 (as shown in the diagram below), rather than being evenly distributed across other worker nodes.
This uneven scheduling caused the following problems:
1. Each Pod created a large number of threads to handle scanning tasks. When all 6 Pods were concentrated on worker node 2, the thread count rapidly increased, eventually exhausting the node's thread resources.
2. When any service deployed on node 2 attempted to create additional threads, the node had reached its thread limit, causing service failures. The logs showed:
"OpenBLAS blas_thread_init: pthread_create failed for thread 44 of 48: Resource temporarily unavailable."
This error indicated thread creation failure because the node could not allocate resources for new threads.
3. Worker node 2 completely crashed and could not continue running tasks, affecting the stability of the entire cluster.
Reference Points
- Linux node thread limit is 65,535 (default configuration, controlled by /proc/sys/kernel/threads-max and ulimit -u).
- The data collection service relied on a multi-threaded model, with each thread responsible for a portion of scanning objects. Due to the large number of scanning objects and being called at one-minute intervals, the service needed to create sufficient threads to execute tasks concurrently, ensuring data collection efficiency.
- The default thread limit (65,535) was designed to support high concurrency requirements. If the limit was too low, the service might not have been able to fully utilize CPU resources, leading to performance bottlenecks.
Issue Summary
Due to K8S scheduling issues, all Pods were concentrated on a single worker node, causing thread resources to be quickly exhausted and ultimately leading to node crash. Although the thread limit (65,535) was designed to support high concurrency requirements, in cases of uneven resource allocation, this limit became a bottleneck that restricted normal service operation.
Stay tuned for the next part where we'll discuss the root cause analysis and solution implementation!

Sep 12,2025
----- Chinese
Êé¢Á¥¢ SREÔºöK8S ÈõÜÁæ§ Pod Ë∞ÉÂ∫¶ÈóÆÈ¢ò‰øÆÂ§ç
Â§ßÂÆ∂Â•ΩÔºåÂí±Êù•ËÅäËÅä‰∏äÁØáÂ∏ñÂ≠êÊèêÂà∞ÁöÑÈóÆÈ¢òÊòØÂ¶Ç‰ΩïËß£ÂÜ≥ÁöÑ„ÄÇ
ÈóÆÈ¢òÂõûÈ°æ
‰∏äÁØáÊèêÂà∞Áî±‰∫é K8S Ë∞ÉÂ∫¶ÈóÆÈ¢òÔºåÊï∞ÊçÆÊî∂ÈõÜappÁöÑÊâÄÊúâ Pod Ë¢´ÈõÜ‰∏≠Ë∞ÉÂ∫¶Âà∞Âçï‰∏™Â∑•‰ΩúËäÇÁÇπ„ÄÇÊØè‰∏™ Pod ‰ºöÂàõÂª∫Â§ßÈáèÁ∫øÁ®ãÊù•Âπ∂ÂèëÂ§ÑÁêÜ‰ªªÂä°ÔºåÊúÄÁªàÂØºËá¥Á∫øÁ®ãËµÑÊ∫êËÄóÂ∞ΩÔºàÁ∫øÁ®ã‰∏äÈôê‰∏∫ 65,535ÔºâÔºåËäÇÁÇπÂ¥©Ê∫ÉÔºåÊúçÂä°ÂÅúÊ≠¢ËøêË°å„ÄÇ
	
Ëß£ÂÜ≥ÊÄùË∑Ø: Â¶Ç‰ΩïËÆ©Â∫îÁî®ÁöÑPod Áõ∏ÂØπÂùáÂåÄÂàÜÂ∏ÉÂà∞ÊâÄÊúâÂ∑•‰ΩúËäÇÁÇπ‰∏ä„ÄÇ
	
ÊñπÊ°à 1ÔºöPod Âèç‰∫≤ÂíåÊÄß
Anti-AffinityÊòØ‰∏ÄÁßçÈÄöËøáË∞ÉÂ∫¶Á∫¶ÊùüÈÅøÂÖç Pod ËÅöÈõÜÂú®Âêå‰∏ÄËäÇÁÇπÁöÑÊñπÊ≥ïÔºåÈÄÇÁî®‰∫éÈúÄË¶ÅÊúçÂä°È´òÂèØÁî®ÊÄßÊàñËµÑÊ∫êÈöîÁ¶ªÁöÑÂú∫ÊôØ„ÄÇ
ÈÖçÁΩÆËßÑÂàôÔºöÈÄöËøáÂÆö‰πâË∞ÉÂ∫¶Á∫¶ÊùüÔºåÂü∫‰∫é Pod Ê†áÁ≠æËÆæÁΩÆÂèç‰∫≤ÂíåÊÄßËßÑÂàô„ÄÇ‰æãÂ¶ÇÔºöÈÅøÂÖçÂêå‰∏ÄÊúçÂä°ÁöÑÂ§ö‰∏™ Pod ÂâØÊú¨Ë∞ÉÂ∫¶Âà∞Âêå‰∏ÄËäÇÁÇπ„ÄÇ
Á∫¶ÊùüÁ±ªÂûãÔºöÂº∫Âà∂ËßÑÂàôÔºàrequiredDuringSchedulingIgnoredDuringExecutionÔºâÔºöPod Ë∞ÉÂ∫¶Êó∂ÂøÖÈ°ªÊª°Ë∂≥Âèç‰∫≤ÂíåÊÄßË¶ÅÊ±ÇÔºåÂê¶ÂàôÊó†Ê≥ïË¢´Ë∞ÉÂ∫¶ÔºõËΩØËßÑÂàôÔºàpreferredDuringSchedulingIgnoredDuringExecutionÔºâÔºöË∞ÉÂ∫¶Êó∂Â∞ΩÈáèÊª°Ë∂≥Âèç‰∫≤ÂíåÊÄßÔºåÊó†Ê≥ïÊª°Ë∂≥Êó∂ÂÖÅËÆ∏Ë∞ÉÂ∫¶Âà∞‰∏çÁ¨¶ÂêàËßÑÂàôÁöÑËäÇÁÇπ„ÄÇ
ÊãìÊâëËåÉÂõ¥ÔºöÈÄöËøá topologyKey ÂÆö‰πâÂèç‰∫≤ÂíåÊÄßÁîüÊïàÁöÑËåÉÂõ¥ÔºåÊØîÂ¶ÇÊåâËäÇÁÇπ‰∏ªÊú∫ÂêçÔºàkubernetes.io/hostnameÔºâÂàÜÂ∏É„ÄÇ
	
ÊñπÊ°à 2ÔºöPod ÊãìÊâëÂàÜÂ∏ÉÁ∫¶Êùü
maxSkew ÊòØ PodÊãìÊâëÂàÜÂ∏ÉÁ∫¶ÊùüÁöÑ‰∏ÄÈÉ®ÂàÜÔºåÁî®‰∫éÈôêÂà∂ Pod Âú®‰∏çÂêåËäÇÁÇπÊàñÂå∫Âüü‰πãÈó¥ÁöÑÂàÜÂ∏ÉÂÅèÂ∑Æ„ÄÇÁõ∏ÊØî Pod Âèç‰∫≤ÂíåÊÄßÔºåmaxSkew Êèê‰æõ‰∫ÜÊõ¥ÁÅµÊ¥ªÁöÑË∞ÉÂ∫¶Á≠ñÁï•ÔºåÂÖÅËÆ∏‰∏ÄÂÆöÁöÑÂàÜÂ∏ÉÂÅèÂ∑Æ„ÄÇ
ÂÆö‰πâÂàÜÂ∏ÉÂÅèÂ∑ÆÔºömaxSkew ÊåáÂÆö Pod Âú®‰∏çÂêåÊãìÊâëÂüüÔºàÂ¶ÇËäÇÁÇπÔºâ‰πãÈó¥ÁöÑÊï∞ÈáèÂ∑ÆÂºÇ„ÄÇ‰æãÂ¶ÇÔºömaxSkew: 1 Ë°®Á§∫ÊØè‰∏™ËäÇÁÇπ‰πãÈó¥ÁöÑ Pod Êï∞ÈáèÂ∑ÆÂºÇÊúÄÂ§ö‰∏∫ 1„ÄÇ
Ë∞ÉÂ∫¶Á≠ñÁï•ÔºöwhenUnsatisfiable: DoNotScheduleÔºöÂ¶ÇÊûúÊó†Ê≥ïÊª°Ë∂≥ÂàÜÂ∏ÉÁ∫¶ÊùüÔºåÂàô Pod ‰∏ç‰ºöË¢´Ë∞ÉÂ∫¶„ÄÇ
Ë∞ÉÂ∫¶Âô®‰ºöÂ∞ΩÈáè‰øùËØÅ Pod ÂùáÂåÄÂàÜÂ∏ÉÂú®‰∏çÂêåËäÇÁÇπ„ÄÇ
	
ÈóÆÈ¢òËß£ÂÜ≥
Âú®ËØÑ‰º∞‰∫Ü‰∏§ÁßçÊñπÊ°àÂêéÔºåÊàëÈÄâÊã©‰∫ÜÊñπÊ°à 2:maxSkewÔºåÂõ†‰∏∫ÂÆÉÂèØ‰ª•Êõ¥ÁÅµÊ¥ªÂú∞ÊéßÂà∂ Pod ÁöÑÂàÜÂ∏ÉÂÅèÂ∑ÆÔºåÂêåÊó∂ÂÖÅËÆ∏‰∏ÄÂÆöÁ®ãÂ∫¶ÁöÑË∞ÉÂ∫¶ÂÆπÂøçÊÄßÔºåÈÅøÂÖçÂõ†Âº∫Âà∂ËßÑÂàôÂØºËá¥Ë∞ÉÂ∫¶Â§±Ë¥•„ÄÇ
Âú®ÈÖçÁΩÆ‰∫ÜÈÄÇÂΩìÁöÑ maxSkew ÂèÇÊï∞ÂêéÔºåÈÄöËøá CI/CD ÈáçÊñ∞ÈÉ®ÁΩ≤‰∫ÜÂ∫îÁî®ÔºåÂÖ∂Pod ÂâØÊú¨ÊàêÂäüÂàÜÂ∏ÉÂà∞Â∑•‰ΩúËäÇÁÇπ‰∏äÔºåÊï¥‰∏™ÊúçÂä°ÂíåÈõÜÁæ§ËøêË°åÁ®≥ÂÆö„ÄÇ
	
ÂêéÁª≠Êàë‰ª¨Êù•ËÅäËÅäËøô‰∏™ÈóÆÈ¢òÊòØÂ¶Ç‰ΩïË¢´ÂèëÁé∞ÁöÑÔºåÊ¨¢ËøéÂ§ßÂÆ∂‰∏ÄËµ∑ËÆ®ËÆ∫

----- English
Exploring SRE: K8S Cluster Pod Scheduling Issue Fix

Hello everyone, let's discuss how we fixed the issue mentioned in the last story:Exploring SRE: K8S Cluster POD Scheduling Issue Description

Issue Recap
As mentioned in the previous post, due to K8S scheduling issues, all data collection app Pods were concentrated on a single worker node. Each Pod created a large number of threads to handle tasks concurrently, ultimately leading to thread resource exhaustion (thread limit of 65,535), causing the node to crash, and service failure.
Solution Approach: How to distribute application Pods relatively evenly across all worker nodes.

Solution 1: Pod Anti-Affinity
Anti-Affinity is a method that uses scheduling constraints to prevent Pods from clustering on the same node, suitable for scenarios requiring high service availability or resource isolation.
Configuration rules: Define scheduling constraints by setting anti-affinity rules based on Pod labels. For example: prevent multiple replicas of the same Pod from being scheduled to the same node.
Constraint types:
- Hard rules (requiredDuringSchedulingIgnoredDuringExecution): Pods must satisfy anti-affinity requirements during scheduling, otherwise they cannot be scheduled
- Soft rules (preferredDuringSchedulingIgnoredDuringExecution): Attempt to satisfy anti-affinity during scheduling, but allow scheduling to non-compliant nodes when requirements cannot be met
Topology scope: Define the scope where anti-affinity takes effect through topologyKey, such as distribution by node hostname (kubernetes.io/hostname).

Solution 2: Pod Topology Spread Constraints
maxSkew is part of Pod topology spread constraints, used to limit the distribution deviation of Pods across different nodes or zones. Compared to Pod anti-affinity, maxSkew provides more flexible scheduling strategies, allowing certain distribution deviations.
Define distribution deviation: maxSkew specifies the quantity difference of Pods between different topology domains (such as nodes). For example: maxSkew: 1 means the Pod quantity difference between each node is at most 1.
Scheduling strategy: whenUnsatisfiable: DoNotSchedule: If distribution constraints cannot be satisfied, the Pod will not be scheduled.
The scheduler will try to ensure Pods are evenly distributed across different nodes.

Issue Fix
After evaluating both solutions, I chose Solution 2: maxSkew, because it can more flexibly control Pod distribution deviation while allowing a certain degree of scheduling tolerance, avoiding scheduling failures caused by hard rules.
After configuring appropriate maxSkew parameters, the application was redeployed through CI/CD, and its Pod replicas were successfully distributed across worker nodes, with the entire service and cluster running stably.

Summary
Anti-Affinity is about separation: "Keep Pod A away from Pod B."
maxSkew is about balance: "Spread all these identical pods out as evenly as you can."

Next, we'll discuss how this problem was discovered. Welcome everyone to join the discussion.

Sep 15, 2025
----- Chinese
Êé¢Á¥¢ SREÔºöK8S ÈõÜÁæ§ POD Ë∞ÉÂ∫¶ÈóÆÈ¢òËøΩË∏™ËøáÁ®ã

ÈóÆÈ¢òÂàùÁé∞Ôºö4Êúà29Êó•ÔºåÁõëÊéßÁ≥ªÁªüÊé¢ÊµãÂà∞ÂºÇÂ∏∏ÂêéÔºåËá™Âä®ÂèëÂá∫ÂëäË≠¶Âπ∂Êé®ÈÄÅÂà∞Âç≥Êó∂ËÅäÂ§©Â∑•ÂÖ∑ÔºåÊèêÁ§∫K8SÈõÜÁæ§‰∏≠ÊúâÂ§ö‰∏™PodÁä∂ÊÄÅ‰∏∫CrashLoopBackOffÔºå‰∏îÈó¥Èöî15ÂàÜÈíüÊåÅÁª≠Êé®ÈÄÅ3Ê¨°‰ª•‰∏ä„ÄÇÊéíÈô§ÂÅ∂ÂèëÊÉÖÂÜµ‰ª•ÂêéÔºåÂÄºÁè≠SRE‰ªãÂÖ•„ÄÇ
	
ÂàùÊ≠•ÂàÜÊûêÔºö
CrashLoopBackOff Ë°®Á§∫PodÊó†Ê≥ïÊ≠£Â∏∏ÂêØÂä®ÔºåÂèØËÉΩÁî±‰∫éÂ∫îÁî®Êú¨Ë∫´ÈóÆÈ¢ò„ÄÅËµÑÊ∫êÈôêÂà∂ÊàñÂ∫ïÂ±ÇËäÇÁÇπÈóÆÈ¢òÂØºËá¥„ÄÇPodÈîôËØØlogÂ¶Ç‰πãÂâçÂ∏ñÂ≠êÊèêÂà∞ÁöÑÔºöthread_init: Resource temporarily unavailable„ÄÇ
‰∏∫‰∫ÜÊõ¥Âø´ÊÅ¢Â§çÊúçÂä°ÔºåÂÜ≥ÂÆö‰ªéÂ∫ïÂ±ÇËäÇÁÇπËµÑÊ∫êÂÖ•ÊâãÔºåÈÄêÊ≠•ÊéíÊü•ÈóÆÈ¢òÊ†πÊ∫ê„ÄÇ
	
ÂÆö‰ΩçÂ∑•‰ΩúËäÇÁÇπÔºöËµÑÊ∫êËÄóÂ∞ΩÂàùÁé∞Á´ØÂÄ™
ÈÄöËøáÊ£ÄÊü•ÈóÆÈ¢òPodÁöÑË∞ÉÂ∫¶ÊÉÖÂÜµÔºåÂèëÁé∞Ëøô‰∫õPodÂùáËøêË°åÂú®Â∑•‰ΩúËäÇÁÇπd01‰∏ä„ÄÇÈÄöËøáSSHÁôªÂΩïÂà∞ËäÇÁÇπd01Â∞ùËØïÊ∑±ÂÖ•ÂàÜÊûêÔºåÂç¥ÂèëÁé∞Âú®ÂàáÊç¢Áî®Êà∑Êó∂Âá∫Áé∞‰ª•‰∏ãÂºÇÂ∏∏Ôºö
-bash: fork: Cannot allocate memory
ÂàùÊ≠•Áé∞Ë±°ÔºöËäÇÁÇπÊó†Ê≥ïÂàõÂª∫Êñ∞ËøõÁ®ãÔºåÊèêÁ§∫‚ÄúCannot allocate memory‚ÄùÔºåË°®ÊòéËäÇÁÇπÂèØËÉΩÂ∑≤ÁªèËÄóÂ∞Ω‰∫ÜËµÑÊ∫êÔºàÂ¶ÇÂÜÖÂ≠òÊàñÁ∫øÁ®ãÔºâ„ÄÇ
Ëá≥Ê≠§ÔºåÂÜÖÂ≠òËÄóÂ∞ΩÁöÑÊ†πÊú¨ÂéüÂõ†Â∞ö‰∏çÊòéÁ°Æ„ÄÇ
Âú®Ê≤°ÊúâÊõ¥Â§öÁ∫øÁ¥¢ÁöÑÊÉÖÂÜµ‰∏ãÔºå‰∏∫‰∫ÜÊÅ¢Â§çÊúçÂä°ÂäüËÉΩÂπ∂‰øùÊåÅÈõÜÁæ§ÁöÑÁ®≥ÂÆöÊÄßÔºåÂõ¢ÈòüÂÜ≥ÂÆöÈáçÂêØËäÇÁÇπd01„ÄÇ
	
ÈóÆÈ¢òÂ§çÁé∞ÔºöÈó¥Ê≠áÊÄß‰∏≠Êñ≠
5Êúà4Êó•ÊôöÔºåÁ±ª‰ººÈóÆÈ¢òÂÜçÊ¨°ÂèëÁîüÔºåÁõëÊéßÁ≥ªÁªüÊåÅÁª≠Êé®ÈÄÅÂëäË≠¶Ôºåd01ËäÇÁÇπ‰∏äÁöÑÂ§ö‰∏™PodÂÜçÊ¨°CrashLoopBackOffÔºåËäÇÁÇπËµÑÊ∫êÂÜçÊ¨°ËÄóÂ∞Ω„ÄÇÂõ¢ÈòüÂÜçÊ¨°ÈáçÂêØËäÇÁÇπÔºåÊÅ¢Â§çÊúçÂä°„ÄÇ
ÈóÆÈ¢òÁâπÁÇπÔºö
ÊØèÈöîÂá†Â§©Â§çÁé∞‰∏ÄÊ¨°ÔºåË°®Áé∞Âá∫Èó¥Ê≠áÊÄß„ÄÇ
ÈáçÂêØÂêéÈóÆÈ¢òÊöÇÊó∂ÁºìËß£Ôºå‰ΩÜÊú™Ëß£ÂÜ≥Ê†πÂõ†„ÄÇ
	
Ê∑±ÂÖ•ÊéíÊü•ÔºöÂÖ≥ÈîÆÂèëÁé∞
Âä≥Âä®ËäÇÂêéÔºåÂõ¢ÈòüÂØπd01ËäÇÁÇπËøõË°åÂÖ®Èù¢Ê£ÄÊü•ÔºåÂèëÁé∞ËäÇÁÇπ‰∏äÂ≠òÂú®Â§ö‰∏™Áõ∏ÂêåÁöÑJavaËøõÁ®ãÔºå‰∏îÊØè‰∏™ËøõÁ®ãÂπ≥Âùá‰ΩøÁî®8000+Á∫øÁ®ã„ÄÇ
Ëøõ‰∏ÄÊ≠•Ë∞ÉÊü•Á°ÆËÆ§ÔºåËøô‰∫õJavaËøõÁ®ãÂùáÂ±û‰∫éÊï∞ÊçÆÊî∂ÈõÜappÁöÑpodÂâØÊú¨ÔºåË¥üË¥£Êâ´Êèè8k+ÂØπË±°Âπ∂Â§ÑÁêÜÊï∞ÊçÆ„ÄÇappÁöÑÂ§ö‰∏™ÂâØÊú¨ÂêåÊó∂Ë∞ÉÂ∫¶Âú®d01‰∏äÊ∂àËÄóÂ§ßÈáèÁ∫øÁ®ãÊàê‰∏∫ËµÑÊ∫êËÄóÂ∞ΩÁöÑÊ†∏ÂøÉÂéüÂõ†„ÄÇ
	
ÊïÖ‰∫ãÁöÑÂêéÁª≠ÔºåÂ§ßÂÆ∂ÂèØ‰ª•ÂèÇËÄÉÂâçÈù¢ÁöÑÂ∏ñÂ≠êÔºåÂú®Ê≠§ÊàëÁÆÄË¶ÅËØ¥Êòé‰∏ãSREÁöÑ‰∏ªË¶ÅËÅåË¥£Ôºö
ÁõëÊéß‰∏éÂëäË≠¶ÔºöÂØπÈõÜÁæ§ËøõË°åÊúâÊïàÁõëÊéßÂπ∂Á°Æ‰øùÁõëÊéßÂ∑•ÂÖ∑Ëá™Âä®‰∏îÂèäÊó∂Êé®ÈÄÅÂëäË≠¶„ÄÇ
‰ºòÂÖàÊÅ¢Â§çÊúçÂä°ÔºöÂø´ÈÄüÊÅ¢Â§çÂäüËÉΩÔºåÁ°Æ‰øùÂëäË≠¶Ê∂àÈô§„ÄÇ
ÈóÆÈ¢òÂÆö‰Ωç‰∏é‰øÆÂ§çÔºöÊ∑±ÂÖ•ÂàÜÊûêÊ†πÂõ†ÔºåË∑üËøõ‰øÆÂ§çÈóÆÈ¢ò„ÄÇ


----- English
Exploring SRE: K8S Cluster Pod Scheduling Issue Track Process

Hello everyone, let's discuss how we track the issue mentioned in the last story:Exploring SRE: K8S Cluster POD Scheduling Issue Description

Initial Analysis:
CrashLoopBackOff indicates that Pods cannot start normally, possibly due to application issues, resource limitations, or underlying node problems. Pod error logs showed the same issue mentioned in previous posts: "thread_init: Resource temporarily unavailable."
To restore service more quickly, we decided to start from the underlying node resources and gradually investigate the root cause.

Locating Worker Node: Resource Exhaustion Emerges
By checking the scheduling status of problematic Pods, we found that these Pods were all running on worker node d01. When we SSH'd into node d01 for deeper analysis, we encountered the following anomaly when switching users:
-bash: fork: Cannot allocate memory

Initial observation: The node could not create new processes, showing "Cannot allocate memory," indicating that the node had likely exhausted resources (such as memory or threads).
At this point, the root cause of memory exhaustion was still unclear.
Without more clues, to restore service functionality and maintain cluster stability, SRE team decided to restart node d01.

Problem Recurrence: Intermittent Interruptions**
On the evening of May 4th, similar problems occurred again. The monitoring system continuously pushed alerts, multiple Pods on node d01 experienced CrashLoopBackOff again, and node resources were exhausted once more. The team restarted the node again to restore service.

Problem characteristics:
- Recurred every few days, showing intermittent behavior
- Problems were temporarily alleviated after restart, but root cause remained unresolved

Deep Investigation: Key Discovery
Couple of days later, the team conducted a comprehensive check of node d01 and discovered multiple identical Java processes on the node, with each process using an average of 8000+ threads.
Further investigation confirmed that these Java processes all belonged to Pod replicas of the data collection app, responsible for scanning 8k+ objects and processing data. Multiple replicas of the app being scheduled simultaneously on d01 and consuming massive threads became the core reason for resource exhaustion.

For the continuation of this story, you can refer to previous posts. Here I'll briefly explain the main responsibilities of SRE:
- **Monitoring and Alerting**: Effectively monitor the cluster and ensure monitoring tools automatically and promptly push alerts
- **Priority Service Recovery**: Quickly restore functionality and ensure alerts are cleared
- **Problem Location and Fix**: Deep analysis of root causes, follow up on problem fixes

Sep 18, 2025
----- Chinese
Êµ∑Âõ†ÈáåÂ∏åÊ≥ïÂàôÔºöÈ¢ÑÈò≤Â∞èÈóÆÈ¢òÔºåÈÅøÂÖçÂ§ßÁÅæÈöæ
Â§ßÂÆ∂Â•ΩÔºå‰ªäÂ§©Êàë‰ª¨Êù•‰∏ÄËµ∑Â≠¶‰π†‰∏ãÂÆâÂÖ®Áîü‰∫ßÁöÑÁêÜËÆ∫Áü•ËØÜ(Êç¢‰∏Ä‰∏™Â≠¶‰π†ÁöÑÈ¶ñÈ°µÂõæÁâáüòÇ)Ôºå‰ªéÊµ∑Âõ†ÈáåÂ∏åÊ≥ïÂàôÂºÄÂßã„ÄÇ
	
1Ô∏è‚É£ Êµ∑Âõ†ÈáåÂ∏åÊ≥ïÂàô
Â¶ÇÂõæ2ÔºåHeinrichÊ≥ïÂàôÊúÄÂàùÊù•Ê∫ê‰∫éÂ∑•‰∏öÂÆâÂÖ®È¢ÜÂüüÔºåÁî±ÂÆâÂÖ®‰∏ìÂÆ∂Herbert William HeinrichÂú®20‰∏ñÁ∫™30Âπ¥‰ª£ÊèêÂá∫„ÄÇÂÆÉÊèèËø∞‰∫Ü‰∫ãÊïÖÁöÑÂèëÁîüËßÑÂæãÔºö
‚ÄºÔ∏è‚ÄºÔ∏è‚ÄºÔ∏è ÊØè‰∏ÄËµ∑‰∏•Èáç‰∫ãÊïÖÁöÑËÉåÂêéÔºåÂæÄÂæÄÊúâ29Ëµ∑ËΩªÂæÆ‰∫ãÊïÖÔºå‰ª•Âèä300Ëµ∑Êú™ÈÅÇ‰∫ã‰ª∂ÊàñÈöêÊÇ£„ÄÇ
Êç¢Âè•ËØùËØ¥ÔºåÈáçÂ§ß‰∫ãÊïÖÂæÄÂæÄÊòØÁî±‰∏ÄÁ≥ªÂàóËæÉÂ∞èÁöÑÈóÆÈ¢òÁßØÁ¥ØÊàñÊú™Ë¢´Ëß£ÂÜ≥ÁöÑÈöêÊÇ£ÊâÄÂºïÂèëÁöÑ„ÄÇÂ¶ÇÊûúÊàë‰ª¨ËÉΩÂèäÊó∂ËØÜÂà´ÂíåÂ§ÑÁêÜËøô‰∫õÂ∞èÈóÆÈ¢òÔºåÂ∞±ÂèØ‰ª•ÊúâÊïàÂú∞È¢ÑÈò≤Êõ¥‰∏•ÈáçÁöÑÂêéÊûú„ÄÇ
	
2Ô∏è‚É£ Âú®SREÈ¢ÜÂüüÔºåÊàë‰ª¨Ëá¥Âäõ‰∫éÈÄöËøáÊèêÈ´òÁ≥ªÁªüÁöÑÂèØÈù†ÊÄßÂíåÁ®≥ÂÆöÊÄßÔºåÊù•‰∏∫Áî®Êà∑Êèê‰æõÂßãÁªàÂ¶Ç‰∏ÄÁöÑÊúçÂä°‰ΩìÈ™å„ÄÇ‰∏∫‰∫ÜÂÆûÁé∞Ëøô‰∏ÄÁõÆÊ†áÔºåÊàë‰ª¨‰∏ç‰ªÖË¶ÅËß£ÂÜ≥Â∑≤ÁªèÂèëÁîüÁöÑÈóÆÈ¢òÔºåÊõ¥ÈúÄË¶Å‰ªéÊΩúÂú®ÈóÆÈ¢ò‰∏≠Â≠¶‰π†ÔºåÈò≤ÊÇ£‰∫éÊú™ÁÑ∂„ÄÇÂú®Ëøô‰∏ÄËøáÁ®ã‰∏≠ÔºåHeinrichÊ≥ïÂàô‰∏∫Êàë‰ª¨Êèê‰æõ‰∫Ü‰∏Ä‰∏™ÊûÅÂÖ∑‰ª∑ÂÄºÁöÑËßÜËßí„ÄÇ
Âú®SREÂÆûË∑µ‰∏≠ÔºåHeinrichÊ≥ïÂàôÊúâÁùÄÂπøÊ≥õÁöÑÂ∫îÁî®Âú∫ÊôØ„ÄÇÂ∞ΩÁÆ°ÂÆÉÊ∫ê‰∫éÂÆâÂÖ®È¢ÜÂüüÔºå‰ΩÜÂÖ∂Ê†∏ÂøÉÊÄùÊÉ≥‚Äî‚ÄîÈÄöËøáÂÖ≥Ê≥®ÂíåËß£ÂÜ≥Â∞èÈóÆÈ¢òÊù•ÈÅøÂÖçÂ§ßÈóÆÈ¢ò‚Äî‚Äî‰∏éSREÁöÑÁõÆÊ†áÈ´òÂ∫¶Â•ëÂêà„ÄÇ‰ª•‰∏ãÊòØÂá†‰∏™ÂÖ≥ÈîÆÁÇπÔºö
 Â∞èÈóÆÈ¢òÁßØÁ¥ØÂØºËá¥Â§ßÊïÖÈöú
Âú®Â§çÊùÇÁöÑÂàÜÂ∏ÉÂºèÁ≥ªÁªü‰∏≠ÔºåÂá†‰πéÊâÄÊúâÁöÑÈáçÂ§ßÁ≥ªÁªüÊïÖÈöúÔºàÂ¶ÇÂÖ®Á´ôÂÆïÊú∫„ÄÅÊï∞ÊçÆ‰∏¢Â§±Á≠âÔºâÈÉΩ‰∏çÊòØÂçï‰∏Ä‰∫ã‰ª∂ÁöÑÁªìÊûúÔºåËÄåÊòØÁî±Â§ö‰∏™Â∞èÈóÆÈ¢òÂè†Âä†ÂØºËá¥ÁöÑ„ÄÇ
 Â¶ÇÂâçÊñáÊèêÂà∞ÁöÑÂÆû‰æãÔºåk8sÈõÜÁæ§‰∏≠Êï∞ÊçÆÊî∂ÈõÜappÁöÑPODÂâØÊú¨‰∏çÂêàÁêÜË∞ÉÂ∫¶ÂèØËÉΩÊúÄÂàùÂè™ÊòØÂΩ±ÂìçÈõÜÁæ§Â∑•‰ΩúËäÇÁÇπÁöÑÁ®≥ÂÆöÔºå‰ΩÜÂ¶ÇÊûú‰∏çÂèäÊó∂Â§ÑÁêÜÔºåÂèØËÉΩ‰ºöÂØºËá¥Êï¥‰∏™ÈõÜÁæ§ÁöÑ‰∏çÁ®≥ÂÆöÁîöËá≥Â¥©Ê∫É„ÄÇ
	
 ÈáçËßÜÊú™ÈÅÇ‰∫ã‰ª∂ÔºàNear MissÔºâ
Êú™ÈÅÇ‰∫ã‰ª∂ÊòØÊåáÈÇ£‰∫õÊ≤°ÊúâÁõ¥Êé•ÂΩ±ÂìçÂà∞Áîü‰∫ßÁéØÂ¢ÉÔºå‰ΩÜÊö¥Èú≤‰∫ÜÊΩúÂú®ÈóÆÈ¢òÁöÑ‰∫ã‰ª∂„ÄÇ
 Ëøô‰∏ÄÁÇπÊàë‰πüÊúâËØùË¶ÅËØ¥ÔºåËØ∑ÂÖ≥Ê≥®ÂêéÁª≠ÂàÜ‰∫´
	
 ÊûÑÂª∫‚ÄúÂÖçÁñ´Á≥ªÁªü‚Äù
SREÁöÑÊ†∏ÂøÉ‰πã‰∏ÄÊòØÊûÑÂª∫ÂèØËßÇÊµãÊÄßÂíåËá™Âä®ÂåñÁöÑÂ∑•ÂÖ∑ÈìæÔºå‰ª•Âø´ÈÄüÂèëÁé∞Âíå‰øÆÂ§çÈóÆÈ¢ò„ÄÇÈÄöËøáÁõëÊéßÂíåÊó•ÂøóÁ≥ªÁªüÔºåSREÂõ¢ÈòüÂèØ‰ª•ÊçïÊçâÂà∞Â∞èÈóÆÈ¢òÁöÑ‰ø°Âè∑ÔºåÊØîÂ¶ÇÈîôËØØÁéáÁöÑËΩªÂæÆ‰∏äÂçá„ÄÅÂª∂ËøüÁöÑÁü≠ÊöÇÊ≥¢Âä®Á≠â„ÄÇËøô‰∫õ‚ÄúÂ∞è‰ø°Âè∑‚ÄùÊ≠£ÊòØHeinrichÊ≥ïÂàô‰∏≠ÈöêÊÇ£ÁöÑË°®Áé∞ÔºåÂèäÊó∂ÂìçÂ∫îÊòØÈ¢ÑÈò≤ÁÅæÈöæÁöÑÂÖ≥ÈîÆ„ÄÇ
 Â§ßÂÆ∂ÊúâÊÉ≥ËøáÊ≤°ÊúâÔºåÂèäÊó∂ÂìçÂ∫îÁöÑÂâçÊèêÊòØÊî∂Âà∞ÈÄöÁü•ÔºåÈÇ£Â¶Ç‰ΩïÈ´òÊïàÂú∞Êî∂Âà∞ÈÄöÁü•Âë¢Ôºü
ÂèëÈÇÆ‰ª∂Ôºü‰ºöË¢´Ê∑πÊ≤°Âú®‰∏ÄÂ†ÜÈÇÆ‰ª∂‰∏≠ÔºåÊØèÈöî5ÂàÜÈíüÊ£ÄÊü•‰∏ÄÊ¨°ÈÇÆÁÆ±‰πü‰∏çÂèØËÉΩÔºõ
ÂèëÂç≥Êó∂Ê∂àÊÅØÔºüÁªìÊûúÁ±ª‰ººÔºåÂ§ßÂÆ∂ÊúâÂï•Âª∫ËÆÆËØ∑üôã
	
3Ô∏è‚É£ ÊÉ≥Ëµ∑Â∞èÊó∂ÂÄôÁúãËøáÂÆ£‰º†ËäÇËÉΩÁÅØÁöÑÂÖ¨ÁõäÂπøÂëäÔºöÂÜçÂ∞èÁöÑÊîØÊåÅ‰πüÊòØ‰∏ÄÁßçÂäõÈáèÔºÅÂú®SREÁöÑ‰∏ñÁïåÈáåÔºåÊØè‰∏ÄÊ¨°Â∞èÁöÑÊîπËøõÔºåÈÉΩÊòØÂØπÂ§ßÁÅæÈöæÁöÑÊúâÊïàÈ¢ÑÈò≤„ÄÇ
----- English
Heinrich's Rule: Prevent Small Problems, Avoid Major Disasters

Hello everyone, today let's learn about safety production theory together (with a different learning homepage image üòÇ), starting with Heinrich's Rule.

1Ô∏è‚É£ Heinrich's Rule
As shown in Figure 2, Heinrich's Rule originally comes from the industrial safety field, proposed by safety expert Herbert William Heinrich in the 1930s. It describes the pattern of accident occurrence:
‚ÄºÔ∏è‚ÄºÔ∏è‚ÄºÔ∏è Behind every serious accident, there are often 29 minor accidents and 300 near misses or hazards.
In other words, major accidents are often triggered by a series of smaller problems that accumulate or unresolved hazards. If we can identify and address these small problems in time, we can effectively prevent more serious consequences.

2Ô∏è‚É£ In the SRE field, we are committed to providing users with a consistent service experience by improving system reliability and stability. To achieve this goal, we need not only to solve problems that have already occurred, but also to learn from potential problems and prevent them before they happen. In this process, Heinrich's Rule provides us with an extremely valuable perspective.

In SRE practice, Heinrich's Rule has wide application scenarios. Although it originates from the safety field, its core idea - preventing major problems by focusing on and solving small problems - is highly aligned with SRE goals. Here are several key points:

**Small Problems Accumulate to Cause Major Failures**
In complex distributed systems, almost all major system failures (such as site-wide outages, data loss, etc.) are not the result of a single event, but are caused by the accumulation of multiple small problems.
As mentioned in the previous example, unreasonable scheduling of data collection app POD replicas in a k8s cluster may initially only affect the stability of cluster worker nodes, but if not handled promptly, it could lead to instability or even collapse of the entire cluster.

**Pay Attention to Near Miss Events**
Near miss events refer to those events that do not directly affect the production environment but expose potential problems.
I also have something to say about this point, please follow subsequent sharing.

**Building an "Immune System"**
One of the core aspects of SRE is building observability and automated toolchains to quickly discover and fix problems. Through monitoring and logging systems, SRE teams can capture signals of small problems, such as slight increases in error rates, brief fluctuations in latency, etc. These "small signals" are exactly the manifestation of hazards in Heinrich's Rule, and timely response is key to preventing disasters.
Have you ever thought about it? The prerequisite for timely response is receiving notifications. So how can we efficiently receive notifications?
Send emails? They'll be buried in a pile of emails, and checking email every 5 minutes is impossible;
Send instant messages? The result is similar. What suggestions do you have? Please üôã

3Ô∏è‚É£ In the SRE world, every small improvement is an effective prevention against major disasters.

Sep 21, 2025
----- Chinese
Kubernetes Secret ÁÆÄ‰ªã
Â§ßÂÆ∂Â•ΩÔºåËÅäËøáÂÆâÂÖ®Áîü‰∫ßÊ≥ïÂàô‰ª•ÂêéÔºåÊàëÊù•ÁªßÁª≠ÂàÜ‰∫´K8S‰ΩøÁî®ËøáÁ®ã‰∏≠ÂèëÁîüÁöÑÊ°à‰æã 
ËøôÊòØ‰∏Ä‰∏™ÈöêÁßò‰∏îÂæàÂÆπÊòìË∏©ÂùëÁöÑÊ°à‰æãÔºå‰ªé‰ªãÁªçK8SÁöÑsecretÂºÄÂßã„ÄÇ
Âú® K8S ‰∏≠ÔºåSecret ÊòØ‰∏ÄÁßç‰∏ìÈó®Áî®‰∫éÂ≠òÂÇ®ÊïèÊÑü‰ø°ÊÅØÔºàÂ¶ÇÂØÜÁ†Å„ÄÅAPI ÂØÜÈí•„ÄÅËØÅ‰π¶Á≠âÔºâÁöÑËµÑÊ∫êÁ±ªÂûã„ÄÇÂÆÉÂèØ‰ª•Â∏ÆÂä©Êàë‰ª¨Êõ¥ÂÆâÂÖ®Âú∞ÁÆ°ÁêÜÂíå‰ΩøÁî®Ëøô‰∫õÊï∞ÊçÆ„ÄÇ
1Ô∏è‚É£ ‰∏∫‰ªÄ‰πàË¶ÅÁî® Secret
Âú®ÁÆ°ÁêÜÊïèÊÑü‰ø°ÊÅØÊó∂ÔºåÁõ¥Êé•Â∞ÜËøô‰∫õÊï∞ÊçÆÂ≠òÂÇ®Âú®‰ª£Á†ÅÊàñÈÖçÁΩÆÊñá‰ª∂‰∏≠ÂèØËÉΩÂ≠òÂú®‰ª•‰∏ãÈóÆÈ¢òÔºö
‚ö†Ô∏è ÂÆâÂÖ®È£éÈô©ÔºöÊïèÊÑü‰ø°ÊÅØÂÆπÊòìÊòéÊñáÊö¥Èú≤ÔºåÁâπÂà´ÊòØÂú®ÁâàÊú¨ÊéßÂà∂Á≥ªÁªüÔºàÂ¶Ç GitÔºâ‰∏≠„ÄÇ
‚ö†Ô∏è Áº∫‰πèÁÅµÊ¥ªÊÄßÔºö‰∏çÂêåÁéØÂ¢É‰∏≠ÁöÑÊïèÊÑü‰ø°ÊÅØÂèØËÉΩ‰∏çÂêåÔºåÁõ¥Êé•Á°¨ÁºñÁ†Å‰ºöÂØºËá¥‰øÆÊîπÂõ∞Èöæ„ÄÇ
‚ö†Ô∏è ÊùÉÈôêÁÆ°ÁêÜ‰∏çË∂≥ÔºöÊó†Ê≥ïÊúâÊïàÊéßÂà∂Âì™‰∫õÁî®Êà∑ÊàñÂ∫îÁî®ÂèØ‰ª•ËÆøÈóÆËøô‰∫õÊï∞ÊçÆ„ÄÇ
‰ΩøÁî® Secret ÁöÑÂ•ΩÂ§ÑÂåÖÊã¨Ôºö
üëç ÂÆâÂÖ®Â≠òÂÇ®ÔºöÂ∞ÜÊïèÊÑü‰ø°ÊÅØ‰∏éÂ∫îÁî®Ëß£ËÄ¶ÔºåÈôç‰ΩéÊ≥ÑÈú≤È£éÈô©„ÄÇ
üëç ÊùÉÈôêÊéßÂà∂ÔºöÈÄöËøá Kubernetes ÁöÑ RBAC Êú∫Âà∂ÔºåÁ°Æ‰øùÂè™ÊúâË¢´ÊéàÊùÉÁöÑÂ∫îÁî®ÂèØ‰ª•ËÆøÈóÆÂØπÂ∫îÁöÑ Secret„ÄÇ
üëç Âä®ÊÄÅÊõ¥Êñ∞ÔºöÊîØÊåÅÊïèÊÑü‰ø°ÊÅØÁöÑÂä®ÊÄÅÊõ¥Êñ∞ÔºåÊó†ÈúÄÈáçÊñ∞ÈÉ®ÁΩ≤Â∫îÁî®„ÄÇ
	
2Ô∏è‚É£ Secret ‰∏≠ÁöÑÂÄºÈúÄË¶Å‰ª• Base64 ÁºñÁ†ÅÂ≠òÂÇ®ÔºåËøôÂπ∂‰∏çÊòØ‰∏∫‰∫ÜÂä†ÂØÜÔºåËÄåÊòØÂá∫‰∫é‰ª•‰∏ãÂéüÂõ†Ôºö
Ê†ºÂºèÂÖºÂÆπÔºöBase64 ÂèØ‰ª•Â∞ÜÊï∞ÊçÆËΩ¨Êç¢‰∏∫Âè™ÂåÖÂê´ ASCII Â≠óÁ¨¶ÁöÑÂ≠óÁ¨¶‰∏≤ÔºåÁ°Æ‰øùÂç≥‰ΩøÊòØ‰∫åËøõÂà∂Êï∞ÊçÆ‰πüËÉΩÂÆâÂÖ®Âú∞Â≠òÂÇ®Âú® YAML/JSON Êñá‰ª∂‰∏≠„ÄÇ
ÂáèÂ∞ëÊòéÊñáÊö¥Èú≤ÔºöËôΩÁÑ∂‰∏çÊòØÂä†ÂØÜÊâãÊÆµÔºå‰ΩÜ Base64 ÂèØ‰ª•Âú®‰∏ÄÂÆöÁ®ãÂ∫¶‰∏äÈÅøÂÖçÊï∞ÊçÆË¢´Áõ¥Êé•ËØÜÂà´„ÄÇ
API Ë¶ÅÊ±ÇÔºöKubernetes ÁöÑ API ËÆæËÆ°Ë¶ÅÊ±Ç Secret ÁöÑÂÄº‰ª• Base64 ÁºñÁ†ÅÂΩ¢ÂºèÂ≠òÂÇ®„ÄÇ
‚ÄºÔ∏è‚ÄºÔ∏è‚ÄºÔ∏è ÈúÄË¶ÅÊ≥®ÊÑèÁöÑÊòØÔºåBase64 Âπ∂‰∏çËÉΩÁúüÊ≠£‰øùÊä§Êï∞ÊçÆÁöÑÂÆâÂÖ®„ÄÇÂ¶ÇÊûúÈúÄË¶ÅÊõ¥È´òÁöÑÂÆâÂÖ®ÊÄßÔºåÂèØ‰ª•ÁªìÂêàÂ§ñÈÉ®ÂØÜÈí•ÁÆ°ÁêÜÁ≥ªÁªü‰ΩøÁî®„ÄÇ
	
3Ô∏è‚É£ Â∫îÁî®ÂèØ‰ª•ÈÄöËøá‰ª•‰∏ãÊñπÂºè‰ΩøÁî® Secret ‰∏≠Â≠òÂÇ®ÁöÑÊïèÊÑü‰ø°ÊÅØÔºö
Ê≥®ÂÖ•ÁéØÂ¢ÉÂèòÈáèÔºåÂ¶ÇÂõæ3ÊâÄÁ§∫Ôºö
Â∞Ü Secret ÁöÑÂÄºÊ≥®ÂÖ•Âà∞ Pod ÁöÑÁéØÂ¢ÉÂèòÈáè‰∏≠ÔºåÂ∫îÁî®ÂèØ‰ª•ÈÄöËøáÁéØÂ¢ÉÂèòÈáèÁõ¥Êé•ËØªÂèñËøô‰∫õÊï∞ÊçÆ„ÄÇËøôÁßçÊñπÂºèÈÄÇÂêàÁÆÄÂçïÁöÑÈÖçÁΩÆÂú∫ÊôØ„ÄÇ
ÊåÇËΩΩÂà∞Êñá‰ª∂Á≥ªÁªü
Â∞Ü Secret ÁöÑÂÄº‰ª•Êñá‰ª∂ÂΩ¢ÂºèÊåÇËΩΩÂà∞ÂÆπÂô®ÂÜÖÔºåÂ∫îÁî®ÂèØ‰ª•ÈÄöËøáËØªÂèñÊñá‰ª∂Êù•Ëé∑ÂèñÊïèÊÑü‰ø°ÊÅØ„ÄÇËøôÁßçÊñπÂºèÈÄÇÂêàÁÆ°ÁêÜÂ§çÊùÇÊàñÂ§ßÊï∞ÊçÆÈáèÁöÑÊïèÊÑüÂÜÖÂÆπ„ÄÇ
	
ÈÄöËøáÂêàÁêÜ‰ΩøÁî® SecretÔºåÂèØ‰ª•Â∏ÆÂä©ÂºÄÂèëËÄÖÊõ¥ÂÆâÂÖ®„ÄÅÈ´òÊïàÂú∞ÁÆ°ÁêÜÊïèÊÑü‰ø°ÊÅØ„ÄÇÊé•‰∏ãÊù•ÊàëÂ∞ÜÂÖ∑‰ΩìËØ¥ÊòéÊ°à‰æãÂÜÖÂÆπ‰ª•ÂèäÂ¶Ç‰Ωï‰øÆÂ§ç„ÄÇ

----- English

Introduction to Kubernetes Secret

Hello everyone, after discussing safety production rules, I'll continue sharing cases that occurred during K8S usage.
This is a hidden and easily overlooked case, starting with an introduction to K8S secrets.

In K8S, Secret is a resource type specifically designed for storing sensitive information (such as passwords, API keys, certificates, etc.). It helps us manage and use this data more securely.

1Ô∏è‚É£ Why Use Secret

When managing sensitive information, directly storing this data in code or configuration files may present the following problems:
‚ö†Ô∏è Security risks: Sensitive information is easily exposed in plain text, especially in version control systems (like Git).
‚ö†Ô∏è Lack of flexibility: Sensitive information may differ across environments, and hard-coding makes modifications difficult.
‚ö†Ô∏è Insufficient permission management: Cannot effectively control which users or applications can access this data.

Benefits of using Secret include:
üëç Secure storage: Decouples sensitive information from applications, reducing exposure risks.
üëç Permission control: Through Kubernetes' RBAC mechanism, ensures only authorized applications can access corresponding Secrets.
üëç Dynamic updates: Supports dynamic updates of sensitive information without redeploying applications.

2Ô∏è‚É£ Values in Secret need to be stored in Base64 encoding, which is not for encryption but for the following reasons:

Format compatibility: Base64 can convert data into strings containing only ASCII characters, ensuring even binary data can be safely stored in YAML/JSON files.
Reduce plain text exposure: While not an encryption method, Base64 can prevent data from being directly identified to some extent.
API requirements: Kubernetes' API design requires Secret values to be stored in Base64 encoded format.
‚ÄºÔ∏è‚ÄºÔ∏è‚ÄºÔ∏è It's important to note that Base64 cannot truly protect data security. If higher security is needed, it can be used in combination with external key management systems.

3Ô∏è‚É£ Applications can use sensitive information stored in Secret through the following methods:

Inject environment variables, as shown in Figure 3:
Inject Secret values into Pod environment variables, allowing applications to directly read this data through environment variables. This method is suitable for simple configuration scenarios.

Mount to file system:
Mount Secret values as files into containers, allowing applications to obtain sensitive information by reading files. This method is suitable for managing complex or large-volume sensitive content.

By properly using Secret, developers can manage sensitive information more securely and efficiently. Next, I will specifically explain the case content and how to fix it.

Sep 22, 2025
----- Chinese
Kubernetes Secret Ë∏©ÂùëÊ°à‰æã
Â§ßÂÆ∂Â•ΩÔºåÊé•‰∏äÁØáK8S secret ÁÆÄ‰ªãÔºå‰∏∫‰∫ÜÂú®ÈõÜÁæ§‰∏≠ÂÆâÂÖ®Â≠òÂÇ®ÊïèÊÑüÊï∞ÊçÆÔºàÂ¶ÇÁî®Êà∑Âêç„ÄÅÂØÜÁ†Å„ÄÅAPI ÂØÜÈí•Á≠âÔºâÔºåÈÄöÂ∏∏‰ΩøÁî® Secret Êù•ËøõË°åÁÆ°ÁêÜ„ÄÇÂ∫îÁî®Á®ãÂ∫èÂèØ‰ª•ÈÄöËøáÊåÇËΩΩÊñá‰ª∂ÊàñÁéØÂ¢ÉÂèòÈáèÁöÑÊñπÂºèËÆøÈóÆ Secret ‰∏≠ÁöÑÈîÆÂÄºÂØπÔºåÂπ∂Ê†πÊçÆÂÆûÈôÖÊÉÖÂÜµ‰ΩøÁî®Ëøô‰∫õ‰ø°ÊÅØ„ÄÇÁÑ∂ËÄåÔºåSecret ÁöÑ‰ΩøÁî®Ëã•Âá∫Áé∞ÈÖçÁΩÆ‰∏çÂΩìÔºåÂèØËÉΩ‰ºöÂØºËá¥Â∫îÁî®Á®ãÂ∫èËøêË°åÂºÇÂ∏∏„ÄÇ
	
ÁúüÂÆûÊ°à‰æãÔºöÂ∫îÁî®Á®ãÂ∫èÂõ† Secret ÁöÑÂÄºÂÜÖÂÆπÈóÆÈ¢òÂØºËá¥Êï∞ÊçÆÂ∫ìËøûÊé•Â§±Ë¥•„ÄÇ
	
1Ô∏è‚É£ ÈóÆÈ¢òÊèèËø∞
ÂàõÂª∫secretÔºåÁî®‰∫éÂ≠òÂÇ®Êï∞ÊçÆÂ∫ìÁöÑËøûÊé•‰ø°ÊÅØ„ÄÇËÆøÈóÆÊï∞ÊçÆÂ∫ìÁöÑÂëΩ‰ª§ÂèØ‰ª•ÂΩíÁ∫≥Âà∞genericÁ±ªÂûãÁöÑsecret‰∏≠,ÊúâÂ¶Ç‰∏ã‰∏§ÁßçÂàõÂª∫ÊñπÊ≥ï:
1. kubectl create secret generic db-credentials --from-file=username=username.txt --from-file=password=password.txt
     username.txtÂíåpassword.txtÊòØ‰∏§‰∏™ÊñáÊú¨Êñá‰ª∂,ÂÜÖÂÆπÂàÜÂà´‰∏∫root,ÂÜÖÂÆπ‰∏∫password
  
2. kubectl create secret generic db-credentials --from-literal=username=root --from-literal=password=password
     Áõ¥Êé•Âú®ÂëΩ‰ª§Ë°å‰∏≠ËæìÂÖ•Áî®Êà∑ÂêçÂíåÂØÜÁ†Å
  
Ëøô‰∏§ÁßçÊñπÊ≥ïÂàõÂª∫ÁöÑsecretÊòØ‰∏ÄÊ†∑ÁöÑ,ÈÉΩÂèØ‰ª•ÈÄöËøákubectl get secret db-credentials -o yamlÊü•ÁúãÂà∞secretÁöÑÂÜÖÂÆπ
‰ΩÜÊòØÁ¨¨‰∫åÁßçÊñπÊ≥ïÂàõÂª∫ÁöÑsecretÂú®yamlÊñá‰ª∂‰∏≠ÊòØ‰ª•stringÁ±ªÂûãÂ≠òÂÇ®ÁöÑ,ËÄåÁ¨¨‰∏ÄÁßçÊñπÊ≥ïÂàõÂª∫ÁöÑsecretÂú®yamlÊñá‰ª∂‰∏≠ÊòØ‰ª•binaryDataÁ±ªÂûãÂ≠òÂÇ®ÁöÑ
Ëøô‰∏§ÁßçÊñπÊ≥ïÂàõÂª∫ÁöÑsecretÂú®‰ΩøÁî®Êó∂Ê≤°ÊúâÂå∫Âà´,ÈÉΩÂèØ‰ª•ÈÄöËøákubectl get secret db-credentials -o yamlÊü•ÁúãÂà∞secretÁöÑÂÜÖÂÆπ

Ê†πÊçÆÂ∫îÁî®Á®ãÂ∫èÁöÑdeploy yamlÊñá‰ª∂ÈÖçÁΩÆÔºåpodÂêØÂä®Êó∂‰ºöËØªÂèñSecret db-credentials‰∏≠ÁöÑ username Âíå password ÂÄºÊù•ËøûÊé•Êï∞ÊçÆÂ∫ì„ÄÇÁÑ∂ËÄåÔºåÂú®ÂêØÂä®Êó∂ÔºåÁ®ãÂ∫èÊä•ÈîôËØØÊó•ÂøóÂ¶Ç‰∏ãÔºö
Unable to obtain connection from database (jdbc:mysql://mysqldb:3306/mysql?Unicode=true&characterEncoding=UTF-8) for user 'root': Access denied for user 'root'@'10.4.214.53' (using password: YES)

	
2Ô∏è‚É£ ‰ªéÊó•ÂøóÂàÜÊûê
Â∫îÁî®Á®ãÂ∫èÊú™ËÉΩÊàêÂäü‰ΩøÁî®Êèê‰æõÁöÑÁî®Êà∑ÂêçÂíåÂØÜÁ†ÅËøûÊé•Êï∞ÊçÆÂ∫ì„ÄÇÈîôËØØÊèêÁ§∫ÊòæÁ§∫ÔºåÊï∞ÊçÆÂ∫ìÊãíÁªù‰∫ÜÁî®Êà∑Âêç root ÁöÑËÆøÈóÆ„ÄÇ

3Ô∏è‚É£ ÊéíÊü•ËøáÁ®ã
Ê£ÄÊü•Êï∞ÊçÆÂ∫ìÈÖçÁΩÆÔºåÈ¶ñÂÖàÊ£ÄÊü•‰∫ÜÂ∫îÁî®Á®ãÂ∫èÁöÑÊï∞ÊçÆÂ∫ìËøûÊé•ÈÖçÁΩÆÔºåÁ°ÆËÆ§Êï∞ÊçÆÂ∫ìÂú∞ÂùÄ„ÄÅÁ´ØÂè£„ÄÅÁî®Êà∑ÂêçÂíåÂØÜÁ†ÅÂùáÊ≠£Á°ÆÊó†ËØØ„ÄÇ
ÈÄöËøá‰ª•‰∏ãÂëΩ‰ª§Êü•Áúã Secret ÁöÑÂÜÖÂÆπÔºö‚Äúkubectl get secret db-credentials -o yaml‚ÄùÔºåÂèëÁé∞ Secret ‰∏≠ÁöÑ username Âíå password Â≠óÊÆµÂ≠òÂÇ®ÁöÑÊòØ Base64 ÁºñÁ†ÅÂêéÁöÑÂ≠óÁ¨¶‰∏≤ÔºåËÄå‰∏çÊòØÁõ¥Êé•Â≠òÂÇ®ÁöÑÊòéÊñáÔºåËøôÊòØÈ¢ÑÊúüË°å‰∏∫„ÄÇ
Ëß£Á†Å Secret ÂÜÖÂÆπ Ëß£Á†ÅÂêéÂèëÁé∞Ôºåusername Âíå password ÁöÑÂÄºÁ°ÆÂÆûÊòØÊ≠£Á°ÆÁöÑÊòéÊñáÂ≠óÁ¨¶‰∏≤„ÄÇ
Âú®Ëøõ‰∏ÄÊ≠•ËØ¶ÁªÜÂØπÊØîÂêéÔºåÂèëÁé∞ÈóÆÈ¢òÂá∫Áé∞Âú® Secret ÁöÑÂàõÂª∫ÊñπÂºè ‰∏ä„ÄÇÊìç‰Ωú‰∫∫Âëò‰ΩøÁî®‰∏äËø∞ÂàõÂª∫secretÁöÑÊñπÊ≥ï‰∏ÄÂàõÂª∫‰∫Ü Secret„ÄÇÂÖ∂‰∏≠Ôºåusername.txt Âíå password.txt ‰∏§‰∏™ÊñáÊú¨Êñá‰ª∂ÊòØÈÄöËøáÊñáÊú¨ÁºñËæëÂô® vim ÁîüÊàêÁöÑÔºåÂú®ËæìÂÖ•ÂÜÖÂÆπÂêéÁõ¥Êé•‰øùÂ≠òÈÄÄÂá∫Ôºåvim ÈªòËÆ§‰ºöÂú®Êñá‰ª∂Êú´Â∞æÊ∑ªÂä†‰∏Ä‰∏™Êç¢Ë°åÁ¨¶Ôºà\nÔºâ„ÄÇÂõ†Ê≠§ÔºåÊñá‰ª∂ÁöÑÁúüÂÆûÂÜÖÂÆπÂàÜÂà´ÂèòÊàê‰∫Ü(root\n)Âíå(password\n), Base64 ÁºñÁ†Å‰ºöÂ∞ÜÊç¢Ë°åÁ¨¶‰πüËßÜ‰∏∫ÊúâÊïàÂ≠óÁ¨¶ÔºåÊúÄÁªàÂØºËá¥ Secret ‰∏≠ÁöÑÂÄºÂåÖÂê´‰∫ÜÈ¢ùÂ§ñÁöÑÊç¢Ë°åÁ¨¶„ÄÇ
	
4Ô∏è‚É£ Ëß£ÂÜ≥ÊñπÊ≥ïÔºö
Áõ¥Êé•‰ΩøÁî®‰ª•‰∏ãÂëΩ‰ª§ÁîüÊàêÊ≤°ÊúâÊç¢Ë°åÁ¨¶ÁöÑÊñá‰ª∂Ôºöecho -n "password" > password.txtÔºåÈáçÊñ∞ÁîüÊàêsecret
‰ΩøÁî®Âõæ3ÁöÑÊñπÊ°à‰∫åÂàõÂª∫ÈáçÊñ∞secretÔºåÂú®ÂëΩ‰ª§‰∏≠Áõ¥Êé•ÊåáÂÆöÂÜÖÂÆπ
ÈáçÂêØÂ∫îÁî®ÔºåÈóÆÈ¢ò‰øÆÂ§ç

----- English

Kubernetes Secret Troubleshooting Case Study

Hello everyone, following up on the previous K8S Secret introduction, Secrets are typically used to securely store sensitive data (such as usernames, passwords, API keys, etc.) in clusters. Applications can access key-value pairs in Secrets by mounting files or environment variables, and use this information according to actual needs. However, improper configuration of Secrets may cause application runtime exceptions.

Real case: Application database connection failure due to Secret value content issues.

1Ô∏è‚É£ Issue Description
Creating a secret in K8S cluster to store database connection information. In general, we create generic type secret with following two methods:
1. kubectl create secret generic db-credentials --from-file=username=username.txt --from-file=password=password.txt
   username.txt and password.txt are two text files, with content "root" and "password" respectively

2. kubectl create secret generic db-credentials --from-literal=username=root --from-literal=password=password
   Directly input username and password in the command line

These two methods create identical secrets, both can be viewed through kubectl get secret db-credentials -o yaml to see the secret content
However, the secret created by the second method is stored as string type in the yaml file, while the secret created by the first method is stored as binaryData type in the yaml file
There is no difference when using secrets created by these two methods, both can be viewed through kubectl get secret db-credentials -o yaml to see the secret content

According to the application's deploy yaml file configuration, the pod reads the username and password values from Secret db-credentials to connect to the database during startup. However, during startup, the program reported the following error log:
Unable to obtain connection from database (jdbc:mysql://mysqldb:3306/mysql?Unicode=true&characterEncoding=UTF-8) for user 'root': Access denied for user 'root'@'10.4.214.53' (using password: YES)

2Ô∏è‚É£ Log Analysis
The application failed to successfully connect to the database using the provided username and password. The error message shows that the database denied access for username 'root'.

3Ô∏è‚É£ Troubleshooting Process
Checking database configuration, first verified the application's database connection configuration, confirming that the database address, port, username, and password were all correct.
Using the command "kubectl get secret db-credentials -o yaml" to view the Secret content, found that the username and password fields in the Secret store Base64 encoded strings, not direct plaintext, which is expected behavior.
Decoding Secret content revealed that the username and password values were indeed correct plaintext strings.
After further detailed comparison, the problem was found to be in the Secret creation method. The operator used the first method mentioned above to create the Secret. The username.txt and password.txt text files were generated using the vim text editor, and after inputting content and saving directly, vim by default adds a newline character (\n) at the end of the file. Therefore, the actual file content became (root\n) and (password\n) respectively. Base64 encoding treats the newline character as a valid character, ultimately causing the Secret values to contain extra newline characters.

4Ô∏è‚É£ Solution:
Directly use the following command to generate files without newline characters: echo -n "password" > password.txt, regenerate the secret
Use the second approach mentioned in figure 3 to recreate the secret, directly specifying content in the command
Restart the application, problem resolved

Sep 23, 2025
----- Chinese
base64 ÁºñÁ†ÅÁÆÄ‰ªã
Â§ßÂÆ∂Â•ΩÔºå‰ªäÂ§©Êàë‰ª¨Êù•ËÅäËÅä‰∏äÁØáÊèêÂà∞ÁöÑbase64ÁºñÁ†ÅÊòØ‰∏™Âï•‰∏ú‰∏úÔºå‰ª•ÂèäÂÆÉÊúâÂï•Áî®Â§Ñ„ÄÇ
 Base64 ÊòØ‰∏ÄÁßçÂ∏∏ËßÅÁöÑÁºñÁ†ÅÊñπÊ≥ïÔºåÂÆÉÁöÑ‰∏ªË¶Å‰ΩúÁî®ÊòØÂ∞Ü‰∫åËøõÂà∂Êï∞ÊçÆÔºàÂ¶ÇÂõæÁâá„ÄÅÊñá‰ª∂„ÄÅÁâπÊÆäÂ≠óÁ¨¶Á≠âÔºâËΩ¨Êç¢ÊàêÊñáÊú¨Ê†ºÂºèÔºå‰ª•‰æøÂú®ÈúÄË¶Å‰º†ËæìÊàñÂ≠òÂÇ®ÁöÑÂú∫ÊôØ‰∏≠‰ΩøÁî®„ÄÇÊù•‰∫ÜËß£‰∏ãÂÆÉÁöÑ‰ΩúÁî®Âíå‰∏Ä‰∫õÂ∫îÁî®Âú∫ÊôØ 
	
1Ô∏è‚É£ ÂæÆ‰ø°ËÅäÂ§©‰∏≠ÁöÑÂõæÁâá‰º†Ëæì
ÂΩì‰Ω†Âú®ÂæÆ‰ø°ÊàñÂÖ∂‰ªñÁ§æ‰∫§ËΩØ‰ª∂‰∏≠ÂèëÈÄÅÂõæÁâáÊó∂ÔºåÂõæÁâáÂÆûÈôÖ‰∏äÊòØ‰ª•‰∫åËøõÂà∂ÂΩ¢ÂºèÂ≠òÂÇ®ÁöÑ„ÄÇÂ¶ÇÊûúÁõ¥Êé•‰º†Ëæì‰∫åËøõÂà∂Êï∞ÊçÆÔºåÂèØËÉΩ‰ºöÂõ†‰∏∫ÁΩëÁªú‰º†ËæìÈóÆÈ¢òÊàñ‰∏çÂÖºÂÆπÁöÑÂ≠óÁ¨¶ÂØºËá¥ÂõæÁâáÊçüÂùè„ÄÇ
 Á§æ‰∫§ËΩØ‰ª∂‰ºöÂ∞ÜÂõæÁâáÁºñÁ†ÅÊàê Base64 Ê†ºÂºèÔºåÂ∞ÜÂõæÁâáÂÜÖÂÆπËΩ¨Êç¢Êàê‰∏ÄÊÆµÊñáÊú¨ÔºàÂ¶Ç iVBORw0KGgoAAAANSUhEUg...Ôºâ„ÄÇËøôÊÆµÊñáÊú¨Êõ¥ÈÄÇÂêàÂú®ÁΩëÁªú‰∏ä‰º†ËæìÔºåÁ°Æ‰øùÂõæÁâáÂèØ‰ª•Ë¢´Êé•Êî∂ÊñπÊ≠£Á°ÆËß£Á†ÅÂπ∂Êü•Áúã„ÄÇ
2Ô∏è‚É£ ÂèëÁîµÂ≠êÈÇÆ‰ª∂Êó∂ÁöÑÈôÑ‰ª∂
Âú®ÂèëÁîµÂ≠êÈÇÆ‰ª∂Êó∂Ê∑ªÂä†ÁöÑÂõæÁâá„ÄÅPDF„ÄÅWord ÊñáÊ°£Á≠âÈôÑ‰ª∂ÔºåÂÆûÈôÖ‰∏äÊòØÊñá‰ª∂ÁöÑ‰∫åËøõÂà∂Êï∞ÊçÆ„ÄÇÂ¶ÇÊûúÁõ¥Êé•Â∞ÜËøô‰∫õÊï∞ÊçÆÂµåÂÖ•ÈÇÆ‰ª∂Ê≠£ÊñáÔºåÂèØËÉΩ‰ºöÂõ†‰∏∫ÁâπÊÆäÂ≠óÁ¨¶ÔºàÂ¶Ç \0„ÄÅ\n Á≠âÔºâÂΩ±ÂìçÈÇÆ‰ª∂ÁöÑÊ†ºÂºè„ÄÇ
 ÈÇÆ‰ª∂ÊúçÂä°‰ºöÂ∞ÜËøô‰∫õÊñá‰ª∂ÈÄöËøá Base64 ÁºñÁ†ÅÔºåËΩ¨Êç¢ÊàêÂèØËØªÁöÑÊñáÊú¨Ê†ºÂºèÔºåÂµåÂÖ•Âà∞ÈÇÆ‰ª∂ÂÜÖÂÆπ‰∏≠„ÄÇ‰æãÂ¶ÇÔºåÂõæÁâáË¢´ÁºñÁ†ÅÊàê‰∏ÄÊÆµ Base64 ÊñáÊú¨ÔºåÊúÄÁªàÊé•Êî∂ÊñπÂèØ‰ª•Ëß£Á†ÅËøòÂéüÊàêÂõæÁâá„ÄÇÂÆÉËÉΩÁ°Æ‰øùÈôÑ‰ª∂‰∏çË¢´ÈîôËØØËß£ÊûêÔºåÈÅøÂÖçÈÇÆ‰ª∂ÂÜÖÂÆπË¢´Á†¥ÂùèÔºåÂêåÊó∂ÂÖºÂÆπ‰∏çÂêåÁöÑÈÇÆ‰ª∂ÂÆ¢Êà∑Á´Ø„ÄÇ
3Ô∏è‚É£ ÁîüÊàê‰∫åÁª¥Á†Å
Âú®ÂïÜÂú∫Êâ´Á†Å‰ªòÊ¨æÊó∂ÔºåÊâãÊú∫‰ºöÁîüÊàê‰∏Ä‰∏™‰∫åÁª¥Á†Å„ÄÇËøô‰∫õ‰∫åÁª¥Á†ÅÂÖ∂ÂÆûÊòØÂ≠óÁ¨¶‰∏≤ÁöÑÁºñÁ†ÅÂΩ¢ÂºèÔºåÂèØËÉΩÂåÖÂê´‰Ω†ÁöÑ‰ªòÊ¨æ‰ø°ÊÅØ„ÄÅÂïÜÊà∑‰ø°ÊÅØÁ≠â„ÄÇ
 ‰∫åÁª¥Á†Å‰∏≠ÂåÖÂê´ÁöÑÂ≠óÁ¨¶‰∏≤ÂèØËÉΩ‰ºöÁî® Base64 ËøõË°åÁºñÁ†ÅÔºåÊØîÂ¶Ç‰Ω†ÁöÑ‰ªòÊ¨æ‰ø°ÊÅØË¢´ËΩ¨Êç¢Êàê‰∏ÄÊÆµ Base64 ÊñáÊú¨ÔºåÂç≥Â∞ÜÊï∞ÊçÆÂéãÁº©Êàê‰∏ÄÁßçÊ†áÂáÜÊ†ºÂºèÔºåÂÜçÂµåÂÖ•‰∫åÁª¥Á†Å‰∏≠„ÄÇ‰æø‰∫éÂø´ÈÄüËØªÂèñ„ÄÅÂêåÊó∂ÈÅøÂÖç‰∏≠ÊñáÊàñÁâπÊÆäÂ≠óÁ¨¶ÂØºËá¥ÁöÑÊï∞ÊçÆÈîôËØØ„ÄÇ
4Ô∏è‚É£ ‰∏ä‰º†Â§¥ÂÉèÂà∞Á§æ‰∫§Â™í‰Ωì
Êúâ‰∫õÂπ≥Âè∞‰ºöÂ∞ÜÂõæÁâáÁºñÁ†ÅÊàê Base64 Ê†ºÂºèÔºåÁÑ∂ÂêéÈÄöËøáÁΩëÁªú‰º†ËæìÂà∞ÊúçÂä°Âô®„ÄÇËøôÁßçÊñπÂºèÂèØ‰ª•ÈÅøÂÖçÁΩëÁªú‰∏çÁ®≥ÂÆöÊó∂Êñá‰ª∂ÊçüÂùè„ÄÇ
Â¶ÇÊûúÊää‰Ω†ÁöÑÊñá‰ª∂„ÄÅÂõæÁâá„ÄÅÂØÜÁ†ÅÊÉ≥Ë±°Êàê‚Äú‰∫åËøõÂà∂‰ª£Á†Å‚ÄùÔºåÂÆÉ‰ª¨ÂèØËÉΩÂåÖÂê´‚ÄúÁúã‰∏çÊáÇÁöÑÂ≠óÁ¨¶‚ÄùÊàñ‚ÄúÁâπÊÆäÁ¨¶Âè∑‚Äù„ÄÇËøô‰∫õÂ≠óÁ¨¶Âú®‰º†ËæìÊàñËÄÖ‰øùÂ≠òÊó∂ÊúâÊó∂‰ºöÂØºËá¥ÈóÆÈ¢ò„ÄÇ
Base64 Â∞±ÂÉè‰∏Ä‰∏™‚ÄúÁøªËØëÂô®‚ÄùÔºåÊääËøô‰∫õ‚ÄúÁúã‰∏çÊáÇÁöÑÂ≠óÁ¨¶‚ÄùÁøªËØëÊàêÁî±Â≠óÊØç„ÄÅÊï∞Â≠óÁªÑÊàêÁöÑ‚Äú‰∫∫Á±ªÂèãÂ•ΩÂûãÂ≠óÁ¨¶‚Äù„ÄÇËøôËÆ©ÂÆÉ‰ª¨Êõ¥ÂÆπÊòìÂú®ÁΩëÁªú‰∏≠‰º†Ëæì„ÄÅÂú®Á≥ªÁªü‰∏≠Â≠òÂÇ®Ôºå‰∏î‰∏ç‰ºöÊçüÂùèÊàñ‰∏¢Â§±„ÄÇ
ËôΩÁÑ∂Êàë‰ª¨ÂèØËÉΩÂØüËßâ‰∏çÂà∞ Base64 ÁöÑÂ≠òÂú®Ôºå‰ΩÜÂÆÉËÉåÂêéÈªòÈªòÂú∞ËÆ©Êï∞ÊçÆ‰º†ËæìÊõ¥Âä†Á®≥ÂÆöÂíåÂÆâÂÖ®„ÄÇ

----- English

Introduction to Base64 Encoding

Hello everyone, today let's talk about what Base64 encoding is, which we mentioned in the previous article, and what it's used for.

Base64 is a common encoding method whose main purpose is to convert binary data (such as images, files, special characters, etc.) into text format for use in scenarios that require transmission or storage. Let's understand its functions and some application scenarios.

1Ô∏è‚É£ Image Transmission in WeChat Chat
When you send images in WeChat or other social software, the images are actually stored in binary format. If binary data is transmitted directly, the images might get corrupted due to network transmission issues or incompatible characters.
Social software will encode images into Base64 format, converting the image content into a text string (like iVBORw0KGgoAAAANSUhEUg...). This text string is more suitable for network transmission, ensuring that images can be correctly decoded and viewed by the recipient.

2Ô∏è‚É£ Email Attachments
When adding attachments like images, PDFs, Word documents to emails, these are actually binary data of files. If this data is directly embedded in the email body, it might affect the email format due to special characters (like \0, \n, etc.).
Email services will encode these files through Base64, converting them into readable text format and embedding them in the email content. For example, images are encoded into Base64 text, and ultimately the recipient can decode and restore them back to images. This ensures attachments are not incorrectly parsed, prevents email content corruption, and maintains compatibility with different email clients.

3Ô∏è‚É£ QR Code Generation
When making payments by scanning codes in shopping malls, your phone generates a QR code. These QR codes are actually encoded forms of strings that may contain your payment information, merchant information, etc.
The strings contained in QR codes may use Base64 encoding, for example, your payment information is converted into Base64 text, compressing the data into a standard format before embedding it in the QR code. This facilitates quick reading while avoiding data errors caused by Chinese characters or special characters.

4Ô∏è‚É£ Uploading Avatars to Social Media
Some platforms encode images into Base64 format and then transmit them to servers over the network. This approach can prevent file corruption when the network is unstable.

If you imagine your files, images, and passwords as "binary code," they may contain "incomprehensible characters" or "special symbols." These characters sometimes cause problems during transmission or storage.
Base64 acts like a "translator," translating these "incomprehensible characters" into "human-friendly characters" composed of letters and numbers. This makes them easier to transmit over networks and store in systems without damage or loss.
Although we may not notice the existence of Base64, it silently makes data transmission more stable and secure behind the scenes.

Sep 28,2025
----- Chinese
Êé¢Á¥¢SREÔºöÂà´ËÆ©‰Ω†ÁöÑÂ∫îÁî®Êó•ÂøóÂèòÊàê‚ÄúËØùÁó®‚Äù
Â§ßÂÆ∂Â•ΩÔºå‰ªäÂ§©ÊàëÊù•ÂàÜ‰∫´‰∏Ä‰∏™Áîü‰∫ßÁéØÂ¢ÉÈÅáÂà∞ÁöÑÈóÆÈ¢òÔºöÂ∫îÁî®Êó•Âøó DEBUG/TRACE ËÆæÁΩÆÁöÑÂ®ÅÂäõ
1Ô∏è‚É£ ÈóÆÈ¢òÊèèËø∞ÔºöÊüêÊó•ÔºåÁõëÊéßÁ≥ªÁªüÊòæÁ§∫Âá†Âè∞ËôöÊãüÊú∫ÂºÇÂ∏∏Ôºö
 CPUÂç†Áî®ËøáÈ´ò
 ÂÜÖÂ≠ò‰ΩøÁî®ÁéáËøáÈ´ò
 ÁΩëÁªúÂ∏¶ÂÆΩÂç†Áî®ËøáÈ´ò Ôºà> 30 m/sÔºâ
	
2Ô∏è‚É£ Ê£ÄÊü•ËøáÁ®ãÔºöÁªèËøáÊ£ÄÊü•ÔºåÂèëÁé∞Âá†Âè∞ÂºÇÂ∏∏ÁöÑËôöÊãüÊú∫ÈÉΩÈÉ®ÁΩ≤‰∫ÜÂêå‰∏Ä‰∏™javaÂ∫îÁî®ÔºåÂπ∂‰∫éÂêå‰∏ÄÊó∂Èó¥ËøõË°å‰∫ÜÊï¥‰ΩìÁâàÊú¨Êõ¥Êñ∞„ÄÇ
Ê£ÄÊü•grafanaÁöÑÂìçÂ∫îÊåáÊ†áÂõæÔºåÂèëÁé∞javaÂ∫îÁî®Êú¨Ë∫´ÁöÑjvmÊåáÊ†áÔºåGCÁä∂ÊÄÅÁõ∏ÂØπÊ≠£Â∏∏ÔºåÁï•ÂæÆÂÅèÈ´ò
Êó•ÂøóÁ≥ªÁªüÊ£ÄÊü•ÔºåÂèëÁé∞Â∫îÁî®Âú®ÁâàÊú¨Êõ¥Êñ∞‰ª•ÂêéÔºåÂºÄÂêØ‰∫ÜDEBUGÁöÑÊó•ÂøóËÆæÂÆö„ÄÇ
	
3Ô∏è‚É£ ÈóÆÈ¢ò‰øÆÂ§çÔºöÊääÂ∫îÁî®ÁöÑÊó•ÂøóËÆæÂÆöÊîπ‰∏∫INFO‰ª•ÂêéÔºåËôöÊãüÊú∫Áä∂ÊÄÅÊÅ¢Â§çÊ≠£Â∏∏
	
4Ô∏è‚É£ ÂÖ≥‰∫éÂ∫îÁî®Êó•ÂøóÔºöÂØπ‰∫éÈÉ®ÁΩ≤Âú®Áîü‰∫ßÁéØÂ¢É‰∏≠ÁöÑÊúçÂä°ÔºåÊó•ÂøóÈÇ£ÂèØÊòØÊéíÊü•ÈóÆÈ¢òÁöÑ‚ÄúÁ¶èÂ∞îÊë©ÊñØ‚ÄùÔºåÊòØ‰∫ÜËß£Â∫îÁî®‚ÄúÂÜÖÂøÉÊ¥ªÂä®‚ÄùÁöÑÁ™óÂè£„ÄÇÂ¶ÇÊûú‰Ω†ÁöÑÂ∫îÁî®ÊòØ‰∏™Ê≠£Âú®Êê¨Á†ñÁöÑÁ†ÅÂÜúÔºåÊó•ÂøóÁ∫ßÂà´Â∞±ÂÉèÊòØÁ†ÅÂÜúÂêë‰∏äÁ∫ßÊ±áÊä•ÁöÑÂ∑•‰ΩúËøõÂ∫¶ÔºåËØ∑ÂèÇËÄÉÂõæ2„ÄÇ
‰ªéÂõæ‰∏≠ÂèØ‰ª•ÁúãÂá∫ÔºåÊó•ÂøóÁ∫ßÂà´‰ªé‰∏äÂæÄ‰∏ãÔºå‰ø°ÊÅØÈáèÊòØÈÄíÂ¢ûÁöÑ„ÄÇÂ∞§ÂÖ∂ÊòØDEBUG Âíå TRACEÔºåÂÆÉ‰ª¨ÁÆÄÁõ¥Â∞±ÊòØÊó•ÂøóÁïåÁöÑ‚ÄúËØùÁó®‚ÄùÔºåÂè™Ë¶ÅËÉΩËØ¥ÁöÑ‰∏ÄËÇ°ËÑëÁöÑÈÉΩË¶ÅË∑ü‰Ω†ËØ¥ÔºÅ
	
5Ô∏è‚É£ ÈÇ£‰πàÂºÄÂêØ‰∫ÜDEBUG/TRACEÁöÑÊó•ÂøóËæìÂá∫ÔºåÂà∞Â∫ïÊòØÂ•ΩÊòØÂùèÔºüËØ∑ÂèÇËÄÉÂõæ3
ÈÄöÂ∏∏ÔºåDEBUG Á∫ßÂà´ÁöÑÊó•Âøó‰∏ªË¶ÅÁî®‰∫éÂºÄÂèëÂíåÊµãËØïÁéØÂ¢É„ÄÇÂú®Áîü‰∫ßÁéØÂ¢É‰∏≠Ôºå‰∏∫‰∫Ü‰øùËØÅÂ∫îÁî®ÊÄßËÉΩ„ÄÅÁ®≥ÂÆöÊÄßÂíåÂÆâÂÖ®ÊÄßÔºåÂ∫îÈÅøÂÖçÈïøÊúüÂºÄÂêØ DEBUG Á∫ßÂà´Êó•Âøó„ÄÇÂ¶ÇÊûúÈúÄË¶ÅÂú®Áîü‰∫ßÁéØÂ¢ÉÊéíÊü•ÁâπÂÆöÈóÆÈ¢òÔºåÂèØ‰ª•ËÄÉËôë‰∏¥Êó∂„ÄÅÊúâËåÉÂõ¥Âú∞ÂºÄÂêØ DEBUG Êó•ÂøóÔºåÂπ∂Âú®ÈóÆÈ¢òËß£ÂÜ≥ÂêéÂèäÊó∂ÂÖ≥Èó≠ÊàñË∞ÉÂõûÊõ¥È´òÁ∫ßÂà´ÔºàÂ¶Ç INFO Êàñ ERRORÔºâ„ÄÇÂêåÊó∂ÔºåÁõëÊéßÁ≥ªÁªü‰πüÈúÄË¶ÅÂØπDEBUG/TRACEÁöÑÊó•ÂøóËÆæÁΩÆ‰ª•ÂèäËæìÂá∫Áä∂ÊÄÅÊúâ‰∏Ä‰∏™ÂÆûÊó∂ÁöÑÁõëÊéßÔºåÂèØ‰ª•Âú®ÈóÆÈ¢òÂá∫Áé∞ÁöÑÁ¨¨‰∏ÄÊó∂Èó¥‰ªãÂÖ•ËøõË°åÊ£ÄÊü•„ÄÅÁ°ÆËÆ§ÔºåÂøÖË¶ÅÊó∂ËøõË°å‰øÆÊîπ„ÄÇ

----- English

Exploring SRE: Don't Let Your Application Logs Become "Chatterboxes"

Hello everyone, today I'm sharing a production environment issue I encountered: the power of application log DEBUG/TRACE settings.

1Ô∏è‚É£ Problem Description: One day, the monitoring system showed several virtual machines with anomalies:
- High CPU usage
- High memory usage
- High network bandwidth usage (> 30 MB/s)

2Ô∏è‚É£ Investigation Process: After investigation, we found that all the abnormal virtual machines had deployed the same Java application and had undergone a complete version update at the same time.
Checking Grafana's response metric charts, we found that the Java application's own JVM metrics and GC status were relatively normal, just slightly elevated.
Log system inspection revealed that after the version update, the application had enabled DEBUG log settings.

3Ô∏è‚É£ Problem Resolution: After changing the application's log setting to INFO, the virtual machine status returned to normal.

4Ô∏è‚É£ About Application Logs: For services deployed in production environments, logs are the "Sherlock Holmes" for troubleshooting issues - they're the window into understanding the application's "inner thoughts." If your application is a developer working hard, log levels are like the developer reporting work progress to supervisors, please refer to Figure 2.
From the chart, you can see that log levels increase in information volume from top to bottom. Especially DEBUG and TRACE - they're simply the "chatterboxes" of the logging world, telling you everything they possibly can!

5Ô∏è‚É£ So is enabling DEBUG/TRACE log output good or bad? Please refer to Figure 3.
Typically, DEBUG level logs are mainly used in development and testing environments. In production environments, to ensure application performance, stability, and security, long-term enabling of DEBUG level logs should be avoided. If you need to troubleshoot specific issues in production, consider temporarily and selectively enabling DEBUG logs, and promptly disable them or revert to higher levels (such as INFO or ERROR) after resolving the problem. Additionally, monitoring systems need real-time monitoring of DEBUG/TRACE log settings and output status, allowing for immediate intervention, inspection, confirmation, and necessary modifications when issues arise.

