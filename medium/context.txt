Sep 7, 2025
----- Chinese
æ¢ç´¢SREï¼šç«™åœ¨å¯é æ€§çš„æœ€å‰çº¿
å¤§å®¶å¥½ï¼Œæˆ‘æ˜¯ä¸€åSREï¼Œä¹Ÿå°±æ˜¯â€œç½‘ç«™å¯é æ€§å·¥ç¨‹å¸ˆâ€ï¼ˆSite Reliability Engineerï¼‰ã€‚è¯´ç™½äº†ï¼Œæˆ‘çš„å·¥ä½œå°±æ˜¯è®©å„ç§ç½‘ç«™ã€Appå’ŒæœåŠ¡ä¸å‡ºé—®é¢˜ï¼Œèƒ½ç¨³å®šã€é«˜æ•ˆåœ°è¿è¡Œã€‚æˆ‘èº«åæ²¡æœ‰æŠ«é£ï¼Œä½†æˆ‘è§‰å¾—è‡ªå·±æ˜¯æŸç§æ„ä¹‰ä¸Šçš„â€œå¹•åè‹±é›„â€ï¼Œå› ä¸ºæˆ‘çš„ä»»åŠ¡å°±æ˜¯æ‹¯æ•‘ç³»ç»Ÿå´©æºƒï¼Œä¿éšœç”¨æˆ·ä½“éªŒã€‚é‚£ä¹ˆï¼Œæˆ‘æ¯å¤©åˆ°åº•åœ¨åšä»€ä¹ˆå‘¢ï¼Ÿè·Ÿæˆ‘ä¸€èµ·çœ‹çœ‹å§ï¼
	
å¯é æ€§ï¼šSREçš„æ ¸å¿ƒä½¿å‘½
ä½œä¸ºSREï¼Œæˆ‘æœ€é‡è¦çš„èŒè´£å°±æ˜¯è®©ç³»ç»Ÿâ€œé è°±â€ã€‚æ¯”å¦‚ï¼Œå½“ä½ æ·±å¤œç‚¹å¤–å–ã€ç”¨Appä¹°ç”µå½±ç¥¨æˆ–è€…åˆ·çŸ­è§†é¢‘æ—¶ï¼Œè¿™äº›æœåŠ¡éƒ½å¾—æ­£å¸¸è¿è¡Œï¼Œä¸ç„¶ä½ å¯èƒ½ä¼šæŠ“ç‹‚ï¼Œå¯¹å§ï¼Ÿä¸ºäº†åšåˆ°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä¼šè®¾å®šä¸€äº›ç›®æ ‡ï¼Œæ¯”å¦‚â€œç³»ç»Ÿ99.9%çš„æ—¶é—´éƒ½è¦åœ¨çº¿â€ï¼Œç„¶åé€šè¿‡ä¼˜åŒ–æ¶æ„ã€ä¿®å¤æ¼æ´ï¼Œæ¥è®©ç³»ç»Ÿå˜å¾—ç¨³å®šåˆåšå¼ºã€‚
	
ç›‘æ§ï¼šSREçš„åƒé‡Œçœ¼
æˆ‘çš„æ—¥å¸¸ç¦»ä¸å¼€â€œç›‘æ§â€ï¼ˆMonitoringï¼‰ã€‚æˆ‘ä¼šç”¨ä¸€äº›é…·ç‚«çš„å·¥å…·ï¼Œæ¯”å¦‚Prometheuså’ŒGrafanaï¼Œè¿˜æœ‰å…¶ä»–ä¸€ç³»åˆ—çš„å·¥å…·æ¥ç›¯ç€ç³»ç»Ÿçš„è¿è¡ŒçŠ¶æ€ã€‚æ¯”å¦‚ï¼Œæµé‡æ˜¯ä¸æ˜¯çªç„¶é£™å‡äº†ï¼ŸæœåŠ¡å™¨æ˜¯ä¸æ˜¯å¼€å§‹å‘çƒ­äº†ï¼Ÿä¸€æ—¦å‘ç°å¼‚å¸¸ï¼Œæˆ‘å°±èƒ½ç¬¬ä¸€æ—¶é—´å‡ºæ‰‹ï¼Œé˜²æ­¢é—®é¢˜æ‰©å¤§ã€‚å¯ä»¥è¯´ï¼Œç›‘æ§å·¥å…·æ˜¯æˆ‘çš„çœ¼ç›ï¼Œå¸®æˆ‘éšæ—¶æŒæ¡ç³»ç»Ÿçš„å¥åº·çŠ¶æ€ã€‚
	
è‡ªåŠ¨åŒ–ï¼šè§£æ”¾åŒæ‰‹çš„ç§˜è¯€
è¯´å®è¯ï¼Œé‡å¤å¹²åŒæ ·çš„äº‹æƒ…çœŸçš„å¾ˆæ— èŠã€‚æ‰€ä»¥ï¼ŒSREçš„åº§å³é“­ä¹‹ä¸€å°±æ˜¯â€œèƒ½è‡ªåŠ¨åŒ–çš„ç»ä¸æ‰‹åŠ¨â€ã€‚æˆ‘çš„å·¥ä½œä¹‹ä¸€å°±æ˜¯å†™è„šæœ¬å’Œç¨‹åºï¼Œè®©ç³»ç»Ÿè‡ªåŠ¨å¤„ç†ä¸€äº›å¸¸è§é—®é¢˜ï¼Œæ¯”å¦‚è‡ªåŠ¨æ‰©å±•æœåŠ¡å™¨å®¹é‡ï¼Œè‡ªåŠ¨é‡å¯æœåŠ¡ï¼Œç”šè‡³è‡ªåŠ¨é¢„è­¦ã€‚æœ‰äº†è‡ªåŠ¨åŒ–ï¼Œæˆ‘å¯ä»¥æŠŠæ—¶é—´èŠ±åœ¨æ›´é‡è¦ã€æ›´æœ‰æŒ‘æˆ˜çš„äº‹æƒ…ä¸Šã€‚
	
å¯æ‰©å±•æ€§ï¼šä»å®¹åº”å¯¹é«˜å³°
æˆ‘ç»å¸¸è·Ÿæœ‹å‹å¼€ç©ç¬‘è¯´ï¼Œæˆ‘çš„å·¥ä½œå°±æ˜¯è®©â€œåŒåä¸€â€ä¸å®•æœºã€‚æ¯æ¬¡å¤§ä¿ƒã€ç›´æ’­çˆ†ç«æ—¶ï¼Œæµé‡éƒ½ä¼šæš´æ¶¨ï¼Œè¿™å¯¹ç³»ç»Ÿæ˜¯ä¸ªå¤§è€ƒéªŒã€‚è€Œæˆ‘çš„ä»»åŠ¡å°±æ˜¯æå‰è®¾è®¡å¥½ç³»ç»Ÿï¼Œè®©å®ƒèƒ½çµæ´»æ‰©å±•ï¼Œæ‰›ä½å‹åŠ›ã€‚çœ‹åˆ°ç”¨æˆ·ä¹°ä¹°ä¹°ã€åˆ·åˆ·åˆ·æ—¶ï¼Œæˆ‘éƒ½è§‰å¾—ç‰¹åˆ«éª„å‚²ï¼Œå› ä¸ºè¿™èƒŒåæœ‰æˆ‘çš„åŠªåŠ›ã€‚
	
äº‹ä»¶ç®¡ç†ï¼šå±æœºä¸­çš„å†·é™å¤§è„‘
å½“ç„¶ï¼Œå¤©æœ‰ä¸æµ‹é£äº‘ï¼Œå†å®Œç¾çš„ç³»ç»Ÿä¹Ÿæœ‰å¯èƒ½çªç„¶å´©äº†ã€‚è¿™ä¸ªæ—¶å€™ï¼Œæˆ‘å°±å˜æˆäº†â€œå±æœºå¤„ç†ä¸“å®¶â€ã€‚å½“è­¦æŠ¥å“èµ·ï¼Œæˆ‘ä¼šè¿…é€Ÿå®šä½é—®é¢˜ï¼Œæƒ³åŠæ³•æ¢å¤æœåŠ¡ï¼ŒåŒæ—¶ç»„ç»‡å›¢é˜Ÿæ€»ç»“å¤ç›˜ï¼Œé¿å…åŒæ ·çš„é”™è¯¯å†æ¬¡å‘ç”Ÿã€‚è™½ç„¶ç´§å¼ ï¼Œä½†æ¯æ¬¡æˆåŠŸè§£å†³é—®é¢˜åï¼Œé‚£ç§æˆå°±æ„ŸçœŸçš„è¶…çˆ½ã€‚
	
è¯´çœŸçš„ï¼ŒSREè¿™ä¸ªèŒä¸šå¯¹æˆ‘æ¥è¯´ä¸åªæ˜¯å·¥ä½œï¼Œæ›´æ˜¯ä¸€ç§æŒ‘æˆ˜å’Œä¹è¶£ã€‚æˆ‘æ—¢èƒ½åŠ¨æ‰‹å†™ä»£ç ï¼Œåˆèƒ½è®¾è®¡ç³»ç»Ÿï¼Œè¿˜èƒ½å¿«é€Ÿè§£å†³é—®é¢˜ï¼Œæ¯å¤©éƒ½å……æ»¡æ–°é²œæ„Ÿã€‚
æ¥ä¸‹æ¥ï¼Œæˆ‘ä¼šæ€»ç»“å¹¶åˆ†äº«SREæ—¥å¸¸å·¥ä½œå†…å®¹ï¼ŒåŒæ—¶æ¬¢è¿å¤§å®¶å’Œæˆ‘ä¸€èµ·äº’åŠ¨ï¼Œäº’ç›¸äº¤æµ ï¼šï¼‰

----- English 
Exploring SRE: Standing at the Forefront of Reliability
Hi everyone, I'm an SRE, which stands for "Site Reliability Engineer." Simply put, my job is to ensure that various websites, apps, and services don't break down and can run stably and efficiently. I may not wear a cape, but I consider myself a "behind-the-scenes hero" in some sense, because my mission is to rescue system crashes and ensure user experience. So, what exactly do I do every day? Let's take a look together!

Reliability: The Core Mission of SRE
As an SRE, my most important responsibility is to make systems "reliable." For example, when you order takeout late at night, buy movie tickets through an app, or scroll through short videos, these services must run normally, otherwise you might go crazy, right? To achieve this, I set some goals, like "the system must be online 99.9% of the time," and then optimize architecture and fix vulnerabilities to make the system stable and robust.

Monitoring: SRE's All-Seeing Eyes
My daily work is inseparable from "monitoring." I use some cool tools like Prometheus and Grafana, along with other series of tools to keep an eye on the system's operational status. For instance, is traffic suddenly spiking? Are the servers starting to overheat? Once I detect anomalies, I can take action immediately to prevent problems from escalating. You could say that monitoring tools are my eyes, helping me stay on top of the system's health status at all times.

Automation: The Secret to Freeing Your Hands
Honestly, doing the same repetitive tasks is really boring. So, one of SRE's mottos is "automate everything that can be automated, never do it manually." Part of my job is writing scripts and programs to let the system automatically handle common issues, such as automatically scaling server capacity, automatically restarting services, and even automatic alerts. With automation, I can spend time on more important and challenging tasks.

Scalability: Handling Peak Traffic with Ease
I often joke with friends that my job is to prevent systems during events like Black Friday from crashing. Every time there are major promotions or live streams go viral, traffic surges dramatically, which is a major test for the system. My task is to design the system in advance so it can scale flexibly and withstand the pressure. When I see users shopping and browsing frantically, I feel particularly proud because my efforts are behind all of this.

Incident Management: The Calm Mind in Crisis
Of course, unexpected things happen, and even the most perfect systems can suddenly crash. At times like these, I become a "crisis management expert." When alarms go off, I quickly locate problems, find ways to restore services, while organizing the team to summarize and review to prevent the same mistakes from happening again. Although it's stressful, the sense of achievement after successfully solving problems is absolutely amazing.

Honestly, the SRE profession is not just a job for me, but also a challenge and source of enjoyment. I can both write code and design systems, as well as solve problems quickly. Every day is full of fresh experiences.

Next, I will summarize and share the daily work content of SRE, and I welcome everyone to interact and exchange ideas with me :)

Sep 9,2025
----- Chinese
æ¢ç´¢SREï¼šK8Sé›†ç¾¤PODè°ƒåº¦é—®é¢˜æè¿°
å¤§å®¶å¥½ï¼Œä»Šå¤©åˆ†äº«ä¸€ä¸ªæ—¥å¸¸å·¥ä½œä¸­é‡åˆ°çš„äº§çº¿é—®é¢˜ï¼Œæ¬¢è¿å¤§å®¶ä¸€èµ·å¤´è„‘é£æš´ã€‚
æˆ‘ä»¬ä½¿ç”¨ Kubernetes é›†ç¾¤éƒ¨ç½²äº†ä¸€ç³»åˆ—æœåŠ¡ï¼Œå…¶ä¸­ä¸€ä¸ªæ•°æ®æ”¶é›†appï¼Œç”¨äºæ‰«æçº¦ 8,500 ä¸ªå¯¹è±¡å¹¶æ”¶é›†ç›¸åº”æ•°æ®ï¼Œé…ç½®å¦‚ä¸‹ï¼š
K8S é›†ç¾¤ï¼š3 ä¸ªä¸»èŠ‚ç‚¹ï¼Œ4 ä¸ªå·¥ä½œèŠ‚ç‚¹ã€‚
æ•°æ®æ”¶é›†app:éƒ¨ç½²äº† 6 ä¸ª Pod (replicas=6)ï¼Œä»¥æé«˜ä»»åŠ¡çš„å¹¶å‘å¤„ç†èƒ½åŠ›ã€‚
appè¿è¡Œæ—¶ä¸ºæ¯ä¸ªæ‰«æå¯¹è±¡åˆ›å»ºçº¿ç¨‹ï¼Œæ¯ä¸ªçº¿ç¨‹è´Ÿè´£å¤„ç†ä¸€éƒ¨åˆ†æ‰«æä»»åŠ¡ã€‚
appé€šè¿‡ K8S çš„ Service ç«¯å£ä»¥ 1 åˆ†é’Ÿçš„é—´éš”è¢«æŒç»­è°ƒç”¨ã€‚
Service é€šè¿‡è´Ÿè½½å‡è¡¡å°†è¯·æ±‚åˆ†é…åˆ°å…·ä½“çš„ Pod ï¼Œå¹¶æ‰§è¡Œæ‰«æä»»åŠ¡ã€‚
ï»¿#SREï»¿
é—®é¢˜æè¿°
æœåŠ¡é€šè¿‡CI/CDéƒ¨ç½²åï¼Œæ‰€æœ‰PODå‰¯æœ¬éƒ½è¢«è°ƒåº¦åˆ°äº†å·¥ä½œèŠ‚ç‚¹ 2(å¦‚å›¾)ï¼Œè€Œæ²¡æœ‰å‡åŒ€åˆ†å¸ƒåˆ°å…¶ä»–å·¥ä½œèŠ‚ç‚¹ã€‚è¿™ç§è°ƒåº¦ä¸å‡è¡¡å¯¼è‡´å¦‚ä¸‹é—®é¢˜ï¼š
æ¯ä¸ª Pod åˆ›å»ºå¤§é‡çº¿ç¨‹å¤„ç†æ‰«æä»»åŠ¡ã€‚å½“ 6 ä¸ª Pod å…¨éƒ¨é›†ä¸­åœ¨å·¥ä½œèŠ‚ç‚¹ 2 ä¸Šè¿è¡Œæ—¶ï¼Œçº¿ç¨‹æ•°é‡å¿«é€Ÿå¢é•¿ï¼Œæœ€ç»ˆè€—å°½äº†èŠ‚ç‚¹çš„çº¿ç¨‹èµ„æºã€‚
å½“èŠ‚ç‚¹2éƒ¨ç½²çš„ä»»æ„æœåŠ¡å°è¯•åˆ›å»ºæ›´å¤šçº¿ç¨‹æ—¶ï¼Œå› èŠ‚ç‚¹å·²è¾¾åˆ°çº¿ç¨‹ä¸Šé™ï¼Œå¯¼è‡´æœåŠ¡æŠ¥é”™ï¼Œæ—¥å¿—æ˜¾ç¤ºï¼šOpenBLAS blas_thread_init: pthread_create failed for thread 44 of 48: Resource temporarily unavailableã€‚è¯¥é”™è¯¯è¡¨æ˜çº¿ç¨‹åˆ›å»ºå¤±è´¥ï¼ŒåŸå› æ˜¯èŠ‚ç‚¹æ— æ³•ä¸ºæ–°çº¿ç¨‹åˆ†é…èµ„æºã€‚
å·¥ä½œèŠ‚ç‚¹ 2 å½»åº•å´©æºƒï¼Œæ— æ³•ç»§ç»­è¿è¡Œä»»åŠ¡ï¼Œå¯¹æ•´ä¸ªé›†ç¾¤çš„ç¨³å®šæ€§é€ æˆäº†å½±å“ã€‚
å‚è€ƒç‚¹
LinuxèŠ‚ç‚¹çº¿ç¨‹ä¸Šé™ä¸º 65,535ï¼ˆé»˜è®¤é…ç½®ï¼Œé€šè¿‡ /proc/sys/kernel/threads-max å’Œ ulimit -u æ§åˆ¶ï¼‰ã€‚
æ•°æ®æ”¶é›†æœåŠ¡ä¾èµ–å¤šçº¿ç¨‹æ¨¡å‹ï¼Œæ¯ä¸ªçº¿ç¨‹è´Ÿè´£ä¸€éƒ¨åˆ†æ‰«æå¯¹è±¡ã€‚ç”±äºæ‰«æå¯¹è±¡æ•°é‡è¾ƒå¤§ä¸”é—´éš”ä¸€åˆ†é’Ÿè¢«è°ƒç”¨ï¼ŒæœåŠ¡éœ€è¦åˆ›å»ºè¶³å¤Ÿå¤šçš„çº¿ç¨‹å¹¶å‘æ‰§è¡Œä»»åŠ¡ï¼Œä»¥ä¿éšœæ•°æ®æ”¶é›†çš„æ•ˆç‡ã€‚
é»˜è®¤çº¿ç¨‹ä¸Šé™ï¼ˆ65,535ï¼‰æ˜¯ä¸ºäº†æ”¯æŒé«˜å¹¶å‘éœ€æ±‚ã€‚å¦‚æœä¸Šé™è¾ƒä½ï¼ŒæœåŠ¡å¯èƒ½æ— æ³•å……åˆ†åˆ©ç”¨ CPU èµ„æºï¼Œå¯¼è‡´æ€§èƒ½ç“¶é¢ˆã€‚
é—®é¢˜æ€»ç»“
ç”±äº K8S è°ƒåº¦é—®é¢˜ï¼Œæ‰€æœ‰ Pod è¢«é›†ä¸­è°ƒåº¦åˆ°å•ä¸ªå·¥ä½œèŠ‚ç‚¹ï¼Œå¯¼è‡´çº¿ç¨‹èµ„æºè¿…é€Ÿè€—å°½ï¼Œæœ€ç»ˆå¼•å‘èŠ‚ç‚¹å´©æºƒã€‚å°½ç®¡çº¿ç¨‹ä¸Šé™ï¼ˆ65,535ï¼‰è®¾è®¡ç”¨äºæ”¯æŒé«˜å¹¶å‘éœ€æ±‚ï¼Œä½†åœ¨èµ„æºåˆ†é…ä¸å‡çš„æƒ…å†µä¸‹ï¼Œè¿™ä¸€ä¸Šé™åè€Œæˆä¸ºäº†ç“¶é¢ˆï¼Œé™åˆ¶äº†æœåŠ¡çš„æ­£å¸¸è¿è¡Œã€‚
ç”±äºå¹³å°å¯¹ç¬”è®°çš„ç¯‡å¹…é™åˆ¶ï¼Œæ— æ³•æ·»åŠ æ›´å¤šå†…å®¹ï¼Œè¯·å…³æ³¨åç»­ï¼Œå’±ä»¬ç»§ç»­èŠ

----- English
Exploring SRE: K8S Cluster POD Scheduling Issue Description
Hi everyone, today I'd like to share a production issue I encountered in my daily work. Welcome everyone to brainstorm together.
Application infrastructureï¼š
We deployed a series of services using a Kubernetes cluster, including a data collection app that scans approximately 8,500 objects and collects corresponding data. The configuration is as follows:

- K8S Cluster: 3 master nodes, 4 worker nodes
- Data collection app: 6 Pods deployed (replicas=6) to improve task concurrency processing capability
- Application runtime: The application creates a thread for each scanning object, with each thread responsible for handling a portion of the scanning tasks
- Invocation pattern: The app is invoked continuously through K8S Service port at 1-minute intervals
- Load distribution: The K8S Service uses load balancing to distribute requests to specific Pods for executing scanning tasks
Issue Description
After the service was deployed via CI/CD, all POD replicas were scheduled to worker node 2 (as shown in the diagram below), rather than being evenly distributed across other worker nodes.
This uneven scheduling caused the following problems:
1. Each Pod created a large number of threads to handle scanning tasks. When all 6 Pods were concentrated on worker node 2, the thread count rapidly increased, eventually exhausting the node's thread resources.
2. When any service deployed on node 2 attempted to create additional threads, the node had reached its thread limit, causing service failures. The logs showed:
"OpenBLAS blas_thread_init: pthread_create failed for thread 44 of 48: Resource temporarily unavailable."
This error indicated thread creation failure because the node could not allocate resources for new threads.
3. Worker node 2 completely crashed and could not continue running tasks, affecting the stability of the entire cluster.
Reference Points
- Linux node thread limit is 65,535 (default configuration, controlled by /proc/sys/kernel/threads-max and ulimit -u).
- The data collection service relied on a multi-threaded model, with each thread responsible for a portion of scanning objects. Due to the large number of scanning objects and being called at one-minute intervals, the service needed to create sufficient threads to execute tasks concurrently, ensuring data collection efficiency.
- The default thread limit (65,535) was designed to support high concurrency requirements. If the limit was too low, the service might not have been able to fully utilize CPU resources, leading to performance bottlenecks.
Issue Summary
Due to K8S scheduling issues, all Pods were concentrated on a single worker node, causing thread resources to be quickly exhausted and ultimately leading to node crash. Although the thread limit (65,535) was designed to support high concurrency requirements, in cases of uneven resource allocation, this limit became a bottleneck that restricted normal service operation.
Stay tuned for the next part where we'll discuss the root cause analysis and solution implementation!

Sep 12,2025
----- Chinese
æ¢ç´¢ SREï¼šK8S é›†ç¾¤ Pod è°ƒåº¦é—®é¢˜ä¿®å¤
å¤§å®¶å¥½ï¼Œå’±æ¥èŠèŠä¸Šç¯‡å¸–å­æåˆ°çš„é—®é¢˜æ˜¯å¦‚ä½•è§£å†³çš„ã€‚
é—®é¢˜å›é¡¾
ä¸Šç¯‡æåˆ°ç”±äº K8S è°ƒåº¦é—®é¢˜ï¼Œæ•°æ®æ”¶é›†appçš„æ‰€æœ‰ Pod è¢«é›†ä¸­è°ƒåº¦åˆ°å•ä¸ªå·¥ä½œèŠ‚ç‚¹ã€‚æ¯ä¸ª Pod ä¼šåˆ›å»ºå¤§é‡çº¿ç¨‹æ¥å¹¶å‘å¤„ç†ä»»åŠ¡ï¼Œæœ€ç»ˆå¯¼è‡´çº¿ç¨‹èµ„æºè€—å°½ï¼ˆçº¿ç¨‹ä¸Šé™ä¸º 65,535ï¼‰ï¼ŒèŠ‚ç‚¹å´©æºƒï¼ŒæœåŠ¡åœæ­¢è¿è¡Œã€‚
	
è§£å†³æ€è·¯: å¦‚ä½•è®©åº”ç”¨çš„Pod ç›¸å¯¹å‡åŒ€åˆ†å¸ƒåˆ°æ‰€æœ‰å·¥ä½œèŠ‚ç‚¹ä¸Šã€‚
	
æ–¹æ¡ˆ 1ï¼šPod åäº²å’Œæ€§
Anti-Affinityæ˜¯ä¸€ç§é€šè¿‡è°ƒåº¦çº¦æŸé¿å… Pod èšé›†åœ¨åŒä¸€èŠ‚ç‚¹çš„æ–¹æ³•ï¼Œé€‚ç”¨äºéœ€è¦æœåŠ¡é«˜å¯ç”¨æ€§æˆ–èµ„æºéš”ç¦»çš„åœºæ™¯ã€‚
é…ç½®è§„åˆ™ï¼šé€šè¿‡å®šä¹‰è°ƒåº¦çº¦æŸï¼ŒåŸºäº Pod æ ‡ç­¾è®¾ç½®åäº²å’Œæ€§è§„åˆ™ã€‚ä¾‹å¦‚ï¼šé¿å…åŒä¸€æœåŠ¡çš„å¤šä¸ª Pod å‰¯æœ¬è°ƒåº¦åˆ°åŒä¸€èŠ‚ç‚¹ã€‚
çº¦æŸç±»å‹ï¼šå¼ºåˆ¶è§„åˆ™ï¼ˆrequiredDuringSchedulingIgnoredDuringExecutionï¼‰ï¼šPod è°ƒåº¦æ—¶å¿…é¡»æ»¡è¶³åäº²å’Œæ€§è¦æ±‚ï¼Œå¦åˆ™æ— æ³•è¢«è°ƒåº¦ï¼›è½¯è§„åˆ™ï¼ˆpreferredDuringSchedulingIgnoredDuringExecutionï¼‰ï¼šè°ƒåº¦æ—¶å°½é‡æ»¡è¶³åäº²å’Œæ€§ï¼Œæ— æ³•æ»¡è¶³æ—¶å…è®¸è°ƒåº¦åˆ°ä¸ç¬¦åˆè§„åˆ™çš„èŠ‚ç‚¹ã€‚
æ‹“æ‰‘èŒƒå›´ï¼šé€šè¿‡ topologyKey å®šä¹‰åäº²å’Œæ€§ç”Ÿæ•ˆçš„èŒƒå›´ï¼Œæ¯”å¦‚æŒ‰èŠ‚ç‚¹ä¸»æœºåï¼ˆkubernetes.io/hostnameï¼‰åˆ†å¸ƒã€‚
	
æ–¹æ¡ˆ 2ï¼šPod æ‹“æ‰‘åˆ†å¸ƒçº¦æŸ
maxSkew æ˜¯ Podæ‹“æ‰‘åˆ†å¸ƒçº¦æŸçš„ä¸€éƒ¨åˆ†ï¼Œç”¨äºé™åˆ¶ Pod åœ¨ä¸åŒèŠ‚ç‚¹æˆ–åŒºåŸŸä¹‹é—´çš„åˆ†å¸ƒåå·®ã€‚ç›¸æ¯” Pod åäº²å’Œæ€§ï¼ŒmaxSkew æä¾›äº†æ›´çµæ´»çš„è°ƒåº¦ç­–ç•¥ï¼Œå…è®¸ä¸€å®šçš„åˆ†å¸ƒåå·®ã€‚
å®šä¹‰åˆ†å¸ƒåå·®ï¼šmaxSkew æŒ‡å®š Pod åœ¨ä¸åŒæ‹“æ‰‘åŸŸï¼ˆå¦‚èŠ‚ç‚¹ï¼‰ä¹‹é—´çš„æ•°é‡å·®å¼‚ã€‚ä¾‹å¦‚ï¼šmaxSkew: 1 è¡¨ç¤ºæ¯ä¸ªèŠ‚ç‚¹ä¹‹é—´çš„ Pod æ•°é‡å·®å¼‚æœ€å¤šä¸º 1ã€‚
è°ƒåº¦ç­–ç•¥ï¼šwhenUnsatisfiable: DoNotScheduleï¼šå¦‚æœæ— æ³•æ»¡è¶³åˆ†å¸ƒçº¦æŸï¼Œåˆ™ Pod ä¸ä¼šè¢«è°ƒåº¦ã€‚
è°ƒåº¦å™¨ä¼šå°½é‡ä¿è¯ Pod å‡åŒ€åˆ†å¸ƒåœ¨ä¸åŒèŠ‚ç‚¹ã€‚
	
é—®é¢˜è§£å†³
åœ¨è¯„ä¼°äº†ä¸¤ç§æ–¹æ¡ˆåï¼Œæˆ‘é€‰æ‹©äº†æ–¹æ¡ˆ 2:maxSkewï¼Œå› ä¸ºå®ƒå¯ä»¥æ›´çµæ´»åœ°æ§åˆ¶ Pod çš„åˆ†å¸ƒåå·®ï¼ŒåŒæ—¶å…è®¸ä¸€å®šç¨‹åº¦çš„è°ƒåº¦å®¹å¿æ€§ï¼Œé¿å…å› å¼ºåˆ¶è§„åˆ™å¯¼è‡´è°ƒåº¦å¤±è´¥ã€‚
åœ¨é…ç½®äº†é€‚å½“çš„ maxSkew å‚æ•°åï¼Œé€šè¿‡ CI/CD é‡æ–°éƒ¨ç½²äº†åº”ç”¨ï¼Œå…¶Pod å‰¯æœ¬æˆåŠŸåˆ†å¸ƒåˆ°å·¥ä½œèŠ‚ç‚¹ä¸Šï¼Œæ•´ä¸ªæœåŠ¡å’Œé›†ç¾¤è¿è¡Œç¨³å®šã€‚
	
åç»­æˆ‘ä»¬æ¥èŠèŠè¿™ä¸ªé—®é¢˜æ˜¯å¦‚ä½•è¢«å‘ç°çš„ï¼Œæ¬¢è¿å¤§å®¶ä¸€èµ·è®¨è®º

----- English
Exploring SRE: K8S Cluster Pod Scheduling Issue Fix

Hello everyone, let's discuss how we fixed the issue mentioned in the last story:Exploring SRE: K8S Cluster POD Scheduling Issue Description

Issue Recap
As mentioned in the previous post, due to K8S scheduling issues, all data collection app Pods were concentrated on a single worker node. Each Pod created a large number of threads to handle tasks concurrently, ultimately leading to thread resource exhaustion (thread limit of 65,535), causing the node to crash, and service failure.
Solution Approach: How to distribute application Pods relatively evenly across all worker nodes.

Solution 1: Pod Anti-Affinity
Anti-Affinity is a method that uses scheduling constraints to prevent Pods from clustering on the same node, suitable for scenarios requiring high service availability or resource isolation.
Configuration rules: Define scheduling constraints by setting anti-affinity rules based on Pod labels. For example: prevent multiple replicas of the same Pod from being scheduled to the same node.
Constraint types:
- Hard rules (requiredDuringSchedulingIgnoredDuringExecution): Pods must satisfy anti-affinity requirements during scheduling, otherwise they cannot be scheduled
- Soft rules (preferredDuringSchedulingIgnoredDuringExecution): Attempt to satisfy anti-affinity during scheduling, but allow scheduling to non-compliant nodes when requirements cannot be met
Topology scope: Define the scope where anti-affinity takes effect through topologyKey, such as distribution by node hostname (kubernetes.io/hostname).

Solution 2: Pod Topology Spread Constraints
maxSkew is part of Pod topology spread constraints, used to limit the distribution deviation of Pods across different nodes or zones. Compared to Pod anti-affinity, maxSkew provides more flexible scheduling strategies, allowing certain distribution deviations.
Define distribution deviation: maxSkew specifies the quantity difference of Pods between different topology domains (such as nodes). For example: maxSkew: 1 means the Pod quantity difference between each node is at most 1.
Scheduling strategy: whenUnsatisfiable: DoNotSchedule: If distribution constraints cannot be satisfied, the Pod will not be scheduled.
The scheduler will try to ensure Pods are evenly distributed across different nodes.

Issue Fix
After evaluating both solutions, I chose Solution 2: maxSkew, because it can more flexibly control Pod distribution deviation while allowing a certain degree of scheduling tolerance, avoiding scheduling failures caused by hard rules.
After configuring appropriate maxSkew parameters, the application was redeployed through CI/CD, and its Pod replicas were successfully distributed across worker nodes, with the entire service and cluster running stably.

Summary
Anti-Affinity is about separation: "Keep Pod A away from Pod B."
maxSkew is about balance: "Spread all these identical pods out as evenly as you can."

Next, we'll discuss how this problem was discovered. Welcome everyone to join the discussion.

Sep 15, 2025
----- Chinese
æ¢ç´¢ SREï¼šK8S é›†ç¾¤ POD è°ƒåº¦é—®é¢˜è¿½è¸ªè¿‡ç¨‹

é—®é¢˜åˆç°ï¼š4æœˆ29æ—¥ï¼Œç›‘æ§ç³»ç»Ÿæ¢æµ‹åˆ°å¼‚å¸¸åï¼Œè‡ªåŠ¨å‘å‡ºå‘Šè­¦å¹¶æ¨é€åˆ°å³æ—¶èŠå¤©å·¥å…·ï¼Œæç¤ºK8Sé›†ç¾¤ä¸­æœ‰å¤šä¸ªPodçŠ¶æ€ä¸ºCrashLoopBackOffï¼Œä¸”é—´éš”15åˆ†é’ŸæŒç»­æ¨é€3æ¬¡ä»¥ä¸Šã€‚æ’é™¤å¶å‘æƒ…å†µä»¥åï¼Œå€¼ç­SREä»‹å…¥ã€‚
	
åˆæ­¥åˆ†æï¼š
CrashLoopBackOff è¡¨ç¤ºPodæ— æ³•æ­£å¸¸å¯åŠ¨ï¼Œå¯èƒ½ç”±äºåº”ç”¨æœ¬èº«é—®é¢˜ã€èµ„æºé™åˆ¶æˆ–åº•å±‚èŠ‚ç‚¹é—®é¢˜å¯¼è‡´ã€‚Podé”™è¯¯logå¦‚ä¹‹å‰å¸–å­æåˆ°çš„ï¼šthread_init: Resource temporarily unavailableã€‚
ä¸ºäº†æ›´å¿«æ¢å¤æœåŠ¡ï¼Œå†³å®šä»åº•å±‚èŠ‚ç‚¹èµ„æºå…¥æ‰‹ï¼Œé€æ­¥æ’æŸ¥é—®é¢˜æ ¹æºã€‚
	
å®šä½å·¥ä½œèŠ‚ç‚¹ï¼šèµ„æºè€—å°½åˆç°ç«¯å€ª
é€šè¿‡æ£€æŸ¥é—®é¢˜Podçš„è°ƒåº¦æƒ…å†µï¼Œå‘ç°è¿™äº›Podå‡è¿è¡Œåœ¨å·¥ä½œèŠ‚ç‚¹d01ä¸Šã€‚é€šè¿‡SSHç™»å½•åˆ°èŠ‚ç‚¹d01å°è¯•æ·±å…¥åˆ†æï¼Œå´å‘ç°åœ¨åˆ‡æ¢ç”¨æˆ·æ—¶å‡ºç°ä»¥ä¸‹å¼‚å¸¸ï¼š
-bash: fork: Cannot allocate memory
åˆæ­¥ç°è±¡ï¼šèŠ‚ç‚¹æ— æ³•åˆ›å»ºæ–°è¿›ç¨‹ï¼Œæç¤ºâ€œCannot allocate memoryâ€ï¼Œè¡¨æ˜èŠ‚ç‚¹å¯èƒ½å·²ç»è€—å°½äº†èµ„æºï¼ˆå¦‚å†…å­˜æˆ–çº¿ç¨‹ï¼‰ã€‚
è‡³æ­¤ï¼Œå†…å­˜è€—å°½çš„æ ¹æœ¬åŸå› å°šä¸æ˜ç¡®ã€‚
åœ¨æ²¡æœ‰æ›´å¤šçº¿ç´¢çš„æƒ…å†µä¸‹ï¼Œä¸ºäº†æ¢å¤æœåŠ¡åŠŸèƒ½å¹¶ä¿æŒé›†ç¾¤çš„ç¨³å®šæ€§ï¼Œå›¢é˜Ÿå†³å®šé‡å¯èŠ‚ç‚¹d01ã€‚
	
é—®é¢˜å¤ç°ï¼šé—´æ­‡æ€§ä¸­æ–­
5æœˆ4æ—¥æ™šï¼Œç±»ä¼¼é—®é¢˜å†æ¬¡å‘ç”Ÿï¼Œç›‘æ§ç³»ç»ŸæŒç»­æ¨é€å‘Šè­¦ï¼Œd01èŠ‚ç‚¹ä¸Šçš„å¤šä¸ªPodå†æ¬¡CrashLoopBackOffï¼ŒèŠ‚ç‚¹èµ„æºå†æ¬¡è€—å°½ã€‚å›¢é˜Ÿå†æ¬¡é‡å¯èŠ‚ç‚¹ï¼Œæ¢å¤æœåŠ¡ã€‚
é—®é¢˜ç‰¹ç‚¹ï¼š
æ¯éš”å‡ å¤©å¤ç°ä¸€æ¬¡ï¼Œè¡¨ç°å‡ºé—´æ­‡æ€§ã€‚
é‡å¯åé—®é¢˜æš‚æ—¶ç¼“è§£ï¼Œä½†æœªè§£å†³æ ¹å› ã€‚
	
æ·±å…¥æ’æŸ¥ï¼šå…³é”®å‘ç°
åŠ³åŠ¨èŠ‚åï¼Œå›¢é˜Ÿå¯¹d01èŠ‚ç‚¹è¿›è¡Œå…¨é¢æ£€æŸ¥ï¼Œå‘ç°èŠ‚ç‚¹ä¸Šå­˜åœ¨å¤šä¸ªç›¸åŒçš„Javaè¿›ç¨‹ï¼Œä¸”æ¯ä¸ªè¿›ç¨‹å¹³å‡ä½¿ç”¨8000+çº¿ç¨‹ã€‚
è¿›ä¸€æ­¥è°ƒæŸ¥ç¡®è®¤ï¼Œè¿™äº›Javaè¿›ç¨‹å‡å±äºæ•°æ®æ”¶é›†appçš„podå‰¯æœ¬ï¼Œè´Ÿè´£æ‰«æ8k+å¯¹è±¡å¹¶å¤„ç†æ•°æ®ã€‚appçš„å¤šä¸ªå‰¯æœ¬åŒæ—¶è°ƒåº¦åœ¨d01ä¸Šæ¶ˆè€—å¤§é‡çº¿ç¨‹æˆä¸ºèµ„æºè€—å°½çš„æ ¸å¿ƒåŸå› ã€‚
	
æ•…äº‹çš„åç»­ï¼Œå¤§å®¶å¯ä»¥å‚è€ƒå‰é¢çš„å¸–å­ï¼Œåœ¨æ­¤æˆ‘ç®€è¦è¯´æ˜ä¸‹SREçš„ä¸»è¦èŒè´£ï¼š
ç›‘æ§ä¸å‘Šè­¦ï¼šå¯¹é›†ç¾¤è¿›è¡Œæœ‰æ•ˆç›‘æ§å¹¶ç¡®ä¿ç›‘æ§å·¥å…·è‡ªåŠ¨ä¸”åŠæ—¶æ¨é€å‘Šè­¦ã€‚
ä¼˜å…ˆæ¢å¤æœåŠ¡ï¼šå¿«é€Ÿæ¢å¤åŠŸèƒ½ï¼Œç¡®ä¿å‘Šè­¦æ¶ˆé™¤ã€‚
é—®é¢˜å®šä½ä¸ä¿®å¤ï¼šæ·±å…¥åˆ†ææ ¹å› ï¼Œè·Ÿè¿›ä¿®å¤é—®é¢˜ã€‚


----- English
Exploring SRE: K8S Cluster Pod Scheduling Issue Track Process

Hello everyone, let's discuss how we track the issue mentioned in the last story:Exploring SRE: K8S Cluster POD Scheduling Issue Description

Initial Analysis:
CrashLoopBackOff indicates that Pods cannot start normally, possibly due to application issues, resource limitations, or underlying node problems. Pod error logs showed the same issue mentioned in previous posts: "thread_init: Resource temporarily unavailable."
To restore service more quickly, we decided to start from the underlying node resources and gradually investigate the root cause.

Locating Worker Node: Resource Exhaustion Emerges
By checking the scheduling status of problematic Pods, we found that these Pods were all running on worker node d01. When we SSH'd into node d01 for deeper analysis, we encountered the following anomaly when switching users:
-bash: fork: Cannot allocate memory

Initial observation: The node could not create new processes, showing "Cannot allocate memory," indicating that the node had likely exhausted resources (such as memory or threads).
At this point, the root cause of memory exhaustion was still unclear.
Without more clues, to restore service functionality and maintain cluster stability, SRE team decided to restart node d01.

Problem Recurrence: Intermittent Interruptions**
On the evening of May 4th, similar problems occurred again. The monitoring system continuously pushed alerts, multiple Pods on node d01 experienced CrashLoopBackOff again, and node resources were exhausted once more. The team restarted the node again to restore service.

Problem characteristics:
- Recurred every few days, showing intermittent behavior
- Problems were temporarily alleviated after restart, but root cause remained unresolved

Deep Investigation: Key Discovery
Couple of days later, the team conducted a comprehensive check of node d01 and discovered multiple identical Java processes on the node, with each process using an average of 8000+ threads.
Further investigation confirmed that these Java processes all belonged to Pod replicas of the data collection app, responsible for scanning 8k+ objects and processing data. Multiple replicas of the app being scheduled simultaneously on d01 and consuming massive threads became the core reason for resource exhaustion.

For the continuation of this story, you can refer to previous posts. Here I'll briefly explain the main responsibilities of SRE:
- **Monitoring and Alerting**: Effectively monitor the cluster and ensure monitoring tools automatically and promptly push alerts
- **Priority Service Recovery**: Quickly restore functionality and ensure alerts are cleared
- **Problem Location and Fix**: Deep analysis of root causes, follow up on problem fixes

Sep 18, 2025
----- Chinese
æµ·å› é‡Œå¸Œæ³•åˆ™ï¼šé¢„é˜²å°é—®é¢˜ï¼Œé¿å…å¤§ç¾éš¾
å¤§å®¶å¥½ï¼Œä»Šå¤©æˆ‘ä»¬æ¥ä¸€èµ·å­¦ä¹ ä¸‹å®‰å…¨ç”Ÿäº§çš„ç†è®ºçŸ¥è¯†(æ¢ä¸€ä¸ªå­¦ä¹ çš„é¦–é¡µå›¾ç‰‡ğŸ˜‚)ï¼Œä»æµ·å› é‡Œå¸Œæ³•åˆ™å¼€å§‹ã€‚
	
1ï¸âƒ£ æµ·å› é‡Œå¸Œæ³•åˆ™
å¦‚å›¾2ï¼ŒHeinrichæ³•åˆ™æœ€åˆæ¥æºäºå·¥ä¸šå®‰å…¨é¢†åŸŸï¼Œç”±å®‰å…¨ä¸“å®¶Herbert William Heinrichåœ¨20ä¸–çºª30å¹´ä»£æå‡ºã€‚å®ƒæè¿°äº†äº‹æ•…çš„å‘ç”Ÿè§„å¾‹ï¼š
â€¼ï¸â€¼ï¸â€¼ï¸ æ¯ä¸€èµ·ä¸¥é‡äº‹æ•…çš„èƒŒåï¼Œå¾€å¾€æœ‰29èµ·è½»å¾®äº‹æ•…ï¼Œä»¥åŠ300èµ·æœªé‚äº‹ä»¶æˆ–éšæ‚£ã€‚
æ¢å¥è¯è¯´ï¼Œé‡å¤§äº‹æ•…å¾€å¾€æ˜¯ç”±ä¸€ç³»åˆ—è¾ƒå°çš„é—®é¢˜ç§¯ç´¯æˆ–æœªè¢«è§£å†³çš„éšæ‚£æ‰€å¼•å‘çš„ã€‚å¦‚æœæˆ‘ä»¬èƒ½åŠæ—¶è¯†åˆ«å’Œå¤„ç†è¿™äº›å°é—®é¢˜ï¼Œå°±å¯ä»¥æœ‰æ•ˆåœ°é¢„é˜²æ›´ä¸¥é‡çš„åæœã€‚
	
2ï¸âƒ£ åœ¨SREé¢†åŸŸï¼Œæˆ‘ä»¬è‡´åŠ›äºé€šè¿‡æé«˜ç³»ç»Ÿçš„å¯é æ€§å’Œç¨³å®šæ€§ï¼Œæ¥ä¸ºç”¨æˆ·æä¾›å§‹ç»ˆå¦‚ä¸€çš„æœåŠ¡ä½“éªŒã€‚ä¸ºäº†å®ç°è¿™ä¸€ç›®æ ‡ï¼Œæˆ‘ä»¬ä¸ä»…è¦è§£å†³å·²ç»å‘ç”Ÿçš„é—®é¢˜ï¼Œæ›´éœ€è¦ä»æ½œåœ¨é—®é¢˜ä¸­å­¦ä¹ ï¼Œé˜²æ‚£äºæœªç„¶ã€‚åœ¨è¿™ä¸€è¿‡ç¨‹ä¸­ï¼ŒHeinrichæ³•åˆ™ä¸ºæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªæå…·ä»·å€¼çš„è§†è§’ã€‚
åœ¨SREå®è·µä¸­ï¼ŒHeinrichæ³•åˆ™æœ‰ç€å¹¿æ³›çš„åº”ç”¨åœºæ™¯ã€‚å°½ç®¡å®ƒæºäºå®‰å…¨é¢†åŸŸï¼Œä½†å…¶æ ¸å¿ƒæ€æƒ³â€”â€”é€šè¿‡å…³æ³¨å’Œè§£å†³å°é—®é¢˜æ¥é¿å…å¤§é—®é¢˜â€”â€”ä¸SREçš„ç›®æ ‡é«˜åº¦å¥‘åˆã€‚ä»¥ä¸‹æ˜¯å‡ ä¸ªå…³é”®ç‚¹ï¼š
 å°é—®é¢˜ç§¯ç´¯å¯¼è‡´å¤§æ•…éšœ
åœ¨å¤æ‚çš„åˆ†å¸ƒå¼ç³»ç»Ÿä¸­ï¼Œå‡ ä¹æ‰€æœ‰çš„é‡å¤§ç³»ç»Ÿæ•…éšœï¼ˆå¦‚å…¨ç«™å®•æœºã€æ•°æ®ä¸¢å¤±ç­‰ï¼‰éƒ½ä¸æ˜¯å•ä¸€äº‹ä»¶çš„ç»“æœï¼Œè€Œæ˜¯ç”±å¤šä¸ªå°é—®é¢˜å åŠ å¯¼è‡´çš„ã€‚
 å¦‚å‰æ–‡æåˆ°çš„å®ä¾‹ï¼Œk8sé›†ç¾¤ä¸­æ•°æ®æ”¶é›†appçš„PODå‰¯æœ¬ä¸åˆç†è°ƒåº¦å¯èƒ½æœ€åˆåªæ˜¯å½±å“é›†ç¾¤å·¥ä½œèŠ‚ç‚¹çš„ç¨³å®šï¼Œä½†å¦‚æœä¸åŠæ—¶å¤„ç†ï¼Œå¯èƒ½ä¼šå¯¼è‡´æ•´ä¸ªé›†ç¾¤çš„ä¸ç¨³å®šç”šè‡³å´©æºƒã€‚
	
 é‡è§†æœªé‚äº‹ä»¶ï¼ˆNear Missï¼‰
æœªé‚äº‹ä»¶æ˜¯æŒ‡é‚£äº›æ²¡æœ‰ç›´æ¥å½±å“åˆ°ç”Ÿäº§ç¯å¢ƒï¼Œä½†æš´éœ²äº†æ½œåœ¨é—®é¢˜çš„äº‹ä»¶ã€‚
 è¿™ä¸€ç‚¹æˆ‘ä¹Ÿæœ‰è¯è¦è¯´ï¼Œè¯·å…³æ³¨åç»­åˆ†äº«
	
 æ„å»ºâ€œå…ç–«ç³»ç»Ÿâ€
SREçš„æ ¸å¿ƒä¹‹ä¸€æ˜¯æ„å»ºå¯è§‚æµ‹æ€§å’Œè‡ªåŠ¨åŒ–çš„å·¥å…·é“¾ï¼Œä»¥å¿«é€Ÿå‘ç°å’Œä¿®å¤é—®é¢˜ã€‚é€šè¿‡ç›‘æ§å’Œæ—¥å¿—ç³»ç»Ÿï¼ŒSREå›¢é˜Ÿå¯ä»¥æ•æ‰åˆ°å°é—®é¢˜çš„ä¿¡å·ï¼Œæ¯”å¦‚é”™è¯¯ç‡çš„è½»å¾®ä¸Šå‡ã€å»¶è¿Ÿçš„çŸ­æš‚æ³¢åŠ¨ç­‰ã€‚è¿™äº›â€œå°ä¿¡å·â€æ­£æ˜¯Heinrichæ³•åˆ™ä¸­éšæ‚£çš„è¡¨ç°ï¼ŒåŠæ—¶å“åº”æ˜¯é¢„é˜²ç¾éš¾çš„å…³é”®ã€‚
 å¤§å®¶æœ‰æƒ³è¿‡æ²¡æœ‰ï¼ŒåŠæ—¶å“åº”çš„å‰ææ˜¯æ”¶åˆ°é€šçŸ¥ï¼Œé‚£å¦‚ä½•é«˜æ•ˆåœ°æ”¶åˆ°é€šçŸ¥å‘¢ï¼Ÿ
å‘é‚®ä»¶ï¼Ÿä¼šè¢«æ·¹æ²¡åœ¨ä¸€å †é‚®ä»¶ä¸­ï¼Œæ¯éš”5åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡é‚®ç®±ä¹Ÿä¸å¯èƒ½ï¼›
å‘å³æ—¶æ¶ˆæ¯ï¼Ÿç»“æœç±»ä¼¼ï¼Œå¤§å®¶æœ‰å•¥å»ºè®®è¯·ğŸ™‹
	
3ï¸âƒ£ æƒ³èµ·å°æ—¶å€™çœ‹è¿‡å®£ä¼ èŠ‚èƒ½ç¯çš„å…¬ç›Šå¹¿å‘Šï¼šå†å°çš„æ”¯æŒä¹Ÿæ˜¯ä¸€ç§åŠ›é‡ï¼åœ¨SREçš„ä¸–ç•Œé‡Œï¼Œæ¯ä¸€æ¬¡å°çš„æ”¹è¿›ï¼Œéƒ½æ˜¯å¯¹å¤§ç¾éš¾çš„æœ‰æ•ˆé¢„é˜²ã€‚
----- English
Heinrich's Rule: Prevent Small Problems, Avoid Major Disasters

Hello everyone, today let's learn about safety production theory together (with a different learning homepage image ğŸ˜‚), starting with Heinrich's Rule.

1ï¸âƒ£ Heinrich's Rule
As shown in Figure 2, Heinrich's Rule originally comes from the industrial safety field, proposed by safety expert Herbert William Heinrich in the 1930s. It describes the pattern of accident occurrence:
â€¼ï¸â€¼ï¸â€¼ï¸ Behind every serious accident, there are often 29 minor accidents and 300 near misses or hazards.
In other words, major accidents are often triggered by a series of smaller problems that accumulate or unresolved hazards. If we can identify and address these small problems in time, we can effectively prevent more serious consequences.

2ï¸âƒ£ In the SRE field, we are committed to providing users with a consistent service experience by improving system reliability and stability. To achieve this goal, we need not only to solve problems that have already occurred, but also to learn from potential problems and prevent them before they happen. In this process, Heinrich's Rule provides us with an extremely valuable perspective.

In SRE practice, Heinrich's Rule has wide application scenarios. Although it originates from the safety field, its core idea - preventing major problems by focusing on and solving small problems - is highly aligned with SRE goals. Here are several key points:

**Small Problems Accumulate to Cause Major Failures**
In complex distributed systems, almost all major system failures (such as site-wide outages, data loss, etc.) are not the result of a single event, but are caused by the accumulation of multiple small problems.
As mentioned in the previous example, unreasonable scheduling of data collection app POD replicas in a k8s cluster may initially only affect the stability of cluster worker nodes, but if not handled promptly, it could lead to instability or even collapse of the entire cluster.

**Pay Attention to Near Miss Events**
Near miss events refer to those events that do not directly affect the production environment but expose potential problems.
I also have something to say about this point, please follow subsequent sharing.

**Building an "Immune System"**
One of the core aspects of SRE is building observability and automated toolchains to quickly discover and fix problems. Through monitoring and logging systems, SRE teams can capture signals of small problems, such as slight increases in error rates, brief fluctuations in latency, etc. These "small signals" are exactly the manifestation of hazards in Heinrich's Rule, and timely response is key to preventing disasters.
Have you ever thought about it? The prerequisite for timely response is receiving notifications. So how can we efficiently receive notifications?
Send emails? They'll be buried in a pile of emails, and checking email every 5 minutes is impossible;
Send instant messages? The result is similar. What suggestions do you have? Please ğŸ™‹

3ï¸âƒ£ In the SRE world, every small improvement is an effective prevention against major disasters.

Sep 21, 2025
----- Chinese
Kubernetes Secret ç®€ä»‹
å¤§å®¶å¥½ï¼ŒèŠè¿‡å®‰å…¨ç”Ÿäº§æ³•åˆ™ä»¥åï¼Œæˆ‘æ¥ç»§ç»­åˆ†äº«K8Sä½¿ç”¨è¿‡ç¨‹ä¸­å‘ç”Ÿçš„æ¡ˆä¾‹ 
è¿™æ˜¯ä¸€ä¸ªéšç§˜ä¸”å¾ˆå®¹æ˜“è¸©å‘çš„æ¡ˆä¾‹ï¼Œä»ä»‹ç»K8Sçš„secretå¼€å§‹ã€‚
åœ¨ K8S ä¸­ï¼ŒSecret æ˜¯ä¸€ç§ä¸“é—¨ç”¨äºå­˜å‚¨æ•æ„Ÿä¿¡æ¯ï¼ˆå¦‚å¯†ç ã€API å¯†é’¥ã€è¯ä¹¦ç­‰ï¼‰çš„èµ„æºç±»å‹ã€‚å®ƒå¯ä»¥å¸®åŠ©æˆ‘ä»¬æ›´å®‰å…¨åœ°ç®¡ç†å’Œä½¿ç”¨è¿™äº›æ•°æ®ã€‚
1ï¸âƒ£ ä¸ºä»€ä¹ˆè¦ç”¨ Secret
åœ¨ç®¡ç†æ•æ„Ÿä¿¡æ¯æ—¶ï¼Œç›´æ¥å°†è¿™äº›æ•°æ®å­˜å‚¨åœ¨ä»£ç æˆ–é…ç½®æ–‡ä»¶ä¸­å¯èƒ½å­˜åœ¨ä»¥ä¸‹é—®é¢˜ï¼š
âš ï¸ å®‰å…¨é£é™©ï¼šæ•æ„Ÿä¿¡æ¯å®¹æ˜“æ˜æ–‡æš´éœ²ï¼Œç‰¹åˆ«æ˜¯åœ¨ç‰ˆæœ¬æ§åˆ¶ç³»ç»Ÿï¼ˆå¦‚ Gitï¼‰ä¸­ã€‚
âš ï¸ ç¼ºä¹çµæ´»æ€§ï¼šä¸åŒç¯å¢ƒä¸­çš„æ•æ„Ÿä¿¡æ¯å¯èƒ½ä¸åŒï¼Œç›´æ¥ç¡¬ç¼–ç ä¼šå¯¼è‡´ä¿®æ”¹å›°éš¾ã€‚
âš ï¸ æƒé™ç®¡ç†ä¸è¶³ï¼šæ— æ³•æœ‰æ•ˆæ§åˆ¶å“ªäº›ç”¨æˆ·æˆ–åº”ç”¨å¯ä»¥è®¿é—®è¿™äº›æ•°æ®ã€‚
ä½¿ç”¨ Secret çš„å¥½å¤„åŒ…æ‹¬ï¼š
ğŸ‘ å®‰å…¨å­˜å‚¨ï¼šå°†æ•æ„Ÿä¿¡æ¯ä¸åº”ç”¨è§£è€¦ï¼Œé™ä½æ³„éœ²é£é™©ã€‚
ğŸ‘ æƒé™æ§åˆ¶ï¼šé€šè¿‡ Kubernetes çš„ RBAC æœºåˆ¶ï¼Œç¡®ä¿åªæœ‰è¢«æˆæƒçš„åº”ç”¨å¯ä»¥è®¿é—®å¯¹åº”çš„ Secretã€‚
ğŸ‘ åŠ¨æ€æ›´æ–°ï¼šæ”¯æŒæ•æ„Ÿä¿¡æ¯çš„åŠ¨æ€æ›´æ–°ï¼Œæ— éœ€é‡æ–°éƒ¨ç½²åº”ç”¨ã€‚
	
2ï¸âƒ£ Secret ä¸­çš„å€¼éœ€è¦ä»¥ Base64 ç¼–ç å­˜å‚¨ï¼Œè¿™å¹¶ä¸æ˜¯ä¸ºäº†åŠ å¯†ï¼Œè€Œæ˜¯å‡ºäºä»¥ä¸‹åŸå› ï¼š
æ ¼å¼å…¼å®¹ï¼šBase64 å¯ä»¥å°†æ•°æ®è½¬æ¢ä¸ºåªåŒ…å« ASCII å­—ç¬¦çš„å­—ç¬¦ä¸²ï¼Œç¡®ä¿å³ä½¿æ˜¯äºŒè¿›åˆ¶æ•°æ®ä¹Ÿèƒ½å®‰å…¨åœ°å­˜å‚¨åœ¨ YAML/JSON æ–‡ä»¶ä¸­ã€‚
å‡å°‘æ˜æ–‡æš´éœ²ï¼šè™½ç„¶ä¸æ˜¯åŠ å¯†æ‰‹æ®µï¼Œä½† Base64 å¯ä»¥åœ¨ä¸€å®šç¨‹åº¦ä¸Šé¿å…æ•°æ®è¢«ç›´æ¥è¯†åˆ«ã€‚
API è¦æ±‚ï¼šKubernetes çš„ API è®¾è®¡è¦æ±‚ Secret çš„å€¼ä»¥ Base64 ç¼–ç å½¢å¼å­˜å‚¨ã€‚
â€¼ï¸â€¼ï¸â€¼ï¸ éœ€è¦æ³¨æ„çš„æ˜¯ï¼ŒBase64 å¹¶ä¸èƒ½çœŸæ­£ä¿æŠ¤æ•°æ®çš„å®‰å…¨ã€‚å¦‚æœéœ€è¦æ›´é«˜çš„å®‰å…¨æ€§ï¼Œå¯ä»¥ç»“åˆå¤–éƒ¨å¯†é’¥ç®¡ç†ç³»ç»Ÿä½¿ç”¨ã€‚
	
3ï¸âƒ£ åº”ç”¨å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼ä½¿ç”¨ Secret ä¸­å­˜å‚¨çš„æ•æ„Ÿä¿¡æ¯ï¼š
æ³¨å…¥ç¯å¢ƒå˜é‡ï¼Œå¦‚å›¾3æ‰€ç¤ºï¼š
å°† Secret çš„å€¼æ³¨å…¥åˆ° Pod çš„ç¯å¢ƒå˜é‡ä¸­ï¼Œåº”ç”¨å¯ä»¥é€šè¿‡ç¯å¢ƒå˜é‡ç›´æ¥è¯»å–è¿™äº›æ•°æ®ã€‚è¿™ç§æ–¹å¼é€‚åˆç®€å•çš„é…ç½®åœºæ™¯ã€‚
æŒ‚è½½åˆ°æ–‡ä»¶ç³»ç»Ÿ
å°† Secret çš„å€¼ä»¥æ–‡ä»¶å½¢å¼æŒ‚è½½åˆ°å®¹å™¨å†…ï¼Œåº”ç”¨å¯ä»¥é€šè¿‡è¯»å–æ–‡ä»¶æ¥è·å–æ•æ„Ÿä¿¡æ¯ã€‚è¿™ç§æ–¹å¼é€‚åˆç®¡ç†å¤æ‚æˆ–å¤§æ•°æ®é‡çš„æ•æ„Ÿå†…å®¹ã€‚
	
é€šè¿‡åˆç†ä½¿ç”¨ Secretï¼Œå¯ä»¥å¸®åŠ©å¼€å‘è€…æ›´å®‰å…¨ã€é«˜æ•ˆåœ°ç®¡ç†æ•æ„Ÿä¿¡æ¯ã€‚æ¥ä¸‹æ¥æˆ‘å°†å…·ä½“è¯´æ˜æ¡ˆä¾‹å†…å®¹ä»¥åŠå¦‚ä½•ä¿®å¤ã€‚

----- English

Introduction to Kubernetes Secret

Hello everyone, after discussing safety production rules, I'll continue sharing cases that occurred during K8S usage.
This is a hidden and easily overlooked case, starting with an introduction to K8S secrets.

In K8S, Secret is a resource type specifically designed for storing sensitive information (such as passwords, API keys, certificates, etc.). It helps us manage and use this data more securely.

1ï¸âƒ£ Why Use Secret

When managing sensitive information, directly storing this data in code or configuration files may present the following problems:
âš ï¸ Security risks: Sensitive information is easily exposed in plain text, especially in version control systems (like Git).
âš ï¸ Lack of flexibility: Sensitive information may differ across environments, and hard-coding makes modifications difficult.
âš ï¸ Insufficient permission management: Cannot effectively control which users or applications can access this data.

Benefits of using Secret include:
ğŸ‘ Secure storage: Decouples sensitive information from applications, reducing exposure risks.
ğŸ‘ Permission control: Through Kubernetes' RBAC mechanism, ensures only authorized applications can access corresponding Secrets.
ğŸ‘ Dynamic updates: Supports dynamic updates of sensitive information without redeploying applications.

2ï¸âƒ£ Values in Secret need to be stored in Base64 encoding, which is not for encryption but for the following reasons:

Format compatibility: Base64 can convert data into strings containing only ASCII characters, ensuring even binary data can be safely stored in YAML/JSON files.
Reduce plain text exposure: While not an encryption method, Base64 can prevent data from being directly identified to some extent.
API requirements: Kubernetes' API design requires Secret values to be stored in Base64 encoded format.
â€¼ï¸â€¼ï¸â€¼ï¸ It's important to note that Base64 cannot truly protect data security. If higher security is needed, it can be used in combination with external key management systems.

3ï¸âƒ£ Applications can use sensitive information stored in Secret through the following methods:

Inject environment variables, as shown in Figure 3:
Inject Secret values into Pod environment variables, allowing applications to directly read this data through environment variables. This method is suitable for simple configuration scenarios.

Mount to file system:
Mount Secret values as files into containers, allowing applications to obtain sensitive information by reading files. This method is suitable for managing complex or large-volume sensitive content.

By properly using Secret, developers can manage sensitive information more securely and efficiently. Next, I will specifically explain the case content and how to fix it.
